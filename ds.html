<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Applied Time Series Analysis with R</title>
  <meta name="description" content="Applied Time Series Analysis with R">
  <meta name="generator" content="bookdown <!--bookdown:version--> and GitBook 2.6.7">

  <meta property="og:title" content="Applied Time Series Analysis with R" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="SMAC-Group/app_ts" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Applied Time Series Analysis with R" />
  
  
  

<meta name="author" content="Stéphane Guerrier, Roberto Molinari and Haotian Xu">


<meta name="date" content="2018-08-19">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  <link rel="shortcut icon" href="favicon.ico" type="image/x-icon">
<!--bookdown:link_prev-->
<!--bookdown:link_next-->
<script src="assets/jquery-2.2.3/jquery.min.js"></script>
<link href="assets/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<script src="assets/gitbook-2.6.7/js/app.min.js"></script>
<script src="assets/gitbook-2.6.7/js/lunr.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="assets/gitbook-2.6.7/js/jquery.highlight.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; position: absolute; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; }
pre.numberSource a.sourceLine:empty
  { position: absolute; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: absolute; left: -5em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



<!--bookdown:title:start-->
<div id="header">
<h1 class="title">Applied Time Series Analysis with R</h1>
<p class="author"><em>Stéphane Guerrier, Roberto Molinari and Haotian Xu</em></p>
<p class="date"><em>2018-08-19</em></p>
</div>
<!--bookdown:title:end-->

<!--bookdown:toc:start-->
  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">
<!--bookdown:toc2:start-->
<ul>
<li><a href="#preface"><span class="toc-section-number">1</span> Preface</a><ul>
<li><a href="#about-this-book"><span class="toc-section-number">1.1</span> About This Book</a></li>
<li><a href="#bibliographic-note"><span class="toc-section-number">1.2</span> Bibliographic Note</a></li>
<li><a href="#acknowledgements"><span class="toc-section-number">1.3</span> Acknowledgements</a></li>
<li><a href="#license"><span class="toc-section-number">1.4</span> License</a></li>
</ul></li>
<li><a href="#basic-elements-of-time-series"><span class="toc-section-number">2</span> Basic Elements of Time Series</a><ul>
<li><a href="#the-wold-decomposition"><span class="toc-section-number">2.1</span> The Wold Decomposition</a><ul>
<li><a href="#the-deterministic-component-signal"><span class="toc-section-number">2.1.1</span> The deterministic component (Signal)</a></li>
<li><a href="#the-random-component-noise"><span class="toc-section-number">2.1.2</span> The random component (Noise)</a></li>
</ul></li>
<li><a href="#basicmodels"><span class="toc-section-number">2.2</span> Basic Time Series Models</a><ul>
<li><a href="#wn"><span class="toc-section-number">2.2.1</span> White Noise</a></li>
<li><a href="#rw"><span class="toc-section-number">2.2.2</span> Random Walk</a></li>
<li><a href="#ar1"><span class="toc-section-number">2.2.3</span> First-Order Autoregressive Model</a></li>
<li><a href="#ma1"><span class="toc-section-number">2.2.4</span> Moving Average Process of Order 1</a></li>
<li><a href="#drift"><span class="toc-section-number">2.2.5</span> Linear Drift</a></li>
</ul></li>
<li><a href="#lts"><span class="toc-section-number">2.3</span> Composite Stochastic Processes</a></li>
</ul></li>
<li><a href="#representations-of-time-series"><span class="toc-section-number">3</span> Representations of Time Series</a><ul>
<li><a href="#the-autocorrelation-and-autocovariance-functions"><span class="toc-section-number">3.1</span> The Autocorrelation and Autocovariance Functions</a><ul>
<li><a href="#a-fundamental-representation"><span class="toc-section-number">3.1.1</span> A Fundamental Representation</a></li>
<li><a href="#admissible-autocorrelation-functions"><span class="toc-section-number">3.1.2</span> Admissible Autocorrelation Functions</a></li>
</ul></li>
<li><a href="#estimation-of-moments"><span class="toc-section-number">3.2</span> Estimation of Moments</a><ul>
<li><a href="#estimation-of-the-mean-function"><span class="toc-section-number">3.2.1</span> Estimation of the Mean Function</a></li>
<li><a href="#sample-autocovariance-and-autocorrelation-functions"><span class="toc-section-number">3.2.2</span> Sample Autocovariance and Autocorrelation Functions</a></li>
<li><a href="#robustness-issues"><span class="toc-section-number">3.2.3</span> Robustness Issues</a></li>
<li><a href="#sample-cross-covariance-and-cross-correlation-functions"><span class="toc-section-number">3.2.4</span> Sample Cross-Covariance and Cross-Correlation Functions</a></li>
</ul></li>
</ul></li>
</ul>
<!--bookdown:toc2:end-->
      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Applied Time Series Analysis with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<!--bookdown:toc:end-->
<!--bookdown:body:start-->
<div id="preface" class="section level1">
<h1><span class="header-section-number">1</span> Preface</h1>
<p>Welcome to Applied Time Series Analysis with <code>R</code>.</p>
<div id="about-this-book" class="section level2">
<h2><span class="header-section-number">1.1</span> About This Book</h2>
<p>This book is intended as a support for the course of 463 (Applied Time Series). It contains an overview of the basic procedures to adequately approach a time series analysis with insight to more advanced analysis of time series. It firstly introduces the basic concepts and theory to appropriately use the applied tools that are presented in the second (and main) part of the book. In the latter part the reader will learn how to use descriptive analysis to identify the important characteristics of a time serues and then employ modelling and inference techniques (made available through R funtions) that allow to describe a time series and make predictions. The last part of the book will give introductory notions on more advanced analysis of time series where the reader will achieve a basic understanding of the tools available to anayse more complex characteristic of time series.</p>
<p>This document is under active development and as a result is likely to contains
many errors. As Montesquieu puts it:</p>
<blockquote>
<p>“<em>La nature semblait avoir sagement pourvu à ce que les sottises des hommes
fussent passagères, et les livres les immortalisent.</em>”</p>
</blockquote>
<!-- ## Contents -->
<!-- This book is structured as follows: -->
<!-- - Basic Elements of Time Series -->
<!--     - Wold representation deterministic + random -->
<!--     - Examples of deterministic components (trend + seasonality) -->
<!--     - Random components: basic time series models -->
<!-- - Fundamental Representations -->
<!--     - Conditions for fundamental representations (e.g. gaussian) -->
<!--     - AutoCovariance and AutoCorrelation Functions -->
<!--     - Estimators: Empirical ACF -->
<!--     - Spectral Density and WV -->
<!-- - Stationarity of Time Series -->
<!--     - Stationarity vs Non-Stationarity -->
<!--     - Linear operators and processes -->
<!--     - Weak and Strong Stationarity -->
<!-- - SARIMA Models -->
<!--     - AR(p) Models -->
<!--     - MA(q) Models -->
<!--     - ARMA(p,q) Models -->
<!--     - ARIMA(p,d,q) Models -->
<!--     - SARIMA(p,d,q)(P,D,Q) Models -->
<!-- - Descriptive Analysis -->
<!--     - Raw Data -->
<!--     - ACF plots -->
<!--     - Identifying models -->
<!--     - Other representations: SDF and WV -->
<!-- - Inference -->
<!--     - Estimation -->
<!--     - Inference -->
<!--     - Model Selection -->
<!-- - Advanced Topics -->
<!--     - GARCH -->
<!--     - State-Space Models -->
<!--     - Multivariate (VAR) Models -->
</div>
<div id="bibliographic-note" class="section level2">
<h2><span class="header-section-number">1.2</span> Bibliographic Note</h2>
<p>This text is heavily inspired by the following three execellent references:</p>
<ol style="list-style-type: decimal">
<li>“<em>Time Series Analysis and Its Applications</em>”, Fourth Edition, Robert H. Shumway &amp; David S. Stoffer.</li>
<li>“<em>Time Series for Macroeconomics and Finance</em>”, John H. Cochrane.</li>
<li>“<em>Cours de Séries Temporelles: Théorie et Applications</em>”, Volume 1, Arthur Charpentier.</li>
</ol>
</div>
<div id="acknowledgements" class="section level2">
<h2><span class="header-section-number">1.3</span> Acknowledgements</h2>
<p>The text has benefited greatly from the contributions of many people who have provided extremely useful comments, suggestions and corrections. These are:</p>
<ul>
<li><a href="https://github.com/zionward">Ziying Wang</a></li>
<li><a href="https://github.com/Lyle-Haoxian">Haoxian Zhong</a></li>
<li><a href="https://www.linkedin.com/in/zhihan-xiong-988152114">Zhihan Xiong</a></li>
<li><a href="https://github.com/Nathanael-Claussen">Nathanael Claussen</a></li>
<li><a href="https:://github.com/munsheet">Justin Lee</a></li>
</ul>
<p>The authors are particularly grateful to James Balamuta who introduced them to the use of the different tools provided by the RStudio environment and greatly contributed to an earlier version of this book:
- <a href="https::/github.com/coatless">James Balamuta</a></p>
</div>
<div id="license" class="section level2">
<h2><span class="header-section-number">1.4</span> License</h2>
<div class="figure">
<img src="images/license/cc.png" alt="This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License." />
<p class="caption">This work is licensed under a <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.</p>
</div>
<!--chapter:end:index.Rmd-->
</div>
</div>
<div id="basic-elements-of-time-series" class="section level1">
<h1><span class="header-section-number">2</span> Basic Elements of Time Series</h1>
<p>We can start the discussion on the basic elements of time series by using a practical example from real data made available through the R software. The data represent the global mean land–ocean temperature shifts from 1880 to 2015 (with base index being the average temperatures from 1951 to 1980) and this time series is represented in the plot below.</p>
<p><img src="ds_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>These data have been used as a support in favour of the argument that the global temperatures are increasing and that global warming has occured over the last half of the twentieth century. The first approach that one would take is to try and measure the average increase by fitting a (linear) model and testing if the (positive) slope is significant. In order to do so, we would require the residuals from the fitted model to independently and identically distributed (iid). Let us fit a model with the years (time) as explanatory variable and check the residuals from this fit.</p>
<p><img src="ds_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>It can be seen from the upper left plot that the trend appears to be removed and, if looking at the residuals as one would usually do in a (linear) regression framework, the residual plots seem to suggest that the modelling has done a relatively good job since no particular pattern seems to emerge and their distribution is quite close to being Gaussian.</p>
<p>However, is it possible to conclude from the plots that the data are <em>iid</em>? More specifically, can we assume that the residuals are independent? This is a fundamental question in order for inference procedures to be carried out in an appropriate manner and to limit false conclusions. Let us provide an example through a simulated data set where we know that there is an upward trend through time (i.e. the slope <span class="math inline">\(\beta = 0.01\)</span>) and our goal would be to show that this trend exists. Considering this, we simulate two cases where, in the first, the residuals are actually Gaussian iid while, in the second, the residuals are Gaussian but are dependent over time. The first case is shown below.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">9</span>)</a>
<a class="sourceLine" id="cb1-2" data-line-number="2"></a>
<a class="sourceLine" id="cb1-3" data-line-number="3"><span class="co"># Simulate time series with iid residuals</span></a>
<a class="sourceLine" id="cb1-4" data-line-number="4">y.ind &lt;-<span class="st"> </span><span class="kw">cumsum</span>(<span class="kw">rep</span>(<span class="fl">0.01</span>, <span class="dv">100</span>)) <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">100</span>)</a>
<a class="sourceLine" id="cb1-5" data-line-number="5"></a>
<a class="sourceLine" id="cb1-6" data-line-number="6"><span class="co"># Simulate time series with dependent residuals</span></a>
<a class="sourceLine" id="cb1-7" data-line-number="7">y.dep &lt;-<span class="st"> </span><span class="kw">cumsum</span>(<span class="kw">rep</span>(<span class="fl">0.01</span>, <span class="dv">100</span>)) <span class="op">+</span><span class="st"> </span><span class="kw">arima.sim</span>(<span class="dt">n =</span> <span class="dv">100</span>, <span class="kw">list</span>(<span class="dt">ar =</span> <span class="kw">c</span>(<span class="fl">0.8897</span>, <span class="fl">-0.4858</span>)))</a>
<a class="sourceLine" id="cb1-8" data-line-number="8"></a>
<a class="sourceLine" id="cb1-9" data-line-number="9"><span class="co"># Define explanatory variable (time)</span></a>
<a class="sourceLine" id="cb1-10" data-line-number="10">time &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="dv">100</span></a>
<a class="sourceLine" id="cb1-11" data-line-number="11"></a>
<a class="sourceLine" id="cb1-12" data-line-number="12"><span class="co"># Fit a linear model to estimate the slope (for the iid setting)</span></a>
<a class="sourceLine" id="cb1-13" data-line-number="13">fit.ind &lt;-<span class="st"> </span><span class="kw">lm</span>(y.ind <span class="op">~</span><span class="st"> </span>time)</a>
<a class="sourceLine" id="cb1-14" data-line-number="14"><span class="kw">summary</span>(fit.ind)</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y.ind ~ time)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -2.6000 -0.7028 -0.1409  0.4439  2.7085 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)   
## (Intercept) 0.002092   0.194085   0.011  0.99142   
## time        0.008899   0.003337   2.667  0.00895 **
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.9632 on 98 degrees of freedom
## Multiple R-squared:  0.06767,    Adjusted R-squared:  0.05816 
## F-statistic: 7.113 on 1 and 98 DF,  p-value: 0.008954</code></pre>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb3-1" data-line-number="1"><span class="co"># Fit a linear model to estimate the slope (for the dependent setting)</span></a>
<a class="sourceLine" id="cb3-2" data-line-number="2">fit.dep &lt;-<span class="st"> </span><span class="kw">lm</span>(y.dep <span class="op">~</span><span class="st"> </span>time)</a>
<a class="sourceLine" id="cb3-3" data-line-number="3"><span class="kw">summary</span>(fit.dep)</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y.dep ~ time)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.6058 -0.9505  0.2111  1.1174  2.4196 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)  
## (Intercept) -0.071260   0.293703  -0.243   0.8088  
## time         0.008441   0.005049   1.672   0.0977 .
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.458 on 98 degrees of freedom
## Multiple R-squared:  0.02773,    Adjusted R-squared:  0.01781 
## F-statistic: 2.795 on 1 and 98 DF,  p-value: 0.09775</code></pre>
<p>As can be seen, the estimated slope (<span class="math inline">\(\approx\)</span> 0.009) is close to the true slope (0.01) and is significant (i.e. the p-value is smaller than the common rejection level 0.05). Hence, from this inference procedure we can conclude that the slope is significant and is roughly equal to 0.01 (which corresponds to the truth). However, let us explore the same analysis when the residuals are not independent.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb5-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">9</span>)</a>
<a class="sourceLine" id="cb5-2" data-line-number="2"></a>
<a class="sourceLine" id="cb5-3" data-line-number="3"><span class="co"># Simulate time series with dependent residuals</span></a>
<a class="sourceLine" id="cb5-4" data-line-number="4">y.dep &lt;-<span class="st"> </span><span class="kw">cumsum</span>(<span class="kw">rep</span>(<span class="fl">0.01</span>, <span class="dv">100</span>)) <span class="op">+</span><span class="st"> </span><span class="kw">arima.sim</span>(<span class="dt">n =</span> <span class="dv">100</span>, <span class="kw">list</span>(<span class="dt">ar =</span> <span class="kw">c</span>(<span class="fl">0.8897</span>, <span class="fl">-0.4858</span>)))</a>
<a class="sourceLine" id="cb5-5" data-line-number="5"></a>
<a class="sourceLine" id="cb5-6" data-line-number="6"><span class="co"># Define explanatory variable (time)</span></a>
<a class="sourceLine" id="cb5-7" data-line-number="7">time &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="dv">100</span></a>
<a class="sourceLine" id="cb5-8" data-line-number="8"></a>
<a class="sourceLine" id="cb5-9" data-line-number="9"><span class="co"># Fit a linear model to estimate the slope (for the dependent setting)</span></a>
<a class="sourceLine" id="cb5-10" data-line-number="10">fit.dep &lt;-<span class="st"> </span><span class="kw">lm</span>(y.dep <span class="op">~</span><span class="st"> </span>time)</a>
<a class="sourceLine" id="cb5-11" data-line-number="11"><span class="kw">summary</span>(fit.dep)</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y.dep ~ time)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.2157 -0.9189 -0.1220  0.9663  3.6754 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept) 0.276142   0.289520   0.954    0.343
## time        0.002288   0.004977   0.460    0.647
## 
## Residual standard error: 1.437 on 98 degrees of freedom
## Multiple R-squared:  0.002152,   Adjusted R-squared:  -0.00803 
## F-statistic: 0.2114 on 1 and 98 DF,  p-value: 0.6467</code></pre>
<p>In this case we can observe that the p-value is greater than 0.05 and consequently the slope does not appear to be significant (although it is in reality). Therefore, the inference procedures can be misleading when not taking into account other possible significant variables or, in this case, forms of dependence that can hide true underlying effects.</p>
<p>The above examples therefore highlight how the approach to analysing time series does not only rely on finding an appropriate model that describes the evolution of a variable as a function of time (which is deterministic). Indeed, the main focus of time series analysis consists in modelling the dependence structure that describes how random variables impact each other as a function of time. In other words, a time series is a collection of random variables whose interaction and dependence structure is indexed by time.</p>
<div id="the-wold-decomposition" class="section level2">
<h2><span class="header-section-number">2.1</span> The Wold Decomposition</h2>
<p>The previous discussion highlighted how a time series can be decomposed into a deterministic component and a random component. Leaving aside technical rigour, this characteristic of time series was put forward in Wold’s Decomposition Theorem who postulated that a time series <span class="math inline">\((Y_t)\)</span> (where <span class="math inline">\(t = 1,...,n\)</span> represents the time index) can be very generically represented as follows:</p>
<p><span class="math display">\[Y_t = D_t + W_t,\]</span></p>
<p>where <span class="math inline">\(D_t\)</span> represents the deterministic part (or <em>signal</em>) that can be modelled through the standard modelling techniques (e.g. linear regression) and <span class="math inline">\(W_t\)</span> that, restricting ourselves to a general class of processes, represents the random part (<em>noise</em>) that requires the analytical and modelling approaches that will be tackled in this book.</p>
<p>Typically, we have <span class="math inline">\(\mathbb{E}[Y_t] \neq 0\)</span> while <span class="math inline">\(\mathbb{E}[W_t] = 0\)</span> (although we may have
<span class="math inline">\(\mathbb{E}[W_t | W_{t-1}, ..., W_1] \neq 0\)</span>). Such models impose some parametric
structure which represents a convenient and flexible way of studying time series
as well as a means to evaluate <em>future</em> values of the series through forecasting.
As we will see, predicting future values is one of the main aspects of time
series analysis. However, making predictions is often a daunting task or as
famously stated by Nils Bohr:</p>
<blockquote>
<p>“<em>Prediction is very difficult, especially about the future.</em>”</p>
</blockquote>
<p>There are plenty of examples of predictions that turned out to be completely
erroneous. For example, three days before the 1929 crash, Irving Fisher,
Professor of Economics at Yale University, famously predicted:</p>
<blockquote>
<p>“<em>Stock prices have reached what looks like a permanently high plateau</em>”.</p>
</blockquote>
<p>Another example is given by Thomas Watson, president of IBM, who said in 1943:</p>
<blockquote>
<p>“<em>I think there is a world market for maybe five computers.</em>”</p>
</blockquote>
<p>Let us now briefly discuss the two components of a time series.</p>
<div id="the-deterministic-component-signal" class="section level3">
<h3><span class="header-section-number">2.1.1</span> The deterministic component (Signal)</h3>
<p>Before shifting our focus to the random component of time series, we will first just briefly underline the main features that should be taken into account for the deterministic component. The first feature that should be analysed is the <em>trend</em> that characterises the time series, more specifically the behaviour of the variable of interest as a specific function of time (as the global temperature time series seen earlier). Let us consider another example of time series based on real data, i.e. the quarterly earnings of Johnson &amp; Johnson between 1960 and 1980 represented below.</p>
<p><img src="ds_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>As can be seen from the plot, the earnings appear to grow over time, therefore we can imagine fitting a line to this data to describe its behaviour (see red line below).</p>
<p><img src="ds_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>Although the line captures a part of the behaviour, it is quite clear that the trend of the time series is not linear. It could therefore be more appropriate to define another function of time to describe it and, consequently, we add a quadratic term of time to obtain the following fit.</p>
<p><img src="ds_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>We can see now that the quadratic function of time allows to better fit the observed time series and closely follow the observations. However, there still appears to be a pattern in the data that isn’t captured by this quadratic model. This pattern appears to be repeated over time: peaks and valleys that seem to occur at regular intervals along the time series. This behaviour is known as <em>seasonality</em> which, in this case, can be explained by the effect of a specific quarter on the behaviour of the earnings. Indeed, it is reasonable to assume that the seasons have impacts on different variables measured over time (e.g. temperatures, earnings linked to sales that vary with seasons, etc.). Let us therefore take the quarters as an explanatory variable and add it to the previous quadratic model (see fit below).</p>
<p><img src="ds_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>This final fit appears to well describe the behaviour of the earnings. Hence, <em>trend</em> and <em>seasonality</em> are the main features that characterize the deterministic component of a time series. However, as discussed earlier, these deterministic components often don’t explain all of the observed time series since there is often a random component characterizing data measured over time. Not considering the latter component can have considerable impacts on the inference procedures (as seen earlier) and it is therefore important to adequately analyse them (see next section).</p>
</div>
<div id="the-random-component-noise" class="section level3">
<h3><span class="header-section-number">2.1.2</span> The random component (Noise)</h3>
<p>From this section onwards we will refer to <em>time series as being solely the random noise component</em>. Keeping this in mind, a <em>time series</em> is a particular kind of <em>stochastic process</em> which, generally speaking, is a collection of random variables indexed by a set of numbers. Not surprisingly, the index of reference for a time series is given by <em>time</em> and, consequently, a time series is a collection of random variables indexed (or “measured”) over time such as, for example, the daily price of a financial asset or the monthly average temperature in a given location. In terms of notation, a time series is often represented as</p>
<p><span class="math display">\[\left(X_1, X_2, ..., X_n \right) \;\;\; \text{ or } \;\;\; \left(X_t\right)_{t = 1,...,n}.\]</span></p>
<p>The time index <span class="math inline">\(t\)</span> is contained within either the set of reals, <span class="math inline">\(\mathbb{R}\)</span>, or
integers, <span class="math inline">\(\mathbb{N}\)</span>. When <span class="math inline">\(t \in \mathbb{R}\)</span>, the time series becomes a
<em>continuous-time</em> stochastic process such a Brownian motion, a model used to
represent the random movement of particles within a suspended liquid or gas. However, within this book, we will limit ourselves to the cases where <span class="math inline">\(t \in \mathbb{N}\)</span>, better known as <em>discrete-time</em> processes. <em>Discrete-time</em> processes are measured sequentially at fixed
and equally spaced intervals in time. This implies that we will uphold two general assumptions for the time series considered in this book:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(t\)</span> is not random, e.g. the time at which each observation is measured is known, and</li>
<li>the time between two consecutive observations is constant.</li>
</ol>
<p>This book will also focus on certain representations of time series based on parametric probabilistic models. For example, one of the fundamental probability models used in time series analysis is called the <em>white noise</em> model and is defined as</p>
<p><span class="math display">\[X_t \mathop \sim \limits^{iid} N(0, \sigma^2).\]</span></p>
<p>This statement simply means that <span class="math inline">\((X_t)\)</span> is normally distributed and independent over time. Ideally, this is the type of process that we would want to observe once we have performed a statistical modelling procedure. However, despite it appearing to be an excessively simple model to be considered for time series, it is actually a crucial component to construct a wide range of more complex time series models (see Chapter 2). Indeed, unlike the white noise process, time series are typically <em>not</em> independent over time. For example, if we suppose that the temperature in State College is unusually low on a given dat, then it is reasonable to assume that temperature the day after will also be low.</p>
<p>With this in mind, let us present the basic parametric models that are used to build even more complex models to describe and predict the behaviour of a time series.</p>
</div>
</div>
<div id="basicmodels" class="section level2">
<h2><span class="header-section-number">2.2</span> Basic Time Series Models</h2>
<p>In this section, we introduce some simple time series models that consitute the building blocks for the more complex and flexible classes of time series commonly used in practice. Before doing so it is useful to define <span class="math inline">\(\Omega_t\)</span> as all the information available up to time
<span class="math inline">\(t-1\)</span>, i.e.</p>
<p><span class="math display">\[\Omega_t = \left(X_{t-1}, X_{t-2}, ..., X_0 \right).\]</span></p>
<p>As we will see further on, this compact notation is quite useful.</p>
<div id="wn" class="section level3">
<h3><span class="header-section-number">2.2.1</span> White Noise</h3>
<p>As we saw earlier, the white noise model is the building block for most time series models and, to better specify the notation used throughout this book, this model is defined as</p>
<p><span class="math display">\[{W_t}\mathop \sim \limits^{iid} N\left( {0,\sigma _w^2} \right).\]</span></p>
<p>This definition implies that:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\mathbb{E}[W_t | \Omega_t] = 0\)</span> for all <span class="math inline">\(t\)</span>,</li>
<li><span class="math inline">\(\text{cov}\left(W_t, W_{t-h} \right) = \boldsymbol{1}_{h = 0} \; \sigma^2\)</span> for
all <span class="math inline">\(t, h\)</span>.</li>
</ol>
<p>More specifically, <span class="math inline">\(h \in \mathbb{N}^+\)</span> is the time difference between lagged variables. Therefore, in this process there is an absence of temporal (or serial) correlation and it is homoskedastic (i.e. it has a constant variance). Going into further details, white noise can be categorzied into two sorts of processes: <em>weak</em> and <em>strong</em>. The process <span class="math inline">\((W_t)\)</span> is
a <em>weak</em> white noise if</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\mathbb{E}[W_t] = 0\)</span> for all <span class="math inline">\(t\)</span>,</li>
<li><span class="math inline">\(\text{var}\left(W_t\right) = \sigma_w^2\)</span> for all <span class="math inline">\(t\)</span>,</li>
<li><span class="math inline">\(\text{cov} \left(W_t, W_{t-h}\right) = 0\)</span> for all <span class="math inline">\(t\)</span> and for all <span class="math inline">\(h \neq 0\)</span>.</li>
</ol>
<p>Note that this definition does not imply that <span class="math inline">\(W_t\)</span> and <span class="math inline">\(W_{t-h}\)</span> are independent (for <span class="math inline">\(h \neq 0\)</span>) but simply uncorrelated. However, the notion of independence is used to define a <em>strong</em> white noise as</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\mathbb{E}[W_t] = 0\)</span> and <span class="math inline">\(\text{var}(W_t) = \sigma^2 &lt; \infty\)</span>, for all <span class="math inline">\(t\)</span>,</li>
<li><span class="math inline">\(F(W_t) = F(W_{t-h})\)</span> for all <span class="math inline">\(t,h\)</span> (where <span class="math inline">\(F(W_t)\)</span> denotes the marginal distribution of <span class="math inline">\(W_t\)</span>),</li>
<li><span class="math inline">\(W_t\)</span> and <span class="math inline">\(W_{t-h}\)</span> are independent for all <span class="math inline">\(t\)</span> and for all <span class="math inline">\(h \neq 0\)</span>.</li>
</ol>
<p>It is clear from these definitions that if a process is a strong white noise it is also a weak white noise. However, the converse is not true as shown in the following example:</p>

<div class="example">
<p><span id="exm:weaknotstrong" class="example"><strong>(#exm:weaknotstrong) </strong></span>
Let <span class="math inline">\(Y_t \mathop \sim F_{t+2}\)</span>, where <span class="math inline">\(F_{t+2}\)</span> denotes
a Student distribution with <span class="math inline">\(t+2\)</span> degrees of freedom. Assuming the
sequence <span class="math inline">\((Y_1, \ldots, Y_n)\)</span> to be independent, we
let <span class="math inline">\(X_t = \sqrt{\frac{t}{t+2}} Y_t\)</span>. Then, the process <span class="math inline">\((X_t)\)</span> is obviously
not a strong white noise as the distribution of <span class="math inline">\(X_t\)</span> changes with <span class="math inline">\(t\)</span>. However
this process is a weak white noise since we have:</p>
<ul>
<li><span class="math inline">\(\mathbb{E}[X_t] = \sqrt{\frac{t}{t+2}} \mathbb{E}[Y_t] = 0\)</span> for all <span class="math inline">\(t\)</span>.</li>
<li><span class="math inline">\(\text{var}(X_t) = \frac{t}{t+2} \text{var}(Y_t) = \frac{t}{t+2} \frac{t+2}{t} = 1\)</span> for all <span class="math inline">\(t\)</span>.</li>
<li><span class="math inline">\(\text{cov}(X_t, X_{t+h}) = 0\)</span> (by independence), for all <span class="math inline">\(t\)</span>, and for all <span class="math inline">\(h \neq 0\)</span>.</li>
</ul>
</div>

<p>This distinction is therefore important and will be extremely relevant when discussing the concept of “stationarity” further on in this book. In general, the white noise model is assumed to be Gaussian in many practical cases and the code below presents an example of how to simulate a Gaussian white noise process.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb7-1" data-line-number="1">n =<span class="st"> </span><span class="dv">1000</span>                               <span class="co"># process length</span></a>
<a class="sourceLine" id="cb7-2" data-line-number="2">sigma2 =<span class="st"> </span><span class="dv">1</span>                             <span class="co"># process variance</span></a>
<a class="sourceLine" id="cb7-3" data-line-number="3">Xt =<span class="st"> </span><span class="kw">gen_gts</span>(n, <span class="kw">WN</span>(<span class="dt">sigma2 =</span> sigma2))</a>
<a class="sourceLine" id="cb7-4" data-line-number="4"><span class="kw">plot</span>(Xt)</a></code></pre></div>
<p><img src="ds_files/figure-html/example_WN-1.png" width="672" /></p>
<p>This model can be found in different applied settings and is often accompanied by some of the models presented in the following paragraphs.</p>
</div>
<div id="rw" class="section level3">
<h3><span class="header-section-number">2.2.2</span> Random Walk</h3>
<p>The term <em>random walk</em> was first introduced by Karl Pearson in the early
nineteen-hundreds and a wide range of random walk models have been defined over the years. For example, one of the simplest forms of a random walk process can be
explained as follows: suppose that you are walking on campus and your
next step can either be to your left, your right, forward or backward
(each with equal probability). Two realizations of such processes are
represented below:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb8-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">5</span>)</a>
<a class="sourceLine" id="cb8-2" data-line-number="2"><span class="kw">RW2dimension</span>(<span class="dt">steps =</span> <span class="dv">10</span><span class="op">^</span><span class="dv">2</span>)</a></code></pre></div>
<p><img src="ds_files/figure-html/RW2d-1.png" width="528" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb9-1" data-line-number="1"><span class="kw">RW2dimension</span>(<span class="dt">steps =</span> <span class="dv">10</span><span class="op">^</span><span class="dv">4</span>)</a></code></pre></div>
<p><img src="ds_files/figure-html/RW2d-2.png" width="528" style="display: block; margin: auto;" /></p>
<p>Such processes inspired Karl Pearson’s famous quote that</p>
<blockquote>
<p>“<em>the most likely place to find a drunken walker is somewhere near his starting point.</em>”</p>
</blockquote>
<p>Empirical evidence of this phenomenon is not too hard to find on a Friday or Saturday night. In this text, we only consider one very specific form of random walk, namely the Gaussian random walk which can be defined as:</p>
<p><span class="math display">\[X_t = X_{t-1} + W_t,\]</span></p>
<p>where <span class="math inline">\(W_t\)</span> is a Gaussian white noise process with initial condition <span class="math inline">\(X_0 = c\)</span> (typically <span class="math inline">\(c = 0\)</span>.) This process can be expressed differently by <em>backsubstitution</em> as follows:</p>
<p><span class="math display">\[\begin{aligned}
  {X_t} &amp;= {X_{t - 1}} + {W_t} \\
   &amp;= \left( {{X_{t - 2}} + {W_{t - 1}}} \right) + {W_t} \\
   &amp;= \vdots \\
  {X_t} &amp;= \sum\limits_{i = 1}^t {{W_i}} + X_0 =  \sum\limits_{i = 1}^t {{W_i}} + c \\ 
\end{aligned} \]</span></p>
<p>A random variable following a random walk can therefore be expressed as the cumulated sum of all the random variables that precede it. The code below presents an example of how to simulate a such process.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb10-1" data-line-number="1">n =<span class="st"> </span><span class="dv">1000</span>                               <span class="co"># process length</span></a>
<a class="sourceLine" id="cb10-2" data-line-number="2">gamma2 =<span class="st"> </span><span class="dv">1</span>                             <span class="co"># innovation variance</span></a>
<a class="sourceLine" id="cb10-3" data-line-number="3">Xt =<span class="st"> </span><span class="kw">gen_gts</span>(n, <span class="kw">RW</span>(<span class="dt">gamma2 =</span> gamma2))</a>
<a class="sourceLine" id="cb10-4" data-line-number="4"><span class="kw">plot</span>(Xt)</a></code></pre></div>
<p><img src="ds_files/figure-html/example_RW-1.png" width="672" /></p>
<p>The random walk model is often used to explain phenomena in many different areas one of which is finance where stock prices follow these kind of processes.</p>
</div>
<div id="ar1" class="section level3">
<h3><span class="header-section-number">2.2.3</span> First-Order Autoregressive Model</h3>
<p>A first-order autoregressive model or AR(1) is a generalization of both
the white noise and the random walk processes which are both special
cases of an AR(1). A (Gaussian) AR(1) process can be defined as</p>
<p><span class="math display">\[{X_t} = {\phi}{X_{t - 1}} + {W_t},\]</span></p>
<p>where <span class="math inline">\(W_t\)</span> is a Gaussian white noise. Clearly, an AR(1) with <span class="math inline">\(\phi = 0\)</span> is
a Gaussian white noise and when <span class="math inline">\(\phi = 1\)</span> the process becomes a random walk.</p>

<div class="remark">
<p> <span class="remark"><em>Remark. </em></span> An AR(1) is in fact a linear combination of past realisations of
a white noise <span class="math inline">\(W_t\)</span> process. Indeed, we have</p>
<p><span class="math display">\[\begin{aligned}
 {X_t} &amp;= {\phi_t}{X_{t - 1}} + {W_t} 
   = {\phi}\left( {{\phi}{X_{t - 2}} + {W_{t - 1}}} \right) + {W_t} \\
   &amp;= \phi^2{X_{t - 2}} + {\phi}{W_{t - 1}} + {W_t} 
   = {\phi^t}{X_0} + \sum\limits_{i = 0}^{t - 1} {\phi^i{W_{t - i}}}.
\end{aligned}\]</span></p>
<p>Under the assumption of infinite past (i.e. <span class="math inline">\(t \in \mathbb{Z}\)</span>) and <span class="math inline">\(|\phi| &lt; 1\)</span>,
we obtain</p>
<p><span class="math display">\[X_t = \sum\limits_{i = 0}^{\infty} {\phi^i {W_{t - i}}},\]</span></p>
since <span class="math inline">\(\operatorname{lim}_{i \to \infty} \; {\phi^i}{X_{t-i}} = 0\)</span>.
</div>

<p>From the conclusion of the above the remark, you may have noticed how we assume that the considered time series have zero expectation. The following remark justifies this assumption.</p>

<div class="remark">
<p> <span class="remark"><em>Remark. </em></span> We generally assume that an AR(1), as well as other time series
models, have zero mean. The reason for this assumption is only to simplfy the
notation but it is easy to consider, for example, an AR(1) process around an
arbitrary mean <span class="math inline">\(\mu\)</span>, i.e.</p>
<p><span class="math display">\[\left(X_t - \mu\right) = \phi \left(X_{t-1} - \mu \right) + W_t,\]</span></p>
<p>which is of course equivalent to</p>
<p><span class="math display">\[X_t = \left(1 - \phi \right) \mu + \phi X_{t-1} + W_t.\]</span></p>
Thus, we will generally only work with zero mean processes since adding means
is simple.
</div>

<p>As for the previously presented models, we provide the code that gives an example of how an AR(1) can be simulated.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb11-1" data-line-number="1">n =<span class="st"> </span><span class="dv">1000</span>                              <span class="co"># process length</span></a>
<a class="sourceLine" id="cb11-2" data-line-number="2">phi =<span class="st"> </span><span class="fl">0.5</span>                             <span class="co"># phi parameter</span></a>
<a class="sourceLine" id="cb11-3" data-line-number="3">sigma2 =<span class="st"> </span><span class="dv">1</span>                            <span class="co"># innovation variance</span></a>
<a class="sourceLine" id="cb11-4" data-line-number="4">Xt =<span class="st"> </span><span class="kw">gen_gts</span>(n, <span class="kw">AR1</span>(<span class="dt">phi =</span> phi, <span class="dt">sigma2 =</span> sigma2))</a>
<a class="sourceLine" id="cb11-5" data-line-number="5"><span class="kw">plot</span>(Xt)</a></code></pre></div>
<p><img src="ds_files/figure-html/example_AR1-1.png" width="672" /></p>
<p>The AR(1) model is one of the most popular and commonly used models in many practical settings going from biology where it is used to explain the evolution of gene expressions to economics where it is used to model macroeconomic trends.</p>
</div>
<div id="ma1" class="section level3">
<h3><span class="header-section-number">2.2.4</span> Moving Average Process of Order 1</h3>
<p>As seen in the previous example, an AR(1) can be expressed as a linear
combination of all past observations of the white noise process <span class="math inline">\((W_t)\)</span>. In a similar manner we can (in some sense) describe the moving average process of order 1 or MA(1) as a “truncated”
version of an AR(1). This model is defined as</p>
<span class="math display">\[\begin{equation} 
  X_t = \theta W_{t-1} + W_t,
\end{equation}\]</span>
<p>where (again) <span class="math inline">\(W_t\)</span> denotes a Gaussian white noise process. As we will see further on, as for the AR(1) model, this model can also be represented as a linear combination of past observations but it has different characteristics which can capture different types of dynamics in various practical cases.</p>
<p>An example on how to generate an MA(1) is given below:</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb12-1" data-line-number="1">n =<span class="st"> </span><span class="dv">1000</span>                              <span class="co"># process length</span></a>
<a class="sourceLine" id="cb12-2" data-line-number="2">sigma2 =<span class="st"> </span><span class="dv">1</span>                            <span class="co"># innovation variance</span></a>
<a class="sourceLine" id="cb12-3" data-line-number="3">theta =<span class="st"> </span><span class="fl">0.5</span>                           <span class="co"># theta parameter</span></a>
<a class="sourceLine" id="cb12-4" data-line-number="4">Xt =<span class="st"> </span><span class="kw">gen_gts</span>(n, <span class="kw">MA1</span>(<span class="dt">theta =</span> theta, <span class="dt">sigma2 =</span> sigma2))</a>
<a class="sourceLine" id="cb12-5" data-line-number="5"><span class="kw">plot</span>(Xt)</a></code></pre></div>
<p><img src="ds_files/figure-html/example_MA1-1.png" width="672" /></p>
<p>The use of this model is widespread, especially combined with the AR(1) model, and can be found in fields such as engineering where it is often used for signal processing.</p>
</div>
<div id="drift" class="section level3">
<h3><span class="header-section-number">2.2.5</span> Linear Drift</h3>
<p>A linear drift is a very simple deterministic time series model which can be
expressed as</p>
<p><span class="math display">\[X_t = X_{t-1} + \omega, \]</span></p>
<p>where <span class="math inline">\(\omega\)</span> is a constant and with the initial condition <span class="math inline">\(X_0 = c\)</span>, where <span class="math inline">\(c\)</span> is an
arbitrary constant (typically <span class="math inline">\(c = 0\)</span>). This process can be expressed in a more
familiar form as follows:</p>
<p><span class="math display">\[
  {X_t} = {X_{t - 1}} + \omega 
   = \left( {{X_{t - 2}} + \omega} \right) + \omega 
   = t{\omega} + c  \]</span></p>
<p>Therefore, a (linear) drift corresponds to a simple linear model with slope <span class="math inline">\(\omega\)</span> and intercept <span class="math inline">\(c\)</span>.</p>

<div class="remark">
 <span class="remark"><em>Remark. </em></span> You may argue that the definition of this model is not useful since it constitutes a simple linear model. However this model is often accompanied by other time series models (such as the ones presented earlier) and its estimation can be greatly improved when considered in conjunction with the other models.
</div>

<p>Given its simple form, a linear drift can simply be generated using the code below:</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb13-1" data-line-number="1">n =<span class="st"> </span><span class="dv">100</span>                               <span class="co"># process length</span></a>
<a class="sourceLine" id="cb13-2" data-line-number="2">omega =<span class="st"> </span><span class="fl">0.5</span>                           <span class="co"># slope parameter</span></a>
<a class="sourceLine" id="cb13-3" data-line-number="3">Xt =<span class="st"> </span><span class="kw">gen_gts</span>(n, <span class="kw">DR</span>(<span class="dt">omega =</span> omega))</a>
<a class="sourceLine" id="cb13-4" data-line-number="4"><span class="kw">plot</span>(Xt)</a></code></pre></div>
<p><img src="ds_files/figure-html/example_Drift-1.png" width="672" /></p>
<p>This time series model is widely used in different areas of signal analysis where mechanical systems and measuring devices can be characterized by this type of behaviour.</p>
</div>
</div>
<div id="lts" class="section level2">
<h2><span class="header-section-number">2.3</span> Composite Stochastic Processes</h2>
<p>In the previous paragraphs we defined and briefly discussed the basic time series models that can individually be used to describe and predict a wide range of phenomena in a variety of fields of application. However, their capability of capturing and explaining the different behaviours of phenomena through time increases considerably when they are combined to form so-called <em>composite models</em> (or composite processes). A composite (stochastic) process can be defined as the sum of underlying (or latent) time series models and in the rest of this book we will use the term <em>latent time series models</em> to refer to these kinds of models. A simple example of such a model is given by</p>
<p><span class="math display">\[\begin{aligned}
Y_t &amp;= Y_{t-1} + W_t + \delta\\
X_t &amp;= Y_t + Z_t,
\end{aligned}\]</span></p>
<p>where <span class="math inline">\(W_t\)</span> and <span class="math inline">\(Z_t\)</span> are two independent Gaussian white noise processes.
This model is often used as a first basis to approximate the number of
individuals in the context ecological population dynamics.
For example, suppose we want to study the population of Chamois in the Swiss Alps.
Let <span class="math inline">\(Y_t\)</span> denote the “true” number of individuals in this population at time <span class="math inline">\(t\)</span>.
It is reasonable to assume that the number of individuals at time <span class="math inline">\(t\)</span> (<span class="math inline">\(Y_t\)</span>) is (approximately) the population at the previous time <span class="math inline">\(t-1\)</span> (e.g the previous year) plus a random variation and a drift. This random variation is due to the natural randomness in ecological population
dynamics and reflects changes such as the number of predators, the abundance
of food, or weather conditions.
On the other hand, ecological <em>drift</em> is often of particular interest for ecologists as
it can be used to determine the “long” term trends of the population
(e.g. if the population is increasing, decreasing, or stable).
Of course, <span class="math inline">\(Y_t\)</span> (the number of individauls) is typically unknown and we observe
a noisy version of it, denoted as <span class="math inline">\(X_t\)</span>.
This process corresponds to the true population plus a measurement error since
some individuals may not be observed while others may have been counted several
times.
Interestingly, this process can clearly be expressed as a
<em>latent time series model</em> (or composite stochastic process) as follows:</p>
<p><span class="math display">\[\begin{aligned}
R_t &amp;= R_{t-1} + W_t \\
S_t &amp;= \delta t \\
X_t &amp;= R_t + S_t + Z_t,
\end{aligned}\]</span></p>
<p>where <span class="math inline">\(R_t\)</span>, <span class="math inline">\(S_t\)</span> and <span class="math inline">\(Z_t\)</span> denote, respectively, a random walk,
a drift, and a white noise. The code below can be used to simulate such data:</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb14-1" data-line-number="1">n =<span class="st"> </span><span class="dv">1000</span>                                <span class="co"># process length</span></a>
<a class="sourceLine" id="cb14-2" data-line-number="2">delta =<span class="st"> </span><span class="fl">0.005</span>                           <span class="co"># delta parameter (drift)</span></a>
<a class="sourceLine" id="cb14-3" data-line-number="3">sigma2 =<span class="st"> </span><span class="dv">10</span>                             <span class="co"># variance parameter (white noise)</span></a>
<a class="sourceLine" id="cb14-4" data-line-number="4">gamma2 =<span class="st"> </span><span class="fl">0.1</span>                            <span class="co"># innovation variance (random walk)</span></a>
<a class="sourceLine" id="cb14-5" data-line-number="5">model =<span class="st"> </span><span class="kw">WN</span>(<span class="dt">sigma2 =</span> sigma2) <span class="op">+</span><span class="st"> </span><span class="kw">RW</span>(<span class="dt">gamma2 =</span> gamma2) <span class="op">+</span><span class="st"> </span><span class="kw">DR</span>(<span class="dt">omega =</span> delta)</a>
<a class="sourceLine" id="cb14-6" data-line-number="6">Xt =<span class="st"> </span><span class="kw">gen_lts</span>(n, model)</a>
<a class="sourceLine" id="cb14-7" data-line-number="7"><span class="kw">plot</span>(Xt)</a></code></pre></div>
<p><img src="ds_files/figure-html/example_composite-1.png" width="672" /></p>
<p>In the above graph, the first three plots represent the latent (unobserved)
processes (i.e. white noise, random walk, and drift) and the last one represents
the sum of the three (i.e. <span class="math inline">\((X_t)\)</span>).</p>
<p>Let us consider a real example where these latent processes are useful to
describe (and predict) the behavior of economic variables such as Personal
Saving Rates (PSR). A process that is used for these settings is the
“random-walk-plus-noise” model, meaning that the data can be explained by a
random walk process in addition to which we observe some other process (e.g.
a white noise model, an autoregressive model such as an AR(1), etc.). The PSR
taken from the Federal Reserve of St. Louis from January 1, 1959, to May 1,
2015, is presented in the following plot:</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb15-1" data-line-number="1"><span class="co"># Load savingrt dataset</span></a>
<a class="sourceLine" id="cb15-2" data-line-number="2"><span class="kw">data</span>(<span class="st">&quot;savingrt&quot;</span>)</a>
<a class="sourceLine" id="cb15-3" data-line-number="3"><span class="co"># Simulate based on data</span></a>
<a class="sourceLine" id="cb15-4" data-line-number="4">savingrt =<span class="st"> </span><span class="kw">gts</span>(<span class="kw">as.vector</span>(savingrt), <span class="dt">start =</span> <span class="dv">1959</span>, <span class="dt">freq =</span> <span class="dv">12</span>, <span class="dt">unit_ts =</span> <span class="st">&quot;%&quot;</span>, </a>
<a class="sourceLine" id="cb15-5" data-line-number="5">            <span class="dt">name_ts =</span> <span class="st">&quot;Saving Rates&quot;</span>, <span class="dt">data_name =</span> <span class="st">&quot;US Personal Saving Rates&quot;</span>)</a>
<a class="sourceLine" id="cb15-6" data-line-number="6"><span class="co"># Plot savingrt simulation</span></a>
<a class="sourceLine" id="cb15-7" data-line-number="7"><span class="kw">plot</span>(savingrt)</a></code></pre></div>
<p><img src="ds_files/figure-html/example_PSR-1.png" width="672" /></p>
<p>It can be observed that the mean of this process seems to vary over time,
suggesting that a random walk can indeed be considered as a possible model
to explain this data. In addition, aside from some “spikes” and occasional
sudden changes, the observations appear to gradually change from one time point
to the other, suggesting that some other form of dependence between them could
exist.</p>
<!--chapter:end:01-intro.Rmd-->
</div>
</div>
<div id="representations-of-time-series" class="section level1">
<h1><span class="header-section-number">3</span> Representations of Time Series</h1>
<p>In this chapter we will discuss and formalize how knowledge about <span class="math inline">\(X_{t-1}\)</span> (or
more generally about all the information from the past, <span class="math inline">\(\Omega_t\)</span>) can provide
us with some information about the properties of <span class="math inline">\(X_t\)</span>. In particular, we will
consider the correlation (or covariance) of <span class="math inline">\(X_t\)</span> at different times such
as <span class="math inline">\(\text{corr} \left(X_t, X_{t+h}\right)\)</span>. This “form” of correlation (covariance) is
called the <em>autocorrelation</em> (<em>autocovariance</em>) and is a very useful tool in
time series analysis. However, if we do not assume that a time series is
characterized by a certain form of “stability”, it would be rather difficult
to estimate <span class="math inline">\(\text{corr} \left(X_t, X_{t+h}\right)\)</span> as this quantity would depend on
both <span class="math inline">\(t\)</span> and <span class="math inline">\(h\)</span> leading to more parameters to estimate than observations
available. Therefore, the concept of <em>stationarity</em> is convenient in this
context as it allows (among other things) to assume that</p>
<p><span class="math display">\[\text{corr} \left(X_t, X_{t+h}\right) = \text{corr} \left(X_{t+j}, X_{t+h+j}\right), \;\;\; \text{for all $j$},\]</span></p>
<p>implying that the autocorrelation (or autocovariance) is only a function of the
lag between observations, rather than time itself. These first of these two concepts (i.e.
autocorrelation and stationarity) will be discussed in this chapter while stationarity will be discussed in the following one. Before
moving on, it is helpful to remember that correlation (or autocorrelation) is
only appropriate to measure a very specific kind of dependence, i.e. the linear
dependence. There are many other forms of dependence as illustrated in the
bottom panels of the graph below, which all have a (true) zero correlation:</p>
<div class="figure" style="text-align: center">
<img src="images/corr_example.png" alt="Different forms of dependence and their Pearson's r values"  />
<p class="caption">
(#fig:correxample)Different forms of dependence and their Pearson’s r values
</p>
</div>
<p>Several other metrics have been introduced in the literature to assess the
degree of “dependence” of two random variables, however this goes beyond the
material discussed in this chapter.</p>
<div id="the-autocorrelation-and-autocovariance-functions" class="section level2">
<h2><span class="header-section-number">3.1</span> The Autocorrelation and Autocovariance Functions</h2>

<div class="definition">
<p><span id="def:acvf" class="definition"><strong>(#def:acvf) </strong></span>The <em>autocovariance function</em> of a series <span class="math inline">\((X_t)\)</span> is defined as</p>
<span class="math display">\[{\gamma_x}\left( {t,t+h} \right) = \text{cov} \left( {{X_t},{X_{t+h}}} \right),\]</span>
</div>

<p>where the definition of covariance is given by:</p>
<p><span class="math display">\[
    \text{cov} \left( {{X_t},{X_{t+h}}} \right) = \mathbb{E}\left[ {{X_t}{X_{t+h}}} \right] - \mathbb{E}\left[ {{X_t}} \right]\mathbb{E}\left[ {{X_{t+h}}} \right].
    \]</span></p>
<p>Similarly, the above expectations are defined to be:</p>
<p><span class="math display">\[\begin{aligned}
     \mathbb{E}\left[ {{X_t}} \right] &amp;= \int\limits_{ - \infty }^\infty  {x \cdot {f_t}\left( x \right)dx},  \\
     \mathbb{E}\left[ {{X_t}{X_{t+h}}} \right] &amp;= \int\limits_{ - \infty }^\infty  {\int\limits_{ - \infty }^\infty  {{x_1}{x_2} \cdot f_{t,t+h}\left( {{x_1},{x_2}} \right)d{x_1}d{x_2}} } ,
     \end{aligned} \]</span></p>
<p>where <span class="math inline">\({f_t}\left( x \right)\)</span> and <span class="math inline">\(f_{t,t+h}\left( {{x_1},{x_2}} \right)\)</span> denote,
respectively, the density of <span class="math inline">\(X_t\)</span> and the joint density of the
pair <span class="math inline">\((X_t, X_{t+h})\)</span>. For the notation, it should be clear that <span class="math inline">\(X_t\)</span> is assumed to be a continous random variable. Since we generally consider stochastic processes with
constant zero mean, we often have</p>
<p><span class="math display">\[{\gamma_x}\left( {t,t+h} \right) = \mathbb{E}\left[X_t X_{t+h} \right]. \]</span></p>
<p>In addition, we normally drop the subscript referring to the time
series (i.e. <span class="math inline">\(x\)</span> in this case) if it is clear from the context which time
series the autocovariance refers to. For example, we generally
use <span class="math inline">\({\gamma}\left( {t,t+h} \right)\)</span> instead
of <span class="math inline">\({\gamma_x}\left( {t,t+h} \right)\)</span>. Moreover, the notation is even further
simplified when the covariance of <span class="math inline">\(X_t\)</span> and <span class="math inline">\(X_{t+h}\)</span> is the same as that
of <span class="math inline">\(X_{t+j}\)</span> and <span class="math inline">\(X_{t+h+j}\)</span> (for all <span class="math inline">\(j\)</span>), i.e. the covariance depends only
on the time between observations and not on the specific time <span class="math inline">\(t\)</span>. This is an
important property called <em>stationarity</em>, which will be discussed in the next
section. In this case, we simply use to following notation:
<span class="math display">\[\gamma \left( {h} \right) = \text{cov} \left( X_t , X_{t+h} \right). \]</span></p>
<p>This notation will generally be used throughout the text and implies
certain properties (i.e. stationarity) on the process <span class="math inline">\((X_t)\)</span>.
Several remarks can be made on the autocovariance:</p>
<ol style="list-style-type: decimal">
<li>The autocovariance function is <em>symmetric</em>.
That is, <span class="math inline">\({\gamma}\left( {h} \right) = {\gamma}\left( -h \right)\)</span>
since <span class="math inline">\(\text{cov} \left( {{X_t},{X_{t+h}}} \right) = \text{cov} \left( X_{t+h},X_{t} \right)\)</span>.</li>
<li>The autocovariance function “contains” the variance of the process
as <span class="math inline">\(\text{var} \left( X_{t} \right) = {\gamma}\left( 0 \right)\)</span>.</li>
<li>We have that <span class="math inline">\(|\gamma(h)| \leq \gamma(0)\)</span> for all <span class="math inline">\(h\)</span>. The proof of this
inequality is direct and follows from the Cauchy-Schwarz inequality, i.e.
<span class="math display">\[ \begin{aligned}
  \left(|\gamma(h)| \right)^2 &amp;= \gamma(h)^2 = \left(\mathbb{E}\left[\left(X_t - \mathbb{E}[X_t] \right)\left(X_{t+h} - \mathbb{E}[X_{t+h}] \right)\right]\right)^2\\
  &amp;\leq \mathbb{E}\left[\left(X_t - \mathbb{E}[X_t] \right)^2 \right] \mathbb{E}\left[\left(X_{t+h} - \mathbb{E}[X_{t+h}] \right)^2 \right] =  \gamma(0)^2. 
  \end{aligned}
  \]</span></li>
<li>Just as any covariance, <span class="math inline">\({\gamma}\left( {h} \right)\)</span> is “scale dependent”
since <span class="math inline">\({\gamma}\left( {h} \right) \in \mathbb{R}\)</span>,
or <span class="math inline">\(-\infty \le {\gamma}\left( {h} \right) \le +\infty\)</span>. We therefore have:</li>
</ol>
<ul>
<li>if <span class="math inline">\(\left| {\gamma}\left( {h} \right) \right|\)</span> is “close” to zero,
then <span class="math inline">\(X_t\)</span> and <span class="math inline">\(X_{t+h}\)</span> are “weakly” (linearly) dependent;</li>
<li>if <span class="math inline">\(\left| {\gamma}\left( {h} \right) \right|\)</span> is “far” from zero,
then the two random variable present a “strong” (linear) dependence.
However it is generally difficult to asses what “close” and “far” from
zero means in this case.</li>
</ul>
<ol start="5" style="list-style-type: decimal">
<li><span class="math inline">\({\gamma}\left( {h} \right)=0\)</span> does not imply that <span class="math inline">\(X_t\)</span> and <span class="math inline">\(X_{t+h}\)</span> are
independent but simply <span class="math inline">\(X_t\)</span> and <span class="math inline">\(X_{t+h}\)</span> are uncorrelated.
The independence is only implied by <span class="math inline">\({\gamma}\left( {h} \right)=0\)</span> in
the jointly Gaussian case.</li>
</ol>
<p>As hinted in the introduction, an important related statistic is the correlation
of <span class="math inline">\(X_t\)</span> with <span class="math inline">\(X_{t+h}\)</span> or <em>autocorrelation</em>, which is defined as</p>
<p><span class="math display">\[\rho \left(  h \right) = \text{corr}\left( {{X_t},{X_{t + h}}} \right) = \frac{{\text{cov}\left( {{X_t},{X_{t + h}}} \right)}}{{{\sigma _{{X_t}}}{\sigma _{{X_{t + h}}}}}} = \frac{\gamma(h) }{\gamma(0)}.\]</span></p>
<p>Similarly to <span class="math inline">\(\gamma(h)\)</span>, it is important to note that the above notation
implies that the autocorrelation function is only a function of the
lag <span class="math inline">\(h\)</span> between observations. Thus, autocovariances and autocorrelations are one
possible way to describe the joint distribution of a time series. Indeed, the
correlation of <span class="math inline">\(X_t\)</span> with <span class="math inline">\(X_{t+1}\)</span> is an obvious measure of how <em>persistent</em> a
time series is.</p>
<p>Remember that just as with any correlation:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\rho \left( h \right)\)</span> is “scale free” so it is much easier to interpret
than <span class="math inline">\(\gamma(h)\)</span>.</li>
<li><span class="math inline">\(|\rho \left( h \right)| \leq 1\)</span> since <span class="math inline">\(|\gamma(h)| \leq \gamma(0)\)</span>.</li>
<li><strong>Causation and correlation are two very different things!</strong></li>
</ol>
<div id="a-fundamental-representation" class="section level3">
<h3><span class="header-section-number">3.1.1</span> A Fundamental Representation</h3>
<p>Autocovariances and autocorrelations also turn out to be very useful tools as
they are one of the <em>fundamental representations</em> of time series. Indeed, if we
consider a zero mean normally distributed process, it is clear that its joint
distribution is fully characterized by the
autocovariances <span class="math inline">\(\mathbb{E}[X_t X_{t+h}]\)</span> (since the joint probability density
only depends of these covariances). Once we know the autocovariances we
know <em>everything</em> there is to know about the process and therefore:</p>
<p><em>if two processes have the same autocovariance function, then they are the
same process.</em></p>
</div>
<div id="admissible-autocorrelation-functions" class="section level3">
<h3><span class="header-section-number">3.1.2</span> Admissible Autocorrelation Functions</h3>
<p>Since the autocorrelation is related to a fundamental representation of time
series, it implies that one might be able to define a stochastic process by
picking a set of autocorrelation values (assuming for example
that <span class="math inline">\(\text{var}(X_t) = 1\)</span>). However, it turns out that not every collection of
numbers, say <span class="math inline">\(\{\rho_1, \rho_2, ...\}\)</span>, can represent the autocorrelation of a
process. Indeed, two conditions are required to ensure the validity of an
autocorrelation sequence:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\operatorname{max}_j \; | \rho_j| \leq 1\)</span>.</li>
<li><span class="math inline">\(\text{var} \left[\sum_{j = 0}^\infty \alpha_j X_{t-j} \right] \geq 0 \;\)</span> for all <span class="math inline">\(\{\alpha_0, \alpha_1, ...\}\)</span>.</li>
</ol>
<p>The first condition is obvious and simply reflects the fact
that <span class="math inline">\(|\rho \left( h \right)| \leq 1\)</span> but the second is far more difficult to
verify. To further our understanding of the latter we let <span class="math inline">\(\alpha_j = 0\)</span> for <span class="math inline">\(j &gt; 1\)</span>,
then condition two implies that</p>
<p><span class="math display">\[\text{var} \left[ \alpha_0 X_{t} + \alpha_1 X_{t-1}  \right] = \gamma_0 \begin{bmatrix}
   \alpha_0 &amp; \alpha_1
   \end{bmatrix}   \begin{bmatrix}
   1 &amp; \rho_1\\
   \rho_1 &amp; 1
   \end{bmatrix} \begin{bmatrix}
   \alpha_0 \\
   \alpha_1
   \end{bmatrix} \geq 0. \]</span></p>
<p>Thus, the matrix</p>
<p><span class="math display">\[ \boldsymbol{A}_1 = \begin{bmatrix}
  1 &amp; \rho_1\\
  \rho_1 &amp; 1
  \end{bmatrix} \]</span></p>
<p>must be positive semi-definite. Taking the determinant we have</p>
<p><span class="math display">\[\operatorname{det} \left(\boldsymbol{A}_1\right) = 1 - \rho_1^2 \]</span></p>
<p>implying that the condition <span class="math inline">\(|\rho_1| \leq 1\)</span> must be respected.
Now, let <span class="math inline">\(\alpha_j = 0\)</span> for <span class="math inline">\(j &gt; 2\)</span>, then we must verify that:</p>
<p><span class="math display">\[\text{var} \left[ \alpha_0 X_{t} + \alpha_1 X_{t-1}  + \alpha_2 X_{t-2} \right] = \gamma_0 \begin{bmatrix}
     \alpha_0 &amp; \alpha_1 &amp;\alpha_2
     \end{bmatrix}   \begin{bmatrix}
     1 &amp; \rho_1 &amp; \rho_2\\
     \rho_1 &amp; 1 &amp; \rho_1 \\
     \rho_2 &amp; \rho_1 &amp; 1
     \end{bmatrix} \begin{bmatrix}
     \alpha_0 \\
     \alpha_1 \\
     \alpha_2
     \end{bmatrix} \geq 0. \]</span></p>
<p>Again, this implies that the matrix</p>
<p><span class="math display">\[ \boldsymbol{A}_2 = \begin{bmatrix}
  1 &amp; \rho_1 &amp; \rho_2\\
  \rho_1 &amp; 1 &amp; \rho_1 \\
  \rho_2 &amp; \rho_1 &amp; 1
  \end{bmatrix} \]</span></p>
<p>must be positive semi-definite and it is easy to verify that</p>
<p><span class="math display">\[\operatorname{det} \left(\boldsymbol{A}_2\right) = \left(1 - \rho_2 \right)\left(- 2 \rho_1^2 + \rho_2 + 1\right). \]</span></p>
<p>Thus, this implies that</p>
<p><span class="math display">\[\begin{aligned} &amp;- 2 \rho_1^2 + \rho_2 + 1 \geq 0 \Rightarrow 1 \geq \rho_2 \geq 2 \rho_1^2 - 1 \\
   &amp;\Rightarrow 1 - \rho_1^2 \geq \rho_2 - \rho_1^2 \geq -(1 - \rho_1^2)\\
   &amp;\Rightarrow 1 \geq \frac{\rho_2 - \rho_1^2 }{1 - \rho_1^2} \geq -1.
   \end{aligned}\]</span></p>
<p>Therefore, <span class="math inline">\(\rho_1\)</span> and <span class="math inline">\(\rho_2\)</span> must lie in a parabolic shaped region defined
by the above inequalities as illustrated in Figure @ref(fig:admissibility).</p>
<div class="figure" style="text-align: center">
<img src="ds_files/figure-html/admissibility-1.png" alt="Admissible autocorrelation functions" width="672" />
<p class="caption">
(#fig:admissibility)Admissible autocorrelation functions
</p>
</div>
<p>From our derivation, it is clear that the restrictions on the autocorrelation are
very complicated, thereby justifying the need for other forms of fundamental
representation which we will explore later in this text. Before moving on to
the estimation of the autocorrelation and autocovariance functions, we must
first discuss the stationarity of <span class="math inline">\((X_t)\)</span>, which will provide a convenient
framework in which <span class="math inline">\(\gamma(h)\)</span> and <span class="math inline">\(\rho(h)\)</span> can be used (rather that <span class="math inline">\(\gamma(t,t+h)\)</span>
for example) and (easily) estimated.</p>
</div>
</div>
<div id="estimation-of-moments" class="section level2">
<h2><span class="header-section-number">3.2</span> Estimation of Moments</h2>
<p>In this section, we discuss how moments and related quantities of stationary
process can be estimated. Informally speaking, the use of “averages” is
meaningful for such processes suggesting that classical moments estimators can
be employed. Indeed, suppose that one is interested in
estimating <span class="math inline">\(\alpha \equiv \mathbb{E}[m (X_t)]\)</span>, where <span class="math inline">\(m(\cdot)\)</span> is a known
function of <span class="math inline">\(X_t\)</span>. If <span class="math inline">\(X_t\)</span> is a strongly stationary process, we have</p>
<p><span class="math display">\[\alpha = \int m(x) \, f(x) dx\]</span></p>
<p>where <span class="math inline">\(f(x)\)</span> denotes the density of <span class="math inline">\(X_t, \; \forall t\)</span>. Replacing <span class="math inline">\(f(x)\)</span> by
<span class="math inline">\(f_n(x)\)</span>, the empirical density, we obtain the following estimator</p>
<p><span class="math display">\[\hat{\alpha} = \frac{1}{n} \sum_{i = 1}^n m\left(x_i\right).\]</span></p>
<p>In the next subsection, we examine how this simple idea can be used to estimate
the mean, autocovariance and autocorrelation functions. Moreover, we discuss
some of the properties of these estimators.</p>
<div id="estimation-of-the-mean-function" class="section level3">
<h3><span class="header-section-number">3.2.1</span> Estimation of the Mean Function</h3>
<p>If a time series is stationary, the mean function is constant and a possible
estimator of this quantity is, as discussed above, given by</p>
<p><span class="math display">\[\bar{X} = {\frac{1}{n}\sum\limits_{t = 1}^n {{X_t}} }.\]</span></p>
<p>Naturally, the <span class="math inline">\(k\)</span>-th moment, say <span class="math inline">\(\beta_k \equiv \mathbb{E}[X_t^k]\)</span> can be
estimated by</p>
<p><span class="math display">\[\hat{\beta}_k = {\frac{1}{n}\sum\limits_{t = 1}^n {{X_t^k}} }, \;\; k \in \left\{x \in \mathbb{N} : \, 0 &lt; x &lt; \infty  \right\}.\]</span></p>
<p>The variance of such an estimator can be derived as follows:</p>
<span class="math display">\[\begin{equation}
  \begin{aligned}
  \text{var} \left( \hat{\beta}_k \right) &amp;= \text{var} \left( {\frac{1}{n}\sum\limits_{t = 1}^n {{X_t^k}} } \right)  \\
  &amp;= \frac{1}{{{n^2}}}\text{var} \left( {{{\left[ {\begin{array}{*{20}{c}}
    1&amp; \cdots &amp;1
    \end{array}} \right]}_{1 \times n}}{{\left[ {\begin{array}{*{20}{c}}
      {{X_1^k}} \\
      \vdots  \\
      {{X_n^k}}
      \end{array}} \right]}_{n \times 1}}} \right)  \\
  &amp;= \frac{1}{{{n^2}}}{\left[ {\begin{array}{*{20}{c}}
    1&amp; \cdots &amp;1
    \end{array}} \right]_{1 \times n}} \, \boldsymbol{\Sigma}(k) \, {\left[ {\begin{array}{*{20}{c}}
      1 \\
      \vdots  \\
      1
      \end{array}} \right]_{n \times 1}}, 
  \end{aligned}
  (\#eq:chap2VarMoment)
\end{equation}\]</span>
<p>where <span class="math inline">\(\boldsymbol{\Sigma}(k) \in \mathbb{R}^{n \times n}\)</span> and its <span class="math inline">\(i\)</span>-th, <span class="math inline">\(j\)</span>-th
element is given by</p>
<p><span class="math display">\[ \left(\boldsymbol{\Sigma}(k)\right)_{i,j} = \text{cov} \left(X_i^k, X_j^k\right).\]</span></p>
<p>In the case <span class="math inline">\(k = 1\)</span>, @ref(eq:chap2VarMoment) can easily be further simplified.
Indeed, we have</p>
<p><span class="math display">\[\begin{aligned}
       \text{var} \left( {\bar X} \right) &amp;= \text{var} \left( {\frac{1}{n}\sum\limits_{t = 1}^n {{X_t}} } \right)  \\
       &amp;= \frac{1}{{{n^2}}}{\left[ {\begin{array}{*{20}{c}}
         1&amp; \cdots &amp;1
         \end{array}} \right]_{1 \times n}}\left[ {\begin{array}{*{20}{c}}
           {\gamma \left( 0 \right)}&amp;{\gamma \left( 1 \right)}&amp; \cdots &amp;{\gamma \left( {n - 1} \right)} \\
           {\gamma \left( 1 \right)}&amp;{\gamma \left( 0 \right)}&amp;{}&amp; \vdots  \\
           \vdots &amp;{}&amp; \ddots &amp; \vdots  \\
           {\gamma \left( {n - 1} \right)}&amp; \cdots &amp; \cdots &amp;{\gamma \left( 0 \right)}
           \end{array}} \right]_{n \times n}{\left[ {\begin{array}{*{20}{c}}
             1 \\
             \vdots  \\
             1
             \end{array}} \right]_{n \times 1}}  \\
       &amp;= \frac{1}{{{n^2}}}\left( {n\gamma \left( 0 \right) + 2\left( {n - 1} \right)\gamma \left( 1 \right) + 2\left( {n - 2} \right)\gamma \left( 2 \right) +  \cdots  + 2\gamma \left( {n - 1} \right)} \right)  \\
       &amp;= \frac{1}{n}\sum\limits_{h =  - n}^n {\left( {1 - \frac{{\left| h \right|}}{n}} \right)\gamma \left( h \right)} .  \\
\end{aligned} \]</span></p>
<p>Obviously, when <span class="math inline">\(X_t\)</span> is a white noise process, the above formula reduces to the
usual <span class="math inline">\(\text{var} \left( {\bar X} \right) = \sigma^2_w/n\)</span>. In the following example,
we consider the case of an AR(1) process and discuss
how <span class="math inline">\(\text{var} \left( {\bar X} \right)\)</span> can be obtained or estimated.</p>

<div class="example">
<p><span id="exm:exactvbootstrap" class="example"><strong>(#exm:exactvbootstrap) </strong></span>For an AR(1), we have <span class="math inline">\(\gamma(h) = \phi^h \sigma_w^2 \left(1 - \phi^2\right)^{-1}\)</span>. Therefore, we obtain (after some computations):</p>
<span class="math display">\[\begin{equation}
    \text{var} \left( {\bar X} \right) = \frac{\sigma_w^2 \left( n - 2\phi - n \phi^2 + 2 \phi^{n + 1}\right)}{n^2\left(1-\phi^2\right)\left(1-\phi\right)^2}.
\end{equation}\]</span>
<p>Unfortunately, deriving such an exact formula is often difficult when considering
more complex models. However, asymptotic approximations are often employed to
simplify the calculation. For example, in our case we have</p>
<p><span class="math display">\[\mathop {\lim }\limits_{n \to \infty } \; n \text{var} \left( {\bar X} \right) = \frac{\sigma_w^2}{\left(1-\phi\right)^2},\]</span></p>
<p>providing the following approximate formula:</p>
<p><span class="math display">\[\text{var} \left( {\bar X} \right) \approx \frac{\sigma_w^2}{n \left(1-\phi\right)^2}.\]</span></p>
Alternatively, simulation methods can also be employed. For example, a possible
strategy would be parametric bootstrap.
</div>


<div class="theorem">
<span id="thm:parabootstrap" class="theorem"><strong>(#thm:parabootstrap) </strong></span>1. Simulate a new sample under the postulated model,
i.e. <span class="math inline">\(X_t^* \sim F_{\boldsymbol{theta}}{\)</span> (<em>note:</em> if <span class="math inline">\(\boldsymbol{theta}\)</span> is unknown it can be
replace by <span class="math inline">\(\hat{\boldsymbol{theta}}\)</span>, a suitable estimator).
2. Compute the statistics of interest on the simulated
sample <span class="math inline">\((X_t^*)\)</span>.
3. Repeat Steps 1 and 2 <span class="math inline">\(B\)</span> times where <span class="math inline">\(B\)</span> is sufficiently “large”
(typically <span class="math inline">\(100 \leq B \leq 10000\)</span>).
4. Compute the empirical variance of the statistics of interest based on
the <span class="math inline">\(B\)</span> independent replications.
</div>

<p>In our example, we would consider <span class="math inline">\((X_t^*)\)</span> to be <span class="math inline">\({\bar{X}^*}\)</span> and seek to
obtain:</p>
<p><span class="math display">\[\hat{\sigma}^2_B = \frac{1}{B-1} \sum_{i = 1}^B \left(\bar{X}^*_i - \bar{X}^* \right)^2, \;\;\; \text{where} \;\;\; \bar{X}^* = \frac{1}{B} \sum_{i=1}^B \bar{X}^*_i,\]</span></p>
<p>where <span class="math inline">\(\bar{X}^*_i\)</span> denotes the value of the mean estimated on the <span class="math inline">\(i\)</span>-th
simulated sample.</p>
<p>The figure below generated by the following code compares these three methods
for <span class="math inline">\(n = 10\)</span>, <span class="math inline">\(B = 1000\)</span>, <span class="math inline">\(\sigma^2 = 1\)</span> and a grid of values for <span class="math inline">\(\phi\)</span> going
from <span class="math inline">\(-0.95\)</span> to <span class="math inline">\(0.95\)</span>:</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb16-1" data-line-number="1"><span class="co"># Define sample size</span></a>
<a class="sourceLine" id="cb16-2" data-line-number="2">n =<span class="st"> </span><span class="dv">10</span></a>
<a class="sourceLine" id="cb16-3" data-line-number="3"></a>
<a class="sourceLine" id="cb16-4" data-line-number="4"><span class="co"># Number of Monte-Carlo replications</span></a>
<a class="sourceLine" id="cb16-5" data-line-number="5">B =<span class="st"> </span><span class="dv">5000</span></a>
<a class="sourceLine" id="cb16-6" data-line-number="6"></a>
<a class="sourceLine" id="cb16-7" data-line-number="7"><span class="co"># Define grid of values for phi</span></a>
<a class="sourceLine" id="cb16-8" data-line-number="8">phi =<span class="st"> </span><span class="kw">seq</span>(<span class="dt">from =</span> <span class="fl">0.95</span>, <span class="dt">to =</span> <span class="fl">-0.95</span>, <span class="dt">length.out =</span> <span class="dv">30</span>)</a>
<a class="sourceLine" id="cb16-9" data-line-number="9"></a>
<a class="sourceLine" id="cb16-10" data-line-number="10"><span class="co"># Define result matrix</span></a>
<a class="sourceLine" id="cb16-11" data-line-number="11">result =<span class="st"> </span><span class="kw">matrix</span>(<span class="ot">NA</span>,B,<span class="kw">length</span>(phi))</a>
<a class="sourceLine" id="cb16-12" data-line-number="12"></a>
<a class="sourceLine" id="cb16-13" data-line-number="13"><span class="co"># Start simulation</span></a>
<a class="sourceLine" id="cb16-14" data-line-number="14"><span class="cf">for</span> (i <span class="cf">in</span> <span class="kw">seq_along</span>(phi)){</a>
<a class="sourceLine" id="cb16-15" data-line-number="15">  <span class="co"># Define model</span></a>
<a class="sourceLine" id="cb16-16" data-line-number="16">  model =<span class="st"> </span><span class="kw">AR1</span>(<span class="dt">phi =</span> phi[i], <span class="dt">sigma2 =</span> <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb16-17" data-line-number="17">  </a>
<a class="sourceLine" id="cb16-18" data-line-number="18">  <span class="co"># Monte-Carlo</span></a>
<a class="sourceLine" id="cb16-19" data-line-number="19">  <span class="cf">for</span> (j <span class="cf">in</span> <span class="kw">seq_len</span>(B)){</a>
<a class="sourceLine" id="cb16-20" data-line-number="20">    <span class="co"># Simulate AR(1)</span></a>
<a class="sourceLine" id="cb16-21" data-line-number="21">    Xt =<span class="st"> </span><span class="kw">gen_gts</span>(n, model)</a>
<a class="sourceLine" id="cb16-22" data-line-number="22">    </a>
<a class="sourceLine" id="cb16-23" data-line-number="23">    <span class="co"># Estimate Xbar</span></a>
<a class="sourceLine" id="cb16-24" data-line-number="24">    result[j,i] =<span class="st"> </span><span class="kw">mean</span>(Xt)</a>
<a class="sourceLine" id="cb16-25" data-line-number="25">  }</a>
<a class="sourceLine" id="cb16-26" data-line-number="26">}</a>
<a class="sourceLine" id="cb16-27" data-line-number="27"></a>
<a class="sourceLine" id="cb16-28" data-line-number="28"><span class="co"># Estimate variance of Xbar</span></a>
<a class="sourceLine" id="cb16-29" data-line-number="29">var.Xbar =<span class="st"> </span><span class="kw">apply</span>(result,<span class="dv">2</span>,var)</a>
<a class="sourceLine" id="cb16-30" data-line-number="30"></a>
<a class="sourceLine" id="cb16-31" data-line-number="31"><span class="co"># Compute theoretical variance</span></a>
<a class="sourceLine" id="cb16-32" data-line-number="32">var.theo =<span class="st"> </span>(n <span class="op">-</span><span class="st"> </span><span class="dv">2</span><span class="op">*</span>phi <span class="op">-</span><span class="st"> </span>n<span class="op">*</span>phi<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span><span class="dv">2</span><span class="op">*</span>phi<span class="op">^</span>(n<span class="op">+</span><span class="dv">1</span>))<span class="op">/</span>(n<span class="op">^</span><span class="dv">2</span><span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>phi<span class="op">^</span><span class="dv">2</span>)<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>phi)<span class="op">^</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb16-33" data-line-number="33"></a>
<a class="sourceLine" id="cb16-34" data-line-number="34"><span class="co"># Compute (approximate) variance</span></a>
<a class="sourceLine" id="cb16-35" data-line-number="35">var.approx =<span class="st"> </span><span class="dv">1</span><span class="op">/</span>(n<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>phi)<span class="op">^</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb16-36" data-line-number="36"></a>
<a class="sourceLine" id="cb16-37" data-line-number="37"><span class="co"># Compare variance estimations</span></a>
<a class="sourceLine" id="cb16-38" data-line-number="38"><span class="kw">plot</span>(<span class="ot">NA</span>, <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>), <span class="dt">ylim =</span> <span class="kw">range</span>(var.approx), <span class="dt">log =</span> <span class="st">&quot;y&quot;</span>, </a>
<a class="sourceLine" id="cb16-39" data-line-number="39">     <span class="dt">ylab =</span> <span class="kw">expression</span>(<span class="kw">paste</span>(<span class="st">&quot;var(&quot;</span>, <span class="kw">bar</span>(X), <span class="st">&quot;)&quot;</span>)),</a>
<a class="sourceLine" id="cb16-40" data-line-number="40">     <span class="dt">xlab=</span> <span class="kw">expression</span>(phi), <span class="dt">cex.lab =</span> <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb16-41" data-line-number="41"><span class="kw">grid</span>()</a>
<a class="sourceLine" id="cb16-42" data-line-number="42"><span class="kw">lines</span>(phi,var.theo, <span class="dt">col =</span> <span class="st">&quot;deepskyblue4&quot;</span>)</a>
<a class="sourceLine" id="cb16-43" data-line-number="43"><span class="kw">lines</span>(phi, var.Xbar, <span class="dt">col =</span> <span class="st">&quot;firebrick3&quot;</span>)</a>
<a class="sourceLine" id="cb16-44" data-line-number="44"><span class="kw">lines</span>(phi,var.approx, <span class="dt">col =</span> <span class="st">&quot;springgreen4&quot;</span>)</a>
<a class="sourceLine" id="cb16-45" data-line-number="45"><span class="kw">legend</span>(<span class="st">&quot;topleft&quot;</span>,<span class="kw">c</span>(<span class="st">&quot;Theoretical variance&quot;</span>,<span class="st">&quot;Bootstrap variance&quot;</span>,<span class="st">&quot;Approximate variance&quot;</span>), </a>
<a class="sourceLine" id="cb16-46" data-line-number="46">       <span class="dt">col =</span> <span class="kw">c</span>(<span class="st">&quot;deepskyblue4&quot;</span>,<span class="st">&quot;firebrick3&quot;</span>,<span class="st">&quot;springgreen4&quot;</span>), <span class="dt">lty =</span> <span class="dv">1</span>,</a>
<a class="sourceLine" id="cb16-47" data-line-number="47">       <span class="dt">bty =</span> <span class="st">&quot;n&quot;</span>,<span class="dt">bg =</span> <span class="st">&quot;white&quot;</span>, <span class="dt">box.col =</span> <span class="st">&quot;white&quot;</span>, <span class="dt">cex =</span> <span class="fl">1.2</span>)</a></code></pre></div>
<p><img src="ds_files/figure-html/estimXbar-1.png" width="672" /></p>
<p>It can be observed that the variance of <span class="math inline">\(\bar{X}\)</span> typically increases with
<span class="math inline">\(\phi\)</span>. As expected when <span class="math inline">\(\phi = 0\)</span>, we have <span class="math inline">\(\text{var}(\bar{X}) = 1/n\)</span> — in
this case the process is a white noise. Moreover, the bootstrap approach
appears to approximate well the curve of (@ref(eq:chap2_exAR1)), while the
asymptotic formula provides a reasonable approximation for <span class="math inline">\(\phi\)</span> being between
-0.5 and 0.5. Naturally, the quality of this approximation would be far better
for a larger sample size (here we consider <span class="math inline">\(n = 10\)</span>, which is a little “extreme”).</p>
</div>
<div id="sample-autocovariance-and-autocorrelation-functions" class="section level3">
<h3><span class="header-section-number">3.2.2</span> Sample Autocovariance and Autocorrelation Functions</h3>
<p>A natural estimator of the <em>autocovariance function</em> is given by:</p>
<p><span class="math display">\[\hat \gamma \left( h \right) = \frac{1}{T}\sum\limits_{t = 1}^{T - h} {\left( {{X_t} - \bar X} \right)\left( {{X_{t + h}} - \bar X} \right)} \]</span></p>
<p>leading to the following “plug-in” estimator of the <em>autocorrelation function</em>:</p>
<p><span class="math display">\[\hat \rho \left( h \right) = \frac{{\hat \gamma \left( h \right)}}{{\hat \gamma \left( 0 \right)}}.\]</span></p>
<p>A graphical representation of the autocorrelation function is often the first
step for any time series analysis (again assuming the process to be stationary).
Consider the following simulated example:</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb17-1" data-line-number="1"><span class="co"># Set seed for reproducibility</span></a>
<a class="sourceLine" id="cb17-2" data-line-number="2"><span class="kw">set.seed</span>(<span class="dv">2241</span>)</a>
<a class="sourceLine" id="cb17-3" data-line-number="3"></a>
<a class="sourceLine" id="cb17-4" data-line-number="4"><span class="co"># Simulate 100 observation from a Gaussian white noise</span></a>
<a class="sourceLine" id="cb17-5" data-line-number="5">Xt =<span class="st"> </span><span class="kw">gen_gts</span>(<span class="dv">100</span>, <span class="kw">WN</span>(<span class="dt">sigma2 =</span> <span class="dv">1</span>))</a>
<a class="sourceLine" id="cb17-6" data-line-number="6"></a>
<a class="sourceLine" id="cb17-7" data-line-number="7"><span class="co"># Compute autocorrelation</span></a>
<a class="sourceLine" id="cb17-8" data-line-number="8">acf_Xt =<span class="st"> </span><span class="kw">ACF</span>(Xt)</a>
<a class="sourceLine" id="cb17-9" data-line-number="9"></a>
<a class="sourceLine" id="cb17-10" data-line-number="10"><span class="co"># Plot autocorrelation</span></a>
<a class="sourceLine" id="cb17-11" data-line-number="11"><span class="kw">plot</span>(acf_Xt, <span class="dt">show.ci =</span> <span class="ot">FALSE</span>)</a></code></pre></div>
<p><img src="ds_files/figure-html/basicACF-1.png" width="672" /></p>
<p>In this example, the true autocorrelation is equal to zero at any
lag <span class="math inline">\(h \neq 0\)</span>, but obviously the estimated autocorrelations are random variables
and are not equal to their true values. It would therefore be useful to have
some knowledge about the variability of the sample autocorrelations (under some
conditions) to assess whether the data comes from a completely random series or
presents some significant correlation at certain lags. The following result
provides an asymptotic solution to this problem:</p>

<div class="theorem">
<span id="thm:approxnormal" class="theorem"><strong>(#thm:approxnormal) </strong></span>If <span class="math inline">\(X_t\)</span> is a strong white noise with finite fourth moment,
then <span class="math inline">\(\hat{\rho}(h)\)</span> is approximately normally distributed with mean <span class="math inline">\(0\)</span> and
variance <span class="math inline">\(n^{-1}\)</span> for all fixed <span class="math inline">\(h\)</span>.
</div>

<p>The proof of this Theorem is given in Appendix @ref(appendixa).</p>
<p>Using this result, we now have an approximate method to assess whether peaks in
the sample autocorrelation are significant by determining whether the observed
peak lies outside the interval <span class="math inline">\(\pm 2/\sqrt{T}\)</span> (i.e. an approximate 95%
confidence interval). Returning to our previous example and adding confidence
bands to the previous graph, we obtain:</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb18-1" data-line-number="1"><span class="co"># Plot autocorrelation with confidence bands </span></a>
<a class="sourceLine" id="cb18-2" data-line-number="2"><span class="kw">plot</span>(acf_Xt)</a></code></pre></div>
<p><img src="ds_files/figure-html/basicACF2-1.png" width="672" /></p>
<p>It can now be observed that most peaks lie within the
interval <span class="math inline">\(\pm 2/\sqrt{T}\)</span> suggesting that the true data generating process
is uncorrelated.</p>

<div class="example">
<span id="exm:acffeatures" class="example"><strong>(#exm:acffeatures) </strong></span>To illustrate how the autocorrelation function can be used to reveal some
“features” of a time series, we download the level of the Standard &amp; Poor’s 500
index, often abbreviated as the S&amp;P 500. This financial index is based on the
market capitalization of 500 large companies having common stock listed on
the New York Stock Exchange or the NASDAQ Stock Market. The graph below
shows the index level and daily returns from 1990.
</div>

<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb19-1" data-line-number="1"><span class="co"># Load package</span></a>
<a class="sourceLine" id="cb19-2" data-line-number="2"><span class="kw">library</span>(quantmod)</a>
<a class="sourceLine" id="cb19-3" data-line-number="3"></a>
<a class="sourceLine" id="cb19-4" data-line-number="4"><span class="co"># Download S&amp;P index</span></a>
<a class="sourceLine" id="cb19-5" data-line-number="5"><span class="kw">getSymbols</span>(<span class="st">&quot;^GSPC&quot;</span>, <span class="dt">from=</span><span class="st">&quot;1990-01-01&quot;</span>, <span class="dt">to =</span> <span class="kw">Sys.Date</span>())</a></code></pre></div>
<pre><code>## [1] &quot;GSPC&quot;</code></pre>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb21-1" data-line-number="1"><span class="co"># Compute returns</span></a>
<a class="sourceLine" id="cb21-2" data-line-number="2">GSPC.ret =<span class="st"> </span><span class="kw">ClCl</span>(GSPC)</a>
<a class="sourceLine" id="cb21-3" data-line-number="3"></a>
<a class="sourceLine" id="cb21-4" data-line-number="4"><span class="co"># Plot index level and returns</span></a>
<a class="sourceLine" id="cb21-5" data-line-number="5"><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</a>
<a class="sourceLine" id="cb21-6" data-line-number="6"><span class="kw">plot</span>(GSPC, <span class="dt">main =</span> <span class="st">&quot; &quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;Index level&quot;</span>)</a></code></pre></div>
<pre><code>## Warning in plot.xts(GSPC, main = &quot; &quot;, ylab = &quot;Index level&quot;): only the
## univariate series will be plotted</code></pre>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb23-1" data-line-number="1"><span class="kw">plot</span>(GSPC.ret, <span class="dt">main =</span> <span class="st">&quot; &quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;Daily returns&quot;</span>)</a></code></pre></div>
<p><img src="ds_files/figure-html/GSPC-1.png" width="768" /></p>
<p>From these graphs, it is clear that the returns are not identically distributed
as the variance seems to vary with time, and clusters with either high or low
volatility can be observed. These characteristics of financial time series is
well known and in the Chapter 5, we will discuss how the variance of such
process can be approximated. Nevertheless, we compute the empirical
autocorrelation function of the S&amp;P 500 return to evaluate the degree
of “linear” dependence between observations. The graph below presents
the empirical autocorrelation.</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb24-1" data-line-number="1">sp500 =<span class="st"> </span><span class="kw">na.omit</span>(GSPC.ret)</a>
<a class="sourceLine" id="cb24-2" data-line-number="2"><span class="kw">names</span>(sp500) =<span class="st"> </span><span class="kw">paste</span>(<span class="st">&quot;S&amp;P 500 (1990-01-01 - &quot;</span>,<span class="kw">Sys.Date</span>(),<span class="st">&quot;)&quot;</span>, <span class="dt">sep =</span> <span class="st">&quot;&quot;</span>)</a>
<a class="sourceLine" id="cb24-3" data-line-number="3"><span class="kw">plot</span>(<span class="kw">ACF</span>(sp500))</a></code></pre></div>
<p><img src="ds_files/figure-html/GSPCacf-1.png" width="672" /></p>
<p>As expected, the autocorrelation is small but it might be reasonable to believe
that this sequence is not purely uncorrelated.</p>
<p>Unfortunately, Theorem 1 is based on an asymptotic argument and since the
confidence bands constructed are also asymptotic, there are no “exact” tools
that can be used in this case. To study the validity of these results when <span class="math inline">\(n\)</span> is
“small” we performed a simulation. In the latter, we simulated processes
following from a Gaussian white noise and examined the empirical distribution of
<span class="math inline">\(\hat{\rho}(3)\)</span> with different sample sizes (i.e. <span class="math inline">\(n\)</span> is set to 5, 10, 30 and
300). Intuitively, the “quality” of the approximation provided by Theorem 1
should increase with the sample size <span class="math inline">\(n\)</span>. The code below performs such a simulation
and compares the empirical distribution of <span class="math inline">\(\sqrt{n} \hat{\rho}(3)\)</span> with a
normal distribution with mean 0 and variance 1 (its asymptotic
distribution), which is depicted using a red line.</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb25-1" data-line-number="1"><span class="co"># Number of Monte Carlo replications</span></a>
<a class="sourceLine" id="cb25-2" data-line-number="2">B =<span class="st"> </span><span class="dv">10000</span></a>
<a class="sourceLine" id="cb25-3" data-line-number="3"></a>
<a class="sourceLine" id="cb25-4" data-line-number="4"><span class="co"># Define considered lag</span></a>
<a class="sourceLine" id="cb25-5" data-line-number="5">h =<span class="st"> </span><span class="dv">3</span></a>
<a class="sourceLine" id="cb25-6" data-line-number="6"></a>
<a class="sourceLine" id="cb25-7" data-line-number="7"><span class="co"># Sample size considered</span></a>
<a class="sourceLine" id="cb25-8" data-line-number="8">N =<span class="st"> </span><span class="kw">c</span>(<span class="dv">5</span>, <span class="dv">10</span>, <span class="dv">30</span>, <span class="dv">300</span>)</a>
<a class="sourceLine" id="cb25-9" data-line-number="9"></a>
<a class="sourceLine" id="cb25-10" data-line-number="10"><span class="co"># Initialisation</span></a>
<a class="sourceLine" id="cb25-11" data-line-number="11">result =<span class="st"> </span><span class="kw">matrix</span>(<span class="ot">NA</span>,B,<span class="kw">length</span>(N))</a>
<a class="sourceLine" id="cb25-12" data-line-number="12"></a>
<a class="sourceLine" id="cb25-13" data-line-number="13"><span class="co"># Set seed</span></a>
<a class="sourceLine" id="cb25-14" data-line-number="14"><span class="kw">set.seed</span>(<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb25-15" data-line-number="15"></a>
<a class="sourceLine" id="cb25-16" data-line-number="16"><span class="co"># Start Monte Carlo</span></a>
<a class="sourceLine" id="cb25-17" data-line-number="17"><span class="cf">for</span> (i <span class="cf">in</span> <span class="kw">seq_len</span>(B)){</a>
<a class="sourceLine" id="cb25-18" data-line-number="18">  <span class="cf">for</span> (j <span class="cf">in</span> <span class="kw">seq_along</span>(N)){</a>
<a class="sourceLine" id="cb25-19" data-line-number="19">    <span class="co"># Simluate process</span></a>
<a class="sourceLine" id="cb25-20" data-line-number="20">    Xt =<span class="st"> </span><span class="kw">rnorm</span>(N[j])</a>
<a class="sourceLine" id="cb25-21" data-line-number="21">    </a>
<a class="sourceLine" id="cb25-22" data-line-number="22">    <span class="co"># Save autocorrelation at lag h</span></a>
<a class="sourceLine" id="cb25-23" data-line-number="23">    result[i,j] =<span class="st"> </span><span class="kw">acf</span>(Xt, <span class="dt">plot =</span> <span class="ot">FALSE</span>)<span class="op">$</span>acf[h<span class="op">+</span><span class="dv">1</span>]</a>
<a class="sourceLine" id="cb25-24" data-line-number="24">  }</a>
<a class="sourceLine" id="cb25-25" data-line-number="25">}</a>
<a class="sourceLine" id="cb25-26" data-line-number="26"></a>
<a class="sourceLine" id="cb25-27" data-line-number="27"><span class="co"># Plot results</span></a>
<a class="sourceLine" id="cb25-28" data-line-number="28"><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">2</span>,<span class="kw">length</span>(N)<span class="op">/</span><span class="dv">2</span>))</a>
<a class="sourceLine" id="cb25-29" data-line-number="29"><span class="cf">for</span> (i <span class="cf">in</span> <span class="kw">seq_along</span>(N)){</a>
<a class="sourceLine" id="cb25-30" data-line-number="30">  <span class="co"># Estimated empirical distribution</span></a>
<a class="sourceLine" id="cb25-31" data-line-number="31">  <span class="kw">hist</span>(<span class="kw">sqrt</span>(N[i])<span class="op">*</span>result[,i], <span class="dt">col =</span> <span class="st">&quot;royalblue1&quot;</span>, </a>
<a class="sourceLine" id="cb25-32" data-line-number="32">       <span class="dt">main =</span> <span class="kw">paste</span>(<span class="st">&quot;Sample size n =&quot;</span>,N[i]), <span class="dt">probability =</span> <span class="ot">TRUE</span>,</a>
<a class="sourceLine" id="cb25-33" data-line-number="33">       <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="op">-</span><span class="dv">4</span>,<span class="dv">4</span>), <span class="dt">xlab =</span> <span class="st">&quot; &quot;</span>)</a>
<a class="sourceLine" id="cb25-34" data-line-number="34">  </a>
<a class="sourceLine" id="cb25-35" data-line-number="35">  <span class="co"># Asymptotic distribution</span></a>
<a class="sourceLine" id="cb25-36" data-line-number="36">  xx =<span class="st"> </span><span class="kw">seq</span>(<span class="dt">from =</span> <span class="dv">-10</span>, <span class="dt">to =</span> <span class="dv">10</span>, <span class="dt">length.out =</span> <span class="dv">10</span><span class="op">^</span><span class="dv">3</span>)</a>
<a class="sourceLine" id="cb25-37" data-line-number="37">  yy =<span class="st"> </span><span class="kw">dnorm</span>(xx,<span class="dv">0</span>,<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb25-38" data-line-number="38">  <span class="kw">lines</span>(xx,yy, <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>, <span class="dt">lwd =</span> <span class="dv">2</span>)</a>
<a class="sourceLine" id="cb25-39" data-line-number="39">}</a></code></pre></div>
<p><img src="ds_files/figure-html/simulationACF-1.png" width="672" /></p>
<p>As expected, it can clearly be observed that the asymptotic approximation is
quite poor when <span class="math inline">\(n = 5\)</span> but as the sample size increases the approximation
improves and is very close when, for example, <span class="math inline">\(n = 300\)</span>. This simulation could
suggest that Theorem 1 provides a relatively “close” approximation of the
distribution of <span class="math inline">\(\hat{\rho}(h)\)</span>.</p>
</div>
<div id="robustness-issues" class="section level3">
<h3><span class="header-section-number">3.2.3</span> Robustness Issues</h3>
<!-- Rob I am sure you would be great to extent this section! I add a small simulation as an example -->
<p>The data generating process delivers a theoretical autocorrelation
(autocovariance) function that, as explained in the previous section,
can then be estimated through the sample autocorrelation (autocovariance)
functions. However, in practice, the sample is often issued from a data
generating process that is “close” to the true one, meaning that the sample
suffers from some form of small contamination. This contamination is typically
represented by a small amount of extreme observations that are called “outliers”
that come from a process that is different from the true data generating process.</p>
<p>The fact that the sample can suffer from outliers implies that the standard
estimation of the autocorrelation (autocovariance) functions through the sample
functions could be highly biased. The standard estimators presented in the
previous section are therefore not “robust” and can behave badly when the sample
suffers from contamination. To illustrate this limitation of a classical estimator,
we consider the following two processes:</p>
<p><span class="math display">\[ 
    \begin{aligned}
    X_t &amp;= \phi X_{t-1} + W_t, \;\;\; W_t \sim \mathcal{N}(0,\sigma_w^2),\\
    Y_t &amp;= \begin{cases}
    X_t       &amp; \quad \text{with probability } 1 - \epsilon\\
    U_t  &amp; \quad \text{with probability } \epsilon\\
    \end{cases}, \;\;\; U_t \sim \mathcal{N}(0,\sigma_u^2),
    \end{aligned}
\]</span></p>
<p>when <span class="math inline">\(\epsilon\)</span> is “small” and <span class="math inline">\(\sigma_u^2 \gg \sigma_w^2\)</span>, the process <span class="math inline">\((Y_t)\)</span>
can be interpreted as a “contaminated” version of <span class="math inline">\((X_t)\)</span>. The figure below
represents one relalization of the processes <span class="math inline">\((X_t)\)</span> and <span class="math inline">\((Y_t)\)</span> using the
following setting: <span class="math inline">\(n = 100\)</span>, <span class="math inline">\(\sigma_u^2 = 10\)</span>, <span class="math inline">\(\phi = 0,5\)</span>, <span class="math inline">\(\sigma_w^2 = 1\)</span> as
well as <span class="math inline">\(\alpha = 0.05\)</span>.</p>
<p>Next, we consider a simulated example to highlight how the performance of a
“classical” autocorrelation can deteriorate if the sample is contaminated (
i.e. what is the impact of using <span class="math inline">\(Y_t\)</span> instead of <span class="math inline">\(X_t\)</span>, the “uncontaminated”
process). In this simulation, we will use the setting presented above and
consider <span class="math inline">\(B = 10^3\)</span> bootstrap replications.</p>
<p>The boxplots in each figure show how the standard autocorrelation estimator is
centered around the true value (red line) when the sample is not
contaminated (left boxplot) while it is considerably biased when the sample is
contaminated (right boxplot), especially at the smallest lags. Indeed, it can
be seen how the boxplots under contamination are often close to zero indicating
that it does not detect much dependence in the data although it should. This is
a known result in robustness, more specifically that outliers in the data can
break the dependence structure and make it more difficult for the latter to be
detected.</p>
<p>In order to limit this problem, different robust estimators exist for time
series problems which are designed to reduce contamination during
the estimation procedure. Among these estimators, there are a few that estimate the
autocorrelation (autocovariance) functions in a robust manner. One of these
estimators is provided in the <code>robacf()</code> function in the “robcor” package.
The following simulated example shows how it limits bias from contamination.
Unlike in the previous simulation, we shall only consider data issued
from the contaminated model, <span class="math inline">\(Y_t\)</span>, and compare the performance of two
estimators (i.e. classical and robust autocorrelation estimators):</p>
<p>The robust estimator remains close to the true value represented by the red line
in the boxplots as opposed to the standard estimator. It can also be observed
that to reduce the bias induced by contamination in the sample, robust
estimators pay a certain price in terms of efficiency as highlighted by the
boxplots that show more variability compared to those of the standard estimator.
To assess how much is “lost” by the robust estimator compared to the classical
one in terms of efficiency, we consider one last simulation where we examine
the performance of two estimators on data issued from the uncontaminated model,
i.e. <span class="math inline">\((X_t)\)</span>. Therefore, the only difference between this simulation and the
previous one is the value of <span class="math inline">\(\alpha\)</span> set equal to <span class="math inline">\(0\)</span>; the code shall thus be omitted
and the results are depicted below:</p>
<p>It can be observed that both estimators provide extremely similar results,
although the robust estimator is slightly more variable.</p>
<p>Next, we consider the issue of robustness on the real data set coming from the
domain of hydrology presented in Section @ref(eda). This data concerns monthly
precipitation (in mm) over a certain period of time (1907 to 1972). Let us
compare the standard and robust estimators of the autocorrelation functions:</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb26-1" data-line-number="1"><span class="co"># TO DO</span></a></code></pre></div>
<p>It can be seen that, under certain assumptions (e.g. linear dependence), the
standard estimator does not detect any significant autocorrelation between lags
since the estimations all lie within the asymptotic confidence intervals. However,
many of the robust estimations lie outside these confidence intervals at different
lags indicating that there could be dependence within the data. If one were only to
rely on the standard estimator in this case, there may be erroneous conclusions
drawn on this data. Robustness issues therefore need to be considered for any
time series analysis, not only when estimating the
autocorrelation (autocovariance) functions.</p>
<p>Finally, we return to S&amp;P 500 returns and compare the classical and robust
autocorrelation estimators, which are presented in the figure below.</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb27-1" data-line-number="1"><span class="co"># TO DO</span></a></code></pre></div>
<p>It can be observed that both estimators are very similar. Nevertheless, some
small discrepancies can be observed. In particular, the robust estimators seem
to indicate an absence of linear dependence while a slightly different
interpretation might be achieved with the classical estimator.</p>
</div>
<div id="sample-cross-covariance-and-cross-correlation-functions" class="section level3">
<h3><span class="header-section-number">3.2.4</span> Sample Cross-Covariance and Cross-Correlation Functions</h3>
<p>A natural estimator of the <em>cross-covariance function</em> is given by:</p>
<p><span class="math display">\[{{\hat \gamma }_{XY}}\left( h \right) = \frac{1}{T}\sum\limits_{t = 1}^{T - h} {\left( {{X_{t + h}} - \bar X} \right)\left( {{Y_t} - \bar Y} \right)} \]</span></p>
<p>With this in mind, the “plug-in” estimator for the <em>cross-correlation function</em>
follows:</p>
<p><span class="math display">\[{{\hat \rho }_{XY}}\left( h \right) = \frac{{{{\hat \gamma }_{XY}}\left( h \right)}}{{\sqrt {{{\hat \gamma }_X}\left( 0 \right)} \sqrt {{{\hat \gamma }_Y}\left( 0 \right)} }}\]</span></p>
<p>Both of the above estimators are again only symmetric under the above index
and lag transformation.</p>
<!--chapter:end:02-fundamental_rep.Rmd-->
</div>
</div>
</div>
<!--bookdown:body:end-->
            </section>

          </div>
        </div>
      </div>
<!--bookdown:link_prev-->
<!--bookdown:link_next-->
    </div>
  </div>
<!--bookdown:config-->

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
