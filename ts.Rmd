--- 
title: "Applied Time Series Analysis with R"
author: "Stéphane Guerrier, Roberto Molinari and Haotian Xu"
date: "`r format(Sys.time(), '%B %d %Y')`"
site: bookdown::bookdown_site
output: bookdown::gitbook
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: "acm"
link-citations: yes
github-repo: SMAC-Group/ts
description: ""
favicon: "favicon.ico"
---

# (PART) Foundation {-}

# Introduction

Welcome to "Applied Time Series Analysis with `R`". This book is intended as a support for the course of STAT 463 (Applied Time Series Analysis) given at Penn State University. It contains an overview of the basic procedures to adequately approach a time series analysis with insight to more advanced analysis of time series. It firstly introduces the basic concepts and theory to appropriately use the applied tools that are presented in the second (and main) part of the book. In the latter part the reader will learn how to use descriptive analysis to identify the important characteristics of a time series and then employ modelling and inference techniques (made available through `R` funtions) that allow to describe a time series and make predictions. The last part of the book will give introductory notions on more advanced analysis of time series where the reader will achieve a basic understanding of the tools available to analyse more complex characteristics of time series. 

<!--
This document is under active development and as a result is likely to contains
many errors. As Montesquieu puts it:

>
> "*La nature semblait avoir sagement pourvu à ce que les sottises des hommes 
> fussent passagères, et les livres les immortalisent.*"
>

-->

```{block2,  type='rmdimportant'}
This document is **under development** and it is therefore preferable to always access the text online to be sure you are using the most up-to-date version. Due to its current development, you may encounter errors ranging from broken code to typos or poorly explained topics. If you do, please let us know! Simply add an issue to the GitHub repository used for this document (which can be accessed here https://github.com/SMAC-Group/ts/issues) and we will make the changes as soon as possible. In addition, if you know RMarkdown and are familiar with GitHub, make a pull request and fix an issue yourself.
```

<!-- ## Contents -->

<!-- This book is structured as follows: -->

<!-- - Basic Elements of Time Series -->
<!--     - Wold representation deterministic + random -->
<!--     - Examples of deterministic components (trend + seasonality) -->
<!--     - Random components: basic time series models -->
<!-- - Fundamental Representations -->
<!--     - Conditions for fundamental representations (e.g. gaussian) -->
<!--     - AutoCovariance and AutoCorrelation Functions -->
<!--     - Estimators: Empirical ACF -->
<!--     - Spectral Density and WV -->
<!-- - Stationarity of Time Series -->
<!--     - Stationarity vs Non-Stationarity -->
<!--     - Linear operators and processes -->
<!--     - Weak and Strong Stationarity -->
<!-- - SARIMA Models -->
<!--     - AR(p) Models -->
<!--     - MA(q) Models -->
<!--     - ARMA(p,q) Models -->
<!--     - ARIMA(p,d,q) Models -->
<!--     - SARIMA(p,d,q)(P,D,Q) Models -->
<!-- - Descriptive Analysis -->
<!--     - Raw Data -->
<!--     - ACF plots -->
<!--     - Identifying models -->
<!--     - Other representations: SDF and WV -->
<!-- - Inference -->
<!--     - Estimation -->
<!--     - Inference -->
<!--     - Model Selection -->
<!-- - Advanced Topics -->
<!--     - GARCH -->
<!--     - State-Space Models -->
<!--     - Multivariate (VAR) Models -->


## Conventions

Throughout this book, `R` code will be typeset using a `monospace` font which is syntax highlighted. For example:

```{r, eval = FALSE}
a = pi
b = 0.5
sin(a*b)
```

Similarly, `R` output lines (that usally appear in your Console) will begin with `##` and will not be syntax highlighted. The output of the above example is the following:

```{r, echo = FALSE}
a = pi
b = 0.5
sin(a*b)
```

Aside from `R` code and its outputs, this book will also insert some boxes that will draw the reader's attention to  important, curious or otherwise informative details. An example of these boxes was seen at the beginning of this introduction where an important aspect was pointed out to the reader regarding the "under construction" nature of this book. Therefore the following boxes and symbols can be used to represent information of different nature:

```{block2,  type='rmdimportant'}
This is an important piece of information.
```

```{block2, type='rmdnote'}
This is some additional information that could be useful to the reader.
```

```{block2, type='rmdcaution'}
This is something that the reader should pay caution to but should not create major problems if not considered. 
```

```{block2, type='rmdwarning'}
This is a warning which should be heeded by the reader to avoid problems of different nature.
```

```{block2, type='rmdtip'}
This is a tip for the reader when following or developing something based on this book.
```

Using the same convention as in @friedman2001elements, the symbol `r emo::ji("scream")` indicates a technically difficult section which may be skipped without interrupting the flow of the discussion.

## Bibliographic Note 

This is not the first (or the last) book that has been written on time series analysis. Indeed, this can be seen as a book that brings together and reorganizes information and material from other sources structuring and tailoring it to a course in basic time series analysis. The main and excellent references (which are far from being an exhaustive review of literature) that can be used to have a more in-depth view of different aspects treated in this book are @cochrane2005time, @hamilton1994time and @shumway2010time.

## Acknowledgements

The text has benefited greatly from the contributions of many people who have provided extremely useful comments, suggestions and corrections. These are:

- [Ziying Wang](https://github.com/zionward)
- [Haoxian Zhong](https://github.com/Lyle-Haoxian)
- [Zhihan Xiong](https://www.linkedin.com/in/zhihan-xiong-988152114)
- [Nathanael Claussen](https://github.com/Nathanael-Claussen)
- [Justin Lee](https:://github.com/munsheet)

The authors are particularly grateful to James Balamuta who introduced them to the use of the different tools provided by the RStudio environment and greatly contributed to an earlier version of this book:

- [James Balamuta](https::/github.com/coatless)


## License 

You can redistribute it and/or modify this book under the terms of the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License (CC BY-NC-SA) 4.0 License.

<a href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img src="images/licence.png" align="left" width="200"/></a>
<br><br><br>

<!--chapter:end:index.Rmd-->

```{r, echo=FALSE, include=FALSE}
library(astsa)
library(mgcv)
library(simts)
library(imudata)
library(timeDate)
```

```{r introduction_code, echo = FALSE, warning=FALSE, message=FALSE}
library(astsa)
library(imudata)
library(simts)
```

# Basic Elements of Time Series 

>
> "*Prévoir consiste à projeter dans l’avenir ce qu’on a perçu dans le passé.*" – Henri Bergson
>

```{block2,  type='rmdimportant'}
To make use of the R code within this chapter you will need to install (if not already done) and load the following libraries:

  - [simts](http://simts.smac-group.com/);
  - [astsa](https://cran.r-project.org/web/packages/astsa/index.html);
  - [mgcv](https://cran.r-project.org/web/packages/mgcv/index.html).
  
These libraries can be install as follows:
```

```{r, eval=FALSE}
install.packages(c("devtools", "astsa", "mgcv"))
devtools::install_github("SMAC-Group/simts")
```

and simply load them using:

```{r, eval=FALSE}
library(astsa)
library(mgcv)
library(simts)
```

We can start the discussion on the basic elements of time series by using a practical example from real data made available through the `R` software. The data represent the global mean land–ocean temperature shifts from 1880 to 2015 (with base index being the average temperatures from 1951 to 1980) and this time series is represented in the plot below.

```{r glotempExample, fig.height = 4.5, fig.width = 7, cache = TRUE, fig.align='center'}
# Load data
data(globtemp, package = "astsa")

# Construct gts object
globtemp = gts(globtemp, start = 1880, freq = 1, unit_ts = "C", name_ts = "Global Temperature Deviations", data_name = "Evolution of Global Temperatures")

# Plot time series
plot(globtemp)
```

These data have been used as a support in favour of the argument that the global temperatures are increasing and that global warming has occured over the last half of the twentieth century. The first approach that one would take is to try and measure the average increase by fitting a model having the form:

$$
X_t = f(t) + \varepsilon_t,
$$
where $X_t$ denotes the global temperatures deviation and $f(\cdot)$ is a "smooth" function such that $\mathbb{E}[X_t] - f(t) = 0$ for all $t$. In general, $\varepsilon_t$ is assumed to follow a normal distribution for simplicity. The goal in this context would therefore be to evaluate if $f(t)$ (or a suitable estimator of this function) is an increasing function (especially over the last decades). In order to do so, we would require the residuals from the fitted model to be independently and identically distributed (iid). Let us fit a (nonparametric) model with the years (time) as explanatory variable using the code below:

```{r fitGam}
time = gts_time(globtemp)
fit = gam(globtemp ~ s(time))
```

and check the residuals from this model using:
 
```{r gamresid, fig.height = 5.5, fig.width = 6.5, fig.align='center'}
simts::simple_diag_plot(globtemp, fit)
```

It can be seen from the upper left plot that the trend appears to be removed and, if looking at the residuals as one would usually do in a regression framework, the residual plots seem to suggest that the modelling has done a relatively good job since no particular pattern seems to emerge and their distribution is quite close to being Gaussian.

However, is it possible to conclude from the plots that the data are *iid* (i.e. independent and identically distributed)? More specifically, can we assume that the residuals are independent? This is a fundamental question in order for inference procedures to be carried out in an appropriate manner and to limit false conclusions. Let us provide an example through a simulated data set where we know that there is an upward trend through time and our goal would be to show that this trend exists. In order to do so we consider a simple model where $f(t)$ has a simple parametric form, i.e. $f(t) = \beta \cdot t$ and we employ the following data generating process:

$$X_t = \beta \cdot t + Y_t,$$
where
$$Y_t = \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + \varepsilon_t,$$
and where $\varepsilon_t \sim \mathcal{N}(0, \sigma^2)$. Intuitively, $Y_t$ is not an *iid* sequence of random variables except in the case where $\phi_1 = \phi_2 = 0$. In the following chapters we shall see that this intuition is correct and that this model is known as an AR(2) model. Considering this, we simulate two cases where, in the first, the residuals are actually *iid* Gaussian while, in the second, the residuals are Gaussian but are dependent over time. In the first case, the only parameters that explain $X_t$ are $\beta = 5 \cdot 10^{-3}$ and $\sigma^2 = 1$ since the residuals $Y_t$ are *iid* (i.e. $\phi_1 = \phi_2 = 0$). In the second case however, aside from the mentioned parameters we also have $\phi_1 = 0.8897$, $\phi_2 = -0.4858$. In both cases, we perform the hypothesis test:

$$
\begin{aligned}
\text{H}_0:& \;\;\; \beta = 0\\
\text{H}_1:& \;\;\; \beta > 0
\end{aligned}
$$
as our hope is to prove, similarly to the global temperature deviation example, that $f(t)$ is an increasing function. Our syntetic data are simulated as follows:

```{r}
# Set seed for reproducibility
set.seed(9)

# Define sample size
n = 100

# Define beta
beta = 0.005

# Define sigma2
sigma2 = 1

# Simulation of Yt
Yt_case1 = gen_gts(WN(sigma2 = sigma2), n = n)
Yt_case2 = gen_gts(AR(phi = c(0.95, -0.5), sigma2 = sigma2), n = n)

# Define explanatory variable (time)
time = 1:n

# Simulation of Xt
Xt_case1 = beta*time + Yt_case1
Xt_case2 = beta*time + Yt_case2

# Fit a linear models
model1 <- lm(Xt_case1 ~ time + 0)
model2 <- lm(Xt_case2 ~ time + 0)
```

The "summary" of our model on the first dataset is given by

```{r}
summary(model1)
```

As can be seen, in the first case the estimated slope ($\approx$ `r round(model1$coefficients[1], 3)`) is close to the true slope (0.005) and is significant (i.e. the p-value is smaller than the common rejection level 0.05) since the p-value of the above mentioned test is given by `r round(summary(model1)$coefficients[4]/2, 4)`. Hence, from this inference procedure we can conclude at the 5% significance level that the slope is significantly larger than zero and is roughly equal to `r round(model1$coefficients[1], 3)` (which is relatively close to the truth). However, let us perform the same analysis when the residuals are not independent (the second case) by examining its "summary":

```{r}
summary(model2)
```

In this case we can observe that the p-value of the above mentioned test is given by `r round(summary(model2)$coefficients[4]/2, 4)` and is therefore greater than the arbitrary value of 0.05. Consequently, we don't have evidence to conclude that the slope coefficient is larger than zero (i.e. we fail to reject H$_0$) although it is actually so in reality. Therefore, the inference procedures can be misleading when not taking into account other possible significant variables or, in this case, forms of dependence that can hide true underlying effects. The above is only one example and there are therefore cases where, despite dependence in the residuals, the estimated slope would be deemed significant even when not considering this dependence structure. However, if we decided to repeat this experiment using a larger quantity of simulated samples, we would probably see that we fail to reject the null hypothesis much more frequently in the case where we don't consider dependence when there actually is. 

These examples therefore highlight how the approach to analysing time series does not only rely on finding an appropriate model that describes the evolution of a variable as a function of time (which is deterministic). Indeed, one of the main focuses of time series analysis consists in modelling the dependence structure that describes how random variables impact each other as a function of time. In other words, a time series is a collection of random variables whose interaction and dependence structure is indexed by time. Based on this structure, one of the main goals of time series analysis is to correctly estimate the dependence mechanism and consequently deliver forecasts that are as accurate as possible considering the deterministic functions of time (and other variables) as well as the random dependence structure.

## The Wold Decomposition

The previous discussion highlighted how a time series can be decomposed into a deterministic component and a random component. Leaving aside technical rigour, this characteristic of time series was put forward in Wold's Decomposition Theorem who postulated that a time series $(Y_t)$ (where $t = 1,...,n$ represents the time index) can be very generically represented as follows:

$$Y_t = D_t + W_t,$$

where $D_t$ represents the deterministic part (or *signal*) that can be modelled through the standard modelling techniques (e.g. linear regression) and $W_t$ that, restricting ourselves to a general class of processes, represents the random part (*noise*) that requires the analytical and modelling approaches that will be tackled in this book.
 
Typically, we have $\mathbb{E}[Y_t] \neq 0$ while $\mathbb{E}[W_t] = 0$ (although we may have
$\mathbb{E}[W_t | W_{t-1}, ..., W_1] \neq 0$). Such models impose some parametric
structure which represents a convenient and flexible way of studying time series
as well as a means to evaluate *future* values of the series through forecasting.
As we will see, predicting future values is one of the main aspects of time
series analysis. However, making predictions is often a daunting task or as
famously stated by Nils Bohr:

> 
> "*Prediction is very difficult, especially about the future.*"
>

There are plenty of examples of predictions that turned out to be completely
erroneous. For example, three days before the 1929 crash, Irving Fisher,
Professor of Economics at Yale University, famously predicted:

>
> "*Stock prices have reached what looks like a permanently high plateau*". 
>

Another example is given by Thomas Watson, president of IBM, who said in 1943:

>
> "*I think there is a world market for maybe five computers.*"
>

Let us now briefly discuss the two components of a time series.


### The Deterministic Component (Signal)

Before shifting our focus to the random component of time series, we will first just underline the main features that should be taken into account for the deterministic component. The first feature that should be analysed is the *trend* that characterises the time series, more specifically the behaviour of the variable of interest as a specific function of time (as the global temperature time series seen earlier). Let us consider another example borrowed from @shumway2010time of time series based on real data, i.e. the quarterly earnings of Johnson & Johnson between 1960 and 1980 represented below.

```{r jjexample, fig.height = 4.5, fig.width = 7, cache = TRUE, fig.align='center'}
# Load data
data(jj, package = "astsa")

# Construct gts object
jj = gts(jj, start = 1960, freq = 4, unit_ts = "$", name_ts = "Quarterly Earnings per Share", data_name = "Johnson & Johnson Quarterly Earnings")

# Plot time series
plot(jj)
```

As can be seen from the plot, the earnings appear to grow over time, therefore we can imagine fitting a straight line to this data to describe its behaviour by considering the following model:

\begin{equation} 
X_t = \alpha + \beta t + \varepsilon_t,
(\#eq:modeljjexample)
\end{equation} 

where $\varepsilon_t$ is iid Gaussian. The results are presented in the graph below:

```{r jjexample2, fig.height = 4.5, fig.width = 7, cache = TRUE, fig.align='center'}
# Fit linear regression
time_jj = gts_time(jj)
fit_jj1 = lm(as.vector(jj) ~ time_jj)

# Plot results and add regression line
plot(jj)
lines(time_jj, predict(fit_jj1), col = "red")
legend("bottomright", c("Time series", "Regression line"), 
       col = c("blue4", "red"), bty = "n", lwd = 1)
```

Although the line captures a part of the behaviour, it is quite clear that the trend of the time series is not linear as can be observed from the diagnotic plot below:

```{r lmresid, fig.height = 5.5, fig.width = 6.5, fig.align='center'}
simple_diag_plot(jj, fit_jj1)
```

It could therefore be more appropriate to define another function of time to describe it and, consequently, we add a quadratic term of time to obtain the following fit. Therefore, the model considered in \@ref(eq:modeljjexample) becomes:

\begin{equation} 
X_t = \alpha + \beta_1 t + \beta_2 t^2 + \varepsilon_t,
(\#eq:modeljjexample2)
\end{equation} 

The results of this regression are presented on the graphs below:

```{r, echo=FALSE, fig.height = 4.5, fig.width = 7, cache = TRUE, fig.align='center'}
fit_jj2 <- lm(as.vector(jj) ~ time_jj + I(time_jj^2))
plot(jj)
lines(time_jj, predict(fit_jj2), col = "red")
legend("bottomright", c("Time series", "Regression line"), 
       col = c("blue4", "red"), bty = "n", lwd = 1)
```

```{r lmresid2, fig.height = 5.5, fig.width = 6.5, fig.align='center'}
simple_diag_plot(jj, fit_jj2)
```

We can see now that the quadratic function of time allows to better fit the observed time series and closely follow the observations. However, there still appears to be a pattern in the data that isn't captured by this quadratic model. This pattern appears to be repeated over time: peaks and valleys that seem to occur at regular intervals along the time series. This behaviour is known as *seasonality* which, in this case, can be explained by the effect of a specific quarter on the behaviour of the earnings. Indeed, it is reasonable to assume that the seasons have impacts on different variables measured over time (e.g. temperatures, earnings linked to sales that vary with seasons, etc.). Let us therefore take the quarters as an explanatory variable and add it to the model considered in \@ref(eq:modeljjexample2), which becomes:

\begin{equation} 
X_t = \alpha + \beta_1 t + \beta_2 t^2 + \sum_{i = 1}^4 \gamma_i I_{t \in \mathcal{A}_i} + \varepsilon_t,
(\#eq:modeljjexample3)
\end{equation}

where

\begin{equation*}
  I_{t \in \mathcal{A}} \equiv \left\{
	\begin{array}{ll}
		1  & \mbox{if } t \in \mathcal{A} \\
		0 & \mbox{if } t \not\in \mathcal{A}
	\end{array}
\right. ,
\end{equation*}

and where

$$
\mathcal{A}_i \equiv \left\{x \in \mathbb{N} | x = i \; \text{mod} \;  4\right\}.
$$

The results are presented below:

```{r, echo=FALSE, fig.align='center'}
quarters = rep(1:4, 21) 
fit_jj3 <- gam(as.vector(jj) ~ quarters + s(time_jj))
plot(jj)
lines(time_jj, predict(fit_jj3), col = "red")
legend("bottomright", c("Time series", "Regression line"), 
       col = c("blue4", "red"), bty = "n", lwd = 1)
```

```{r lmresid3, fig.height = 5.5, fig.width = 6.5, fig.align='center'}
simple_diag_plot(jj, fit_jj3)
```

This final fit appears to well describe the behaviour of the earnings although there still appears to be a problem of heteroskedasticity (i.e. change in variance) and random seasonality (both of which will be treated further on in this text). Hence, *trend* and *seasonality* are the main features that characterize the deterministic component of a time series. However, as discussed earlier, these deterministic components often don't explain all of the observed time series since there is often a random component characterizing data measured over time. Not considering the latter component can have considerable impacts on the inference procedures (as seen earlier) and it is therefore important to adequately analyse them (see next section).

### The Random Component (Noise)

From this section onwards we will refer to *time series as being solely the random noise component*. Keeping this in mind, a *time series* is a particular kind of *stochastic process* which, generally speaking, is a collection of random variables indexed by a set of numbers. Not surprisingly, the index of reference for a time series is given by *time* and, consequently, a time series is a collection of random variables indexed (or "measured") over time such as, for example, the daily price of a financial asset or the monthly average temperature in a given location. In terms of notation, a time series is often represented as

 \[\left(X_1, X_2, ..., X_T \right) \;\;\; \text{ or } \;\;\; \left(X_t\right)_{t = 1,...,T}.\]
 
The time index $t$ is contained within either the set of reals, $\mathbb{R}$, or
integers, $\mathbb{Z}$. When $t \in \mathbb{R}$, the time series becomes a
*continuous-time* stochastic process such as a Brownian motion, a model used to
represent the random movement of particles within a suspended liquid or gas. However, within this book, we will limit ourselves to the cases where $t \in \mathbb{Z}$, better known as *discrete-time* processes. Discrete-time processes are measured sequentially at fixed
and equally spaced intervals in time. This implies that we will uphold two general assumptions for the time series considered in this book:

1. $t$ is not random, e.g. the time at which each observation is measured is known, and
2. the time between two consecutive observations is constant. 

This book will also focus on certain representations of time series based on parametric probabilistic models. For example, one of the fundamental probability models used in time series analysis is called the *white noise* model and is defined as

\[X_t \mathop \sim \limits^{iid} N(0, \sigma^2).\]

This statement simply means that $(X_t)$ is normally distributed and independent over time. Ideally, this is the type of process that we would want to observe once we have performed a statistical modelling procedure. However, despite it appearing to be an excessively simple model to be considered for time series, it is actually a crucial component to construct a wide range of more complex time series models (see Chapter \@ref(fundtimeseries)). Indeed, unlike the white noise process, time series are typically *not* independent over time. For example, if we suppose that the temperature in State College is unusually low on a given day, then it is reasonable to assume that the temperature the day after will also be low.

With this in mind, let us now give a quick overview of the information that can be retrieved on a time series from a simple descriptive representation.


#### Exploratory Data Analysis for Time Series {#eda}

When dealing with relatively small time series (e.g. a few thousands or less), it is
often useful to look at a graph of the original data. A graph can be
an informative tool for "detecting" some features of a time series such as trends and
the presence of outliers. This is indeed what was done in the previous paragraphs when analysing the global temperature data or the Johnson & Johnson data.

To go more in depth with respect to the previous paragraphs, a trend is typically assumed to be present in a time series when the data exhibit some form of long term increase or decrease or combination of increases or decreases. Such trends could be linear or non-linear and represent
an important part of the "signal" of a model (as seen for the Johnson & Johnson time series). Here are a few examples of
non-linear trends:

1. **Seasonal trends** (periodic): These are the cyclical patterns which repeat
after a fixed/regular time period. This could be due to business cycles
(e.g. bust/recession, recovery).
    
2. **Non-seasonal trends** (periodic): These patterns cannot be associated to
seasonal variation and can for example be due to an external variable such as,
for example, the impact of economic indicators on stock returns. Note that such
trends are often hard to detect based on a graphical analysis of the data.
    
3. **"Other" trends**: These trends have typically no regular patterns and are
over a segment of time, known as a "window", that change the statistical
properties of a time series. A common example of such trends is given by the
vibrations observed before, during and after an earthquake.

Moreover, when observing "raw" time series data it is also interesting to
evaluate if some of the following phenomena occur:

1. **Change in Mean:** Does the mean of the process shift over time?
2. **Change in Variance:** Does the variance of the process evolve with time?
3. **Change in State:** Does the time series appear to change between "states"
   having distinct statistical properties?
4. **Outliers** Does the time series contain some "extreme" observations? 
   (Note that this is typically difficult to assess visually.)

```{example, label="earthquake"}
In the figure below, we present an example of displacement recorded
during an earthquake as well as an explosion.
```

```{r example_EQ, fig.height = 4, fig.width = 7, cache = TRUE, fig.align='center'}
data(EQ5, package = "astsa")
data(EXP6, package = "astsa")

# Construct gts object
eq5 <- gts(EQ5, start = 0, freq = 1, unit_ts = "p/s", name_ts = "Earthquake Arrival Phases", data_name = "Earthquake Arrival Phases")
exp6 <- gts(EXP6, start = 0, freq = 1, unit_ts = "p/s", name_ts = "Explosion Arrival Phases", data_name = "Explosion Arrival Phases")

# Plot time series
plot(eq5)
plot(exp6)
```

From the graph, it can be observed that the statistical properties of the time
series appear to change over time. For instance, the variance of the time series
shifts at around $t = 1150$ for both series.
The shift in variance also opens "windows" where there appear to be distinct
states. In the case of the explosion data, this is particularly relevant around
$t = 50, \cdots, 250$ and then again from $t = 1200, \cdots, 1500$. Even within
these windows, there are "spikes" that could be considered as outliers most
notably around $t = 1200$ in the explosion series.

Extreme observations or outliers are commonly observed in real time series data, this is illustrated in the following example. 

```{example, label="precipitation"}
We consider here a data set coming from the domain of hydrology. The data 
concerns monthly precipitation (in mm) over a certain period of time (1907 to
1972) and is interesting for scientists in order to study water cycles. The
data are presented in the graph below:
```

```{r example_hydro, fig.height = 4, fig.width = 7, cache = TRUE, message = FALSE, warning = FALSE, fig.align='center'}
# Load hydro dataset
data("hydro")

# Simulate based on data
hydro = gts(as.vector(hydro), start = 1907, freq = 12, unit_ts = "in.", 
            name_ts = "Precipitation", data_name = "Hydrology data")

# Plot hydro 
plot(hydro)
```

We can see how most observations lie below 2mm but there appear to be different observations that go beyond this and appear to be larger than the others. These could be possible outliers that can greatly affect the estimation procedure if not taken adequately into account.

Next, we consider an example coming from high-frequency finance. The figure below presents the returns or price innovations
(i.e. the changes in price from one observation to the
next) for Starbuck's stock on July 1, 2011 for about 150 seconds (left
panel) and about 400 minutes (right panel). 

```{r example_Starbucks, fig.height = 4, fig.width = 7, cache = TRUE, message = FALSE, fig.align='center'}
# Load "high-frequency" Starbucks returns for July 01 2011
data(sbux.xts, package = "highfrequency")

# Plot returns
par(mfrow = c(1,2))

plot(gts(sbux.xts[1:89]), 
     main = "Starbucks: 150 Seconds", 
     ylab = "Returns") 

plot(gts(sbux.xts), 
     main = "Starbucks: 400 Minutes", 
     ylab = "Returns")
```

It can be observed on the left panel that observations are not equally spaced.
Indeed, in high-frequency data the intervals between two points are typically not
constant and are, even worse, random variables. This implies that the time when
a new observation will be available is in general unknown. On the right panel,
one can observe that the variability of the data seems to change during the
course of the trading day. Such a phenomenon is well known in the finance
community since a lot of variation occurs at the start (and the end) of the 
day while the middle of the day is associated with small changes. 
Moreover, clear extreme observations can also be noted in this graph at
around 11:00.

```{example, label="imu"}
Finally, let us consider the limitations of a direct graphical representation of
a time series when the sample size is large. Indeed, due to visual limitations,
a direct plotting of the data will probably result in an uninformative
aggregation of points between which it is unable to distinguish anything. This is
illustrated in the following example.

We consider here the data coming from the calibration procedure of
an Inertial Measurement Unit (IMU) which, in general terms, is used to enhance
navigation precision or reconstruct three dimensional movements:
```

```{r, echo = FALSE, fig.align='center'}
knitr::include_url("https://www.youtube.com/embed/htoBvSq8jLA")
```

These sensors are used in a very wide range of applications such as robotics, virtual reality, 
vehicle stability control, human and animal motion capture and so forth:

```{r, echo = FALSE, fig.align='center'}
knitr::include_url("https://www.youtube.com/embed/g4tgtPA54_Y")
```

The signals coming from these instruments are measured at
high frequencies over a long time and are often characterized by linear trends
and numerous underlying stochastic processes.

The code below retrieves some data from an IMU and plots it directly:

```{block2,  type='rmdimportant'}
To access the IMU time series represented below you must install the imudata package which can be found at this [link](https://github.com/SMAC-Group/imudata ).
```


```{r example_IMU, fig.height = 4, fig.width = 7, cache = TRUE, message = FALSE, dev='png', fig.align='center'}
# Load IMU data
data(imu6, package = "imudata")

# Construct gst object
Xt = gts(imu6[,1], data_name = "Gyroscope data", unit_time = "hour", 
         freq = 100*60*60, name_ts = "Angular rate", 
         unit_ts = bquote(rad^2/s^2))

# Plot time series
plot(Xt)
```

Although a linear trend and other processes are present in this signal
(time series), it is practically impossible to understand or guess anything
from the plot. For this reason, other types of representations are available to understand the behaviour of a time series and will be discussed in the next chapter. Having discussed these representations (and the relative issues with these representations) let us present the basic parametric models that are used to build even more complex models to describe and predict the behaviour of a time series.

## Modelling Time Series {#basicmodels}

Before discussing the basic time series models used to describe and forecast, we briefly discuss the concept of dependence within time series.

### Dependence within Time Series

As mentioned earlier, it is straightforward to assume that observations measured through time are dependent on each other (in that observations at time $t$ have some form of impact on observations at time $t+1$ or beyond). Due to this characteristic, one of the main interests in time series is prediction where, if $(X_t)_{t=1,\ldots,T}$ is an identically distributed but not independent sequence, we often want to know the value of ${X}_{T+h}$ for $h > 0$ (i.e. an estimator of $\mathbb{E}[X_{T+h}| X_T,...]$). In order to tackle this issue, we first need to understand the dependence between $X_{1},\ldots,X_{T}$ and, even before this, we have to formally define what **independence** is.

```{definition, IndepEvents, name = "Independence of Events"}
Two events $A$ and $B$ are independent if 
\begin{align*}
\mathbb{P}(A \cap B) = \mathbb{P}(A)\mathbb{P}(B),
\end{align*}
with $\mathbb{P}(A)$ denoting the probability of event $A$ occuring and $\mathbb{P}(A \cap B)$ denoting the joint probability (i.e. the probability that events $A$ and $B$ occur jointly). In general, $A_{1},\ldots,A_{n}$ are independent if 
\begin{align*}
\mathbb{P}(A_1 \ldots A_n) = \mathbb{P}(A_1) \ldots \mathbb{P}(A_n) \;\; \forall \; A_i \in S, \;\; i=1,\ldots,n
\end{align*}
where $S$ is the sample space.
```
<br>

```{definition, IndepRV, name = "Independence of Random Variables"}
Two random variables $X$ and $Y$ with Cumulative Distribution Functions (CDF) $F_X(x)$ and $F_Y(y)$, respectively, are independent if and only if their joint CDF $F_{X,Y}(x,y)$ is such that 
\begin{align*}
F_{X,Y}(x,y) = F_{X}(x) F_{Y}(y).
\end{align*}
In general, random variables $X_1, \ldots, X_n$ with CDF $F_{X_1}(x_1), \ldots, F_{X_n}(x_n)$ are respectively independent if and only if their joint CDF $F_{X_1, \ldots, X_n}(x_1, \ldots, x_n)$ is such that
\begin{align*}
F_{X_1,\ldots,X_n}(x_1,\ldots,x_n) = F_{X_1}(x_1) \ldots F_{X_n}(x_n).
\end{align*}
```
<br>

```{definition, iid, name = "iid sequence"}
The sequence $X_{1},X_{2},\ldots,X_{T}$ is said to be independent and identically distributed (i.e. iid) if and only if
\begin{align*}
\mathbb{P}(X_{i}<x) = \mathbb{P}(X_{j}<x) \;\; \forall x \in \mathbb{R}, \forall i,j \in \{1,\ldots,T\},
\end{align*}
and
\begin{align*}
\mathbb{P}(X_{1}<x_{1},X_{2}<x_{2},\ldots,X_{T}<x_{T})=\mathbb{P}(X_{1}<x_1) \ldots \mathbb{P}(X_{T}<x_T) \;\; \forall T\geq2, x_1, \ldots, x_T \in \mathbb{R}.
\end{align*}
```
<br>

The basic idea behind the above definitions of independence is the fact that the probability of an event regarding variable $X_i$ remains unaltered no matter what occurs for variable $X_j$ (for $i \neq j$). However, for time series, this is often not the case and $X_t$ often has some impact on $X_{t+h}$ for some $h$ (not too large). In order to explain (and predict) the impact of an observation on future observations, a series of models have been adopted through the years thereby providing a comprehensive framework to explain dependence through time. The following paragraphs introduce some of these basic models.


### Basic Time Series Models

In this section, we introduce some simple time series models that consitute the building blocks for the more complex and flexible classes of time series commonly used in practice. Before doing so it is useful to define $\Omega_t$ as all the information available up to time
$t-1$, i.e.

\[\Omega_t \equiv \left(X_{t-1}, X_{t-2}, ..., X_0 \right).\]

As we will see further on, this compact notation is quite useful.

### White Noise {#wn}

As we saw earlier, the white noise model is the building block for most time series models and, to better specify the notation used throughout this book, this model is defined as

\[{W_t}\mathop \sim \limits^{iid} N\left( {0,\sigma _w^2} \right).\]

This definition implies that:

1. $\mathbb{E}[W_t | \Omega_t] = 0$ for all $t$,
2. $\text{cov}\left(W_t, W_{t-h} \right) = \boldsymbol{1}_{h = 0} \; \sigma^2$ for
all $t, h$.

More specifically, $h \in \mathbb{N}^+$ is the time difference between lagged variables. Therefore, in this process there is an absence of temporal (or serial) correlation and it is homoskedastic (i.e. it has a constant variance). Going into further details, white noise can be categorzied into two sorts of processes: *weak* and *strong*. The process $(W_t)$ is
a *weak* white noise if

1. $\mathbb{E}[W_t] = 0$ for all $t$,
2. $\text{var}\left(W_t\right) = \sigma_w^2$ for all $t$,
3. $\text{cov} \left(W_t, W_{t-h}\right) = 0$ for all $t$ and for all $h \neq 0$.

Note that this definition does not imply that $W_t$ and $W_{t-h}$ are independent (for $h \neq 0$) but simply uncorrelated. However, the notion of independence is used to define a *strong* white noise as

1. $\mathbb{E}[W_t] = 0$ and $\text{var}(W_t) = \sigma^2 < \infty$, for all $t$,
2. $F(W_t) = F(W_{t-h})$ for all $t,h$ (where $F(W_t)$ denotes the marginal distribution of $W_t$),
3. $W_t$ and $W_{t-h}$ are independent for all $t$ and for all $h \neq 0$.

It is clear from these definitions that if a process is a strong white noise it is also a weak white noise. However, the converse is not true as shown in the following example:

```{example, label="weaknotstrong"}

Let $Y_t \mathop \sim F_{t+2}$, where $F_{t+2}$ denotes
a Student distribution with $t+2$ degrees of freedom. Assuming the 
sequence $(Y_1, \ldots, Y_n)$ to be independent, we 
let $X_t = \sqrt{\frac{t}{t+2}} Y_t$. Then, the process $(X_t)$ is obviously
not a strong white noise as the distribution of $X_t$ changes with $t$. However
this process is a weak white noise since we have:

- $\mathbb{E}[X_t] = \sqrt{\frac{t}{t+2}} \mathbb{E}[Y_t] = 0$ for all $t$.
- $\text{var}(X_t) = \frac{t}{t+2} \text{var}(Y_t) = \frac{t}{t+2} \frac{t+2}{t} = 1$ for all $t$.
- $\text{cov}(X_t, X_{t+h}) = 0$ (by independence), for all $t$, and for all $h \neq 0$.

```

This distinction is therefore important and will be extremely relevant when discussing the concept of "stationarity" further on in this book. In general, the white noise model is assumed to be Gaussian in many practical cases and the code below presents an example of how to simulate a Gaussian white noise process.

```{r example_WN, fig.height = 4, fig.width = 7, cache = TRUE}
n = 1000                               # process length
sigma2 = 1                             # process variance
Xt = gen_gts(n, WN(sigma2 = sigma2))
plot(Xt)
```

This model can be found in different applied settings and is often accompanied by some of the models presented in the following paragraphs.


### Random Walk {#rw}

The term *random walk* was first introduced by Karl Pearson in the early
nineteen-hundreds and a wide range of random walk models have been defined over the years. For example, one of the simplest forms of a random walk process can be
explained as follows: suppose that you are walking on campus and your
next step can either be to your left, your right, forward or backward
(each with equal probability). Two realizations of such processes are
represented below:

```{r RW2d, fig.height = 5, fig.width = 5.5, cache = TRUE, fig.align='center'}
set.seed(5)
RW2dimension(steps = 10^2)
RW2dimension(steps = 10^4)
```

Such processes inspired Karl Pearson's famous quote that

>
> "*the most likely place to find a drunken walker is somewhere near his starting point.*"
> 

Empirical evidence of this phenomenon is not too hard to find on a Friday or Saturday night. This two-dimensional process may easily be extended to three dimensions and a simulated example of such a process is presented in the animation below:

<center>
![](asset/rw3.gif)
</center>


In this text, we only consider one very specific form of random walk, namely the Gaussian random walk which can be defined as:

$$X_t = X_{t-1} + W_t,$$

where $W_t$ is a Gaussian white noise process with initial condition $X_0 = c$ (typically $c = 0$.) This process can be expressed differently by *backsubstitution* as follows:

\[\begin{aligned}
  {X_t} &= {X_{t - 1}} + {W_t} \\
   &= \left( {{X_{t - 2}} + {W_{t - 1}}} \right) + {W_t} \\
   &= \vdots \\
  {X_t} &= \sum\limits_{i = 1}^t {{W_i}} + X_0 =  \sum\limits_{i = 1}^t {{W_i}} + c \\ 
\end{aligned} \]

A random variable following a random walk can therefore be expressed as the cumulated sum of all the random variables that precede it. The code below presents an example of how to simulate a such process.

```{r example_RW, fig.height = 4, fig.width = 7, cache=TRUE}
n = 1000                               # process length
gamma2 = 1                             # innovation variance
Xt = gen_gts(n, RW(gamma2 = gamma2))
plot(Xt)
```

The random walk model is often used to explain phenomena in many different areas one of which is finance where stock prices follow these kind of processes. 


### First-Order Autoregressive Model {#ar1}

A first-order autoregressive model or AR(1) is a generalization of both
the white noise and the random walk processes which are both special
cases of an AR(1). A (Gaussian) AR(1) process can be defined as

\[{X_t} = {\phi}{X_{t - 1}} + {W_t},\]

where $W_t$ is a Gaussian white noise. Clearly, an AR(1) with $\phi = 0$ is
a Gaussian white noise and when $\phi = 1$ the process becomes a random walk.

```{exercise, ar1realizations}
An AR(1) is in fact a linear combination of past realisations of
a white noise $W_t$ process. Indeed, we have

\[\begin{aligned}
 {X_t} &= {\phi_t}{X_{t - 1}} + {W_t} 
   = {\phi}\left( {{\phi}{X_{t - 2}} + {W_{t - 1}}} \right) + {W_t} \\
   &= \phi^2{X_{t - 2}} + {\phi}{W_{t - 1}} + {W_t} 
   = {\phi^t}{X_0} + \sum\limits_{i = 0}^{t - 1} {\phi^i{W_{t - i}}}.
\end{aligned}\]

Under the assumption of infinite past (i.e. $t \in \mathbb{Z}$) and $|\phi| < 1$,
we obtain

\[X_t = \sum\limits_{i = 0}^{\infty} {\phi^i {W_{t - i}}},\]

since $\operatorname{lim}_{i \to \infty} \; {\phi^i}{X_{t-i}} = 0$.
```

From the conclusion of the above the remark, you may have noticed how we assume that the considered time series have zero expectation. The following remark justifies this assumption.

```{exercise, ar1mean}
We generally assume that an AR(1), as well as other time series
models, have zero mean. The reason for this assumption is only to simplfy the
notation but it is easy to consider, for example, an AR(1) process around an
arbitrary mean $\mu$, i.e.

\[\left(X_t - \mu\right) = \phi \left(X_{t-1} - \mu \right) + W_t,\]

which is of course equivalent to

\[X_t = \left(1 - \phi \right) \mu + \phi X_{t-1} + W_t.\]

Thus, we will generally only work with zero mean processes since adding means
is simple.
```

As for the previously presented models, we provide the code that gives an example of how an AR(1) can be simulated.

```{r example_AR1, fig.height = 4, fig.width = 7, cache=TRUE}
n = 1000                              # process length
phi = 0.5                             # phi parameter
sigma2 = 1                            # innovation variance
Xt = gen_gts(n, AR1(phi = phi, sigma2 = sigma2))
plot(Xt)
```

The AR(1) model is one of the most popular and commonly used models in many practical settings going from biology where it is used to explain the evolution of gene expressions to economics where it is used to model macroeconomic trends.
 

### Moving Average Process of Order 1 {#ma1}

As seen in the previous example, an AR(1) can be expressed as a linear
combination of all past observations of the white noise process $(W_t)$. In a similar manner we can (in some sense) describe the moving average process of order 1 or MA(1) as a "truncated"
version of an AR(1). This model is defined as

\begin{equation} 
  X_t = \theta W_{t-1} + W_t,
\end{equation}
 
where (again) $W_t$ denotes a Gaussian white noise process. As we will see further on, as for the AR(1) model, this model can also be represented as a linear combination of past observations but it has different characteristics which can capture different types of dynamics in various practical cases.

An example on how to generate an MA(1) is given below:

```{r example_MA1, fig.height = 4, fig.width = 7, cache=TRUE}
n = 1000                              # process length
sigma2 = 1                            # innovation variance
theta = 0.5                           # theta parameter
Xt = gen_gts(n, MA1(theta = theta, sigma2 = sigma2))
plot(Xt)
```

The use of this model is widespread, especially combined with the AR(1) model, and can be found in fields such as engineering where it is often used for signal processing.


### Linear Drift {#drift}

A linear drift is a very simple deterministic time series model which can be
expressed as 

\[X_t = X_{t-1} + \omega, \]

where $\omega$ is a constant and with the initial condition $X_0 = c$, where $c$ is an
arbitrary constant (typically $c = 0$). This process can be expressed in a more
familiar form as follows:

\[
  {X_t} = {X_{t - 1}} + \omega 
   = \left( {{X_{t - 2}} + \omega} \right) + \omega 
   = t{\omega} + c . \]

Therefore, a (linear) drift corresponds to a simple linear model with slope $\omega$ and intercept $c$.

```{exercise, remdrift}
You may argue that the definition of this model is not useful since it constitutes a simple linear model. However this model is often accompanied by other time series models (such as the ones presented earlier) and its estimation can be greatly improved when considered in conjunction with the other models.
```

Given its simple form, a linear drift can simply be generated using the code below:

```{r example_Drift, fig.height = 4, fig.width = 7, cache=TRUE}
n = 100                               # process length
omega = 0.5                           # slope parameter
Xt = gen_gts(n, DR(omega = omega))
plot(Xt)
```

This time series model is widely used in different areas of signal analysis where mechanical systems and measuring devices can be characterized by this type of behaviour.


## Composite Stochastic Processes {#lts}

In the previous paragraphs we defined and briefly discussed the basic time series models that can individually be used to describe and predict a wide range of phenomena in a variety of fields of application. However, their capability of capturing and explaining the different behaviours of phenomena through time increases considerably when they are combined to form so-called *composite models* (or composite processes). A composite (stochastic) process can be defined as the sum of underlying (or latent) time series models and in the rest of this book we will use the term *latent time series models* to refer to these kinds of models. A simple example of such a model is given by

\[\begin{aligned}
Y_t &= Y_{t-1} + W_t + \delta\\
X_t &= Y_t + Z_t,
\end{aligned}\]

where $W_t$ and $Z_t$ are two independent Gaussian white noise processes. 
This model is often used as a first basis to approximate the number of
individuals in the context ecological population dynamics. 
For example, suppose we want to study the population of Chamois in the Swiss Alps.
Let $Y_t$ denote the "true" number of individuals in this population at time $t$.
It is reasonable to assume that the number of individuals at time $t$ ($Y_t$) is (approximately) the population at the previous time $t-1$ (e.g the previous year) plus a random variation and a drift. This random variation is due to the natural randomness in ecological population
dynamics and reflects changes such as the number of predators, the abundance
of food, or weather conditions.
On the other hand, ecological *drift* is often of particular interest for ecologists as 
it can be used to determine the "long" term trends of the population 
(e.g. if the population is increasing, decreasing, or stable).
Of course, $Y_t$ (the number of individauls) is typically unknown and we observe
a noisy version of it, denoted as $X_t$. 
This process corresponds to the true population plus a measurement error since
some individuals may not be observed while others may have been counted several
times.
Interestingly, this process can clearly be expressed as a
*latent time series model* (or composite stochastic process) as follows:

\[\begin{aligned}
R_t &= R_{t-1} + W_t \\
S_t &= \delta t \\
X_t &= R_t + S_t + Z_t,
\end{aligned}\]

where $R_t$, $S_t$ and $Z_t$ denote, respectively, a random walk,
a drift, and a white noise. The code below can be used to simulate such data:

```{r example_composite, fig.height = 6, fig.width = 7, cache=TRUE}
n = 1000                                # process length
delta = 0.005                           # delta parameter (drift)
sigma2 = 10                             # variance parameter (white noise)
gamma2 = 0.1                            # innovation variance (random walk)
#model = WN(sigma2 = sigma2) + RW(gamma2 = gamma2) + DR(omega = delta)
#Xt = gen_lts(n, model)
#plot(Xt)
```

In the above graph, the first three plots represent the latent (unobserved)
processes (i.e. white noise, random walk, and drift) and the last one represents
the sum of the three (i.e. $(X_t)$).

Let us consider a real example where these latent processes are useful to
describe (and predict) the behavior of economic variables such as Personal 
Saving Rates (PSR). A process that is used for these settings is the 
"random-walk-plus-noise" model, meaning that the data can be explained by a 
random walk process in addition to which we observe some other process (e.g. 
a white noise model, an autoregressive model such as an AR(1), etc.). The PSR 
taken from the Federal Reserve of St. Louis from January 1, 1959, to  May 1,
2015, is presented in the following plot:

```{r example_PSR, fig.height = 4, fig.width = 7, cache=TRUE}
# Load savingrt dataset
data("savingrt")
# Simulate based on data
savingrt = gts(as.vector(savingrt), start = 1959, freq = 12, unit_ts = "%", 
            name_ts = "Saving Rates", data_name = "US Personal Saving Rates")
# Plot savingrt simulation
plot(savingrt)
```

It can be observed that the mean of this process seems to vary over time,
suggesting that a random walk can indeed be considered as a possible model
to explain this data. In addition, aside from some "spikes" and occasional 
sudden changes, the observations appear to gradually change from one time point
to the other, suggesting that some other form of dependence between them could
exist.

<!--chapter:end:01-intro.Rmd-->

# Fundamental Properties of Time Series {#fundtimeseries}


>
> "*One of the first things taught in introductory statistics textbooks is that correlation is not causation. It is also one of the first things forgotten.*" – Thomas Sowell
>

```{block2,  type='rmdimportant'}
To make use of the R code within this chapter you will need to install (if not already done) and load the following libraries:

  - [quantmod](https://cran.r-project.org/web/packages/quantmod/index.html);
  - [simts](http://simts.smac-group.com/);
  - [astsa](https://cran.r-project.org/web/packages/astsa/index.html);
  - [robcor](https://cran.r-project.org/web/packages/robcor/index.html).
```

```{r, echo=FALSE, include=FALSE}
library(astsa)
library(mgcv)
library(simts)
library(imudata)
library(timeDate)
library(robcor)
```

In this chapter we will discuss and formalize how knowledge about $X_{t-1}$ (or
more generally about all the information from the past, $\Omega_t$) can provide 
us with some information about the properties of $X_t$. In particular, we will 
consider the correlation (or covariance) of $X_t$ at different times such 
as $\text{corr} \left(X_t, X_{t+h}\right)$. This "form" of correlation (covariance) is
called the *autocorrelation* (*autocovariance*) and is a very useful tool in
time series analysis. However, if we do not assume that a time series is 
characterized by a certain form of "stability", it would be rather difficult 
to estimate $\text{corr} \left(X_t, X_{t+h}\right)$ as this quantity would depend on 
both $t$ and $h$ leading to more parameters to estimate than observations 
available. Therefore, the concept of *stationarity* is convenient in this
context as it allows (among other things) to assume that

\[\text{corr} \left(X_t, X_{t+h}\right) = \text{corr} \left(X_{t+j}, X_{t+h+j}\right), \;\;\; \text{for all $j$},\]

implying that the autocorrelation (or autocovariance) is only a function of the
lag between observations, rather than time itself. We will first discuss the concept of autocorrelation in time series, then we will discuss stationarity which will then allow us to adequately define and study estimators of the autocorrelation functions. Before 
moving on, it is helpful to remember that correlation (or autocorrelation) is
only appropriate to measure a very specific kind of dependence, i.e. linear
dependence. There are many other forms of dependence as illustrated in the
bottom panels of the graph below, which all have a (true) zero correlation:

```{r correxample, cache = TRUE, echo = FALSE, fig.cap="Different forms of dependence and their Pearson's r values", fig.align = 'center'}
knitr::include_graphics("images/corr_example.png")
```

Several other metrics have been introduced in the literature to assess the
degree of "dependence" of two random variables, however this goes beyond the
material discussed in this chapter.

 
## The Autocorrelation and Autocovariance Functions 

We will introduce the autocorrelation function by first defining the **autocovariance function**.


```{definition, label="acvf"}
The *autocovariance function* of a series $(X_t)$ is defined as 

\[{\gamma_x}\left( {t,t+h} \right) \equiv \text{cov} \left( {{X_t},{X_{t+h}}} \right),\]
```

where the definition of covariance is given by:
  
  \[
    \text{cov} \left( {{X_t},{X_{t+h}}} \right) \equiv \mathbb{E}\left[ {{X_t}{X_{t+h}}} \right] - \mathbb{E}\left[ {{X_t}} \right]\mathbb{E}\left[ {{X_{t+h}}} \right].
    \]

Similarly, the above expectations are defined as:
  
  \[\begin{aligned}
     \mathbb{E}\left[ {{X_t}} \right] &\equiv \int\limits_{ - \infty }^\infty  {x \cdot {f_t}\left( x \right)dx},  \\
     \mathbb{E}\left[ {{X_t}{X_{t+h}}} \right] &\equiv \int\limits_{ - \infty }^\infty  {\int\limits_{ - \infty }^\infty  {{x_1}{x_2} \cdot f_{t,t+h}\left( {{x_1},{x_2}} \right)d{x_1}d{x_2}} } ,
     \end{aligned} \]

where ${f_t}\left( x \right)$ and $f_{t,t+h}\left( {{x_1},{x_2}} \right)$ denote,
respectively, the density of $X_t$ and the joint density of the 
pair $(X_t, X_{t+h})$. Considering the notation used above, it should be clear that $X_t$ is assumed to be a continous random variable. Since we generally consider stochastic processes with constant zero mean, we often have

\[{\gamma_x}\left( {t,t+h} \right) = \mathbb{E}\left[X_t X_{t+h} \right]. \]

In addition, in the context of this book we will normally drop the subscript referring to the time series (i.e. $x$ in this case) if it is clear from the context which time
series the autocovariance refers to. For example, we generally use ${\gamma}\left( {t,t+h} \right)$ instead of ${\gamma_x}\left( {t,t+h} \right)$. Moreover, the notation is even further simplified when the covariance of $X_t$ and $X_{t+h}$ is the same as that of $X_{t+j}$ and $X_{t+h+j}$ (for all $j$), i.e. the covariance depends only 
on the time between observations and not on the specific time $t$. This is an 
important property called *stationarity*, which will be discussed in the next
section. In this case, we simply use to following notation:

  \[\gamma \left( {h} \right) = \text{cov} \left( X_t , X_{t+h} \right). \]

This is the definition of autocovariance that will be used from this point onwards and therefore this notation will generally be used throughout the text thereby implying 
certain properties for the process $(X_t)$ (i.e. stationarity) . 
With this in mind, several remarks can be made on the autocovariance function:
  
  1. The autocovariance function is *symmetric*. 
That is, ${\gamma}\left( {h} \right) = {\gamma}\left( -h \right)$ 
  since $\text{cov} \left( {{X_t},{X_{t+h}}} \right) = \text{cov} \left( X_{t+h},X_{t} \right)$.
2. The autocovariance function "contains" the variance of the process 
as $\text{var} \left( X_{t} \right) = {\gamma}\left( 0 \right)$.
3. We have that $|\gamma(h)| \leq \gamma(0)$ for all $h$. The proof of this 
inequality is direct and follows from the Cauchy-Schwarz inequality, i.e.
\[ \begin{aligned}
  \left(|\gamma(h)| \right)^2 &= \gamma(h)^2 = \left(\mathbb{E}\left[\left(X_t - \mathbb{E}[X_t] \right)\left(X_{t+h} - \mathbb{E}[X_{t+h}] \right)\right]\right)^2\\
  &\leq \mathbb{E}\left[\left(X_t - \mathbb{E}[X_t] \right)^2 \right] \mathbb{E}\left[\left(X_{t+h} - \mathbb{E}[X_{t+h}] \right)^2 \right] =  \gamma(0)^2. 
  \end{aligned}
  \] 
4. Just as any covariance, ${\gamma}\left( {h} \right)$ is "scale dependent"
since ${\gamma}\left( {h} \right) \in \mathbb{R}$, 
or $-\infty \le {\gamma}\left( {h} \right) \le +\infty$. We therefore have:
  - if $\left| {\gamma}\left( {h} \right) \right|$ is "close" to zero, 
then $X_t$ and $X_{t+h}$ are "weakly" (linearly) dependent;
- if $\left| {\gamma}\left( {h} \right) \right|$ is "far" from zero, 
then the two random variable present a "strong" (linear) dependence. 
However it is generally difficult to asses what "close" and "far" from 
zero means in this case. 
5. ${\gamma}\left( {h} \right)=0$ does not imply that $X_t$ and $X_{t+h}$ are 
independent but simply $X_t$ and $X_{t+h}$ are uncorrelated. 
The independence is only implied by ${\gamma}\left( {h} \right)=0$ in
the jointly Gaussian case.

As hinted in the introduction, an important related statistic is the correlation
of $X_t$ with $X_{t+h}$ or *autocorrelation*, which is defined as

$$\rho \left(  h \right) = \text{corr}\left( {{X_t},{X_{t + h}}} \right) = \frac{{\text{cov}\left( {{X_t},{X_{t + h}}} \right)}}{{{\sigma _{{X_t}}}{\sigma _{{X_{t + h}}}}}} = \frac{\gamma(h) }{\gamma(0)}.$$
  
Similarly to $\gamma(h)$, it is important to note that the above notation 
implies that the autocorrelation function is only a function of the 
lag $h$ between observations. Thus, autocovariances and autocorrelations are one
possible way to describe the joint distribution of a time series. Indeed, the 
correlation of $X_t$ with $X_{t+1}$ is an obvious measure of how *persistent* a
time series is. 

Remember that just as with any correlation:
  
1. $\rho \left( h \right)$ is "scale free" so it is much easier to interpret 
than $\gamma(h)$.
2. $|\rho \left( h \right)| \leq 1$ since $|\gamma(h)| \leq \gamma(0)$.
3. **Causation and correlation are two very different things!**
  
### A Fundamental Representation
  
Autocovariances and autocorrelations also turn out to be very useful tools as
they are one of the *fundamental representations* of time series. Indeed, if we
consider a zero mean normally distributed process, it is clear that its joint
distribution is fully characterized by the 
autocovariances $\mathbb{E}[X_t X_{t+h}]$ (since the joint probability density only depends of these covariances). Once we know the autocovariances we
know *everything* there is to know about the process and therefore:
  
> if two processes have the same autocovariance function, then they are the same process.
  
### Admissible Autocorrelation Functions `r emo::ji("scream")`
  
Since the autocorrelation function is one of the fundamental representations of time
series, it implies that one might be able to define a stochastic process by 
picking a set of autocorrelation values (assuming for example that $\text{var}(X_t) = 1$). 
However, it turns out that not every collection of
numbers, say $\{\rho_1, \rho_2, ...\}$, can represent the autocorrelation of a
process. Indeed, two conditions are required to ensure the validity of an
autocorrelation sequence:
  
1. $\operatorname{max}_j \; \left| \rho_j \right| \leq 1$.
2. $\text{var} \left[\sum_{j = 0}^\infty \alpha_j X_{t-j} \right] \geq 0 \;$ for all $\{\alpha_0, \alpha_1, ...\}$.

The first condition is obvious and simply reflects the fact 
that $|\rho \left( h \right)| \leq 1$ but the second is far more difficult to
verify. To further our understanding of the latter we let $\alpha_j = 0$ for $j > 1$ and see that in this case the second condition implies that

\[\text{var} \left[ \alpha_0 X_{t} + \alpha_1 X_{t-1}  \right] = \gamma_0 \begin{bmatrix}
   \alpha_0 & \alpha_1
   \end{bmatrix}   \begin{bmatrix}
   1 & \rho_1\\
   \rho_1 & 1
   \end{bmatrix} \begin{bmatrix}
   \alpha_0 \\
   \alpha_1
   \end{bmatrix} \geq 0. \]

Thus, the matrix 

\[ \boldsymbol{A}_1 = \begin{bmatrix}
  1 & \rho_1\\
  \rho_1 & 1
  \end{bmatrix}, \]

must be positive semi-definite. Taking the determinant we have 

\[\operatorname{det} \left(\boldsymbol{A}_1\right) = 1 - \rho_1^2, \]

implying that the condition $|\rho_1| \leq 1$ must be respected. 
Now, let $\alpha_j = 0$ for $j > 2$, then we must verify that:
  
  \[\text{var} \left[ \alpha_0 X_{t} + \alpha_1 X_{t-1}  + \alpha_2 X_{t-2} \right] = \gamma_0 \begin{bmatrix}
     \alpha_0 & \alpha_1 &\alpha_2
     \end{bmatrix}   \begin{bmatrix}
     1 & \rho_1 & \rho_2\\
     \rho_1 & 1 & \rho_1 \\
     \rho_2 & \rho_1 & 1
     \end{bmatrix} \begin{bmatrix}
     \alpha_0 \\
     \alpha_1 \\
     \alpha_2
     \end{bmatrix} \geq 0. \]

Again, this implies that the matrix

\[ \boldsymbol{A}_2 = \begin{bmatrix}
  1 & \rho_1 & \rho_2\\
  \rho_1 & 1 & \rho_1 \\
  \rho_2 & \rho_1 & 1
  \end{bmatrix}, \]

must be positive semi-definite and it is easy to verify that

\[\operatorname{det} \left(\boldsymbol{A}_2\right) = \left(1 - \rho_2 \right)\left(- 2 \rho_1^2 + \rho_2 + 1\right). \]

Thus, this implies that 

\[\begin{aligned} &- 2 \rho_1^2 + \rho_2 + 1 \geq 0 \Rightarrow 1 \geq \rho_2 \geq 2 \rho_1^2 - 1 \\
   &\Rightarrow 1 - \rho_1^2 \geq \rho_2 - \rho_1^2 \geq -(1 - \rho_1^2)\\
   &\Rightarrow 1 \geq \frac{\rho_2 - \rho_1^2 }{1 - \rho_1^2} \geq -1.
   \end{aligned}\]

Therefore, $\rho_1$ and $\rho_2$ must lie in a parabolic shaped region defined
by the above inequalities as illustrated in Figure \@ref(fig:admissibility).

```{r admissibility, cache = TRUE, echo = FALSE, fig.cap = "Admissible autocorrelation functions", fig.align='center', fig.height = 5, fig.width = 7}
plot(NA, xlim = c(-1.1,1.1), ylim = c(-1.1,1.1), xlab = expression(rho[1]),
     ylab = expression(rho[2]), cex.lab = 1.5)
grid()

# Adding boundary of constraint |rho_1| < 1
abline(v = c(-1,1), lty = 2, col = "darkgrey")

# Adding boundary of constraint |rho_2| < 1
abline(h = c(-1,1), lty = 2, col = "darkgrey")

# Adding boundary of non-linear constraint
rho1 = seq(from = -1, to = 1, length.out = 10^3)
rho2 = (rho1^2 - 1) + rho1^2 
lines(rho1, rho2, lty = 2, col = "darkgrey")

# Adding admissible region
polygon(c(rho1,rev(rho1)),c(rho2,rep(1,10^3)),
        border = NA, col= rgb(0,0,1, alpha = 0.1))

# Adding text
text(0,0, c("Admissible Region"))
```

From our derivation, it is clear that the restrictions on the autocorrelation are
very complicated, thereby justifying the need for other forms of fundamental 
representation which we will explore later in this text. Before moving on to
the estimation of the autocorrelation and autocovariance functions, we must 
first discuss the stationarity of $(X_t)$, which will provide a convenient 
framework in which $\gamma(h)$ and $\rho(h)$ can be used (rather that $\gamma(t,t+h)$ 
for example) and (easily) estimated.


## Stationarity {#stationary}

There are two kinds of stationarity that are commonly used. They are defined 
as follows:
  
```{definition, label="strongstationarity"}
A process $(X_t)$ is *strongly stationary* or *strictly stationary* if the joint probability distribution of $(X_{t-h}, ..., X_t, ..., X_{t+h})$ is independent of $t$ for all $t,h \in \mathbb{Z}$.
```
<br>

```{definition, label="weakstationarity"}
A process $(X_t)$ is *weakly stationary*, *covariance stationary* or *second order stationary* if $\mathbb{E}[X_t]$ and $\mathbb{E}[X_t^2]$ are finite and $\mathbb{E}[X_t X_{t-h}]$ depends only on $h$ and not on $t$ for all $t,h \in \mathbb{Z}$.
```
<br>

These types of stationarity are *not equivalent* and the presence of one kind 
of stationarity does not imply the other. That is, a time series can be strongly
stationary but not weakly stationary and vice versa. In some cases, a time 
series can be both strongly and weakly stationary and this occurs, for 
example, in the (jointly) Gaussian case. Stationarity of $(X_t)$ matters
because *it provides the framework in which averaging dependent data makes
sense*, thereby allowing us to easily obtain estimates for certain quantities
such as autocorrelation.

Several remarks and comments can be made on these definitions:
  
- As mentioned earlier, strong stationarity *does not imply* weak stationarity. For example, an $iid$ Cauchy process is strongly but not weakly stationary (why? `r emo::ji("thinking_face")`).


- Weak stationarity *does not imply* strong stationarity. For example, consider the following weak white noise process:
  \begin{equation*}
X_t = \begin{cases}
U_{t}      & \quad \text{if } t \in \{2k:\, k\in \mathbb{Z} \}, \\
V_{t}      & \quad \text{if } t \in \{2k+1:\, k\in \mathbb{Z} \},\\
\end{cases}
\end{equation*}
where ${U_t} \mathop \sim \limits^{iid} N\left( {1,1} \right)$ and ${V_t}\mathop \sim \limits^{iid} \mathcal{E}\left( 1 \right)$ is a weakly stationary process that is *not* strongly stationary.

- Strong stationarity combined with bounded values of 
  $\mathbb{E}[X_t^2]$ *implies* weak stationarity.
- Weak stationarity combined with normally distributed processes *implies* 
  strong stationarity.
  
```{exercise, moment, name = "Existence of moments"}
It is important to note that, for a given value of $r$ and a random variable $X$, the expected $\mathbb{E}[X^r]$ may be infinite, or may not exist. If there exists an $r \in \mathbb{N}^+$ such that $\mathbb{E}[|X|^r]$ exists and is bounded, then we have that $\mathbb{E}[X^j] < \infty$ for all $j = 1,...,r$. This result is implied by Jensen’s inequality. Indeed, the function $f(x) = x^k$ is convex for $x > 0$ and $k > 1$, so we have
$$\mathbb{E}\left[|X |^j\right]^{\frac{r}{j}} \leq \mathbb{E}\left[(|X |^j)^{\frac{r}{j}}\right] = \mathbb{E}\left[|X|^r\right] < \infty.$$
Therefore, we obtain $\mathbb{E}[X^j] \leq \mathbb{E}[|X|^j] < \infty$.
```

### Assessing Weak Stationarity of Time Series Models

It is important to understand how to verify if a postulated model is (weakly)
stationary. In order to do so, we must ensure that our model satisfies the
following three properties:
  
1. $\mathbb{E}\left[X_t \right] = \mu_t = \mu < \infty$,
2. $\text{var}\left[X_t \right] = \sigma^2_t = \sigma^2 < \infty$,
3. $\text{cov}\left(X_t, X_{t+h} \right) = \gamma \left(h\right)$ (i.e. the autocovariance only depends on $h$ and not on $t$).

In the following examples, we evaluate the stationarity of the processes
introduced in Section \@ref(basicmodels).

```{example, label="gwn", name = "Gaussian White Noise"} 
It is easy to verify that this process is stationary. Indeed, we have:
  
1. $\mathbb{E}\left[ {{X_t}} \right] = 0$,
2. $\gamma(0) = \sigma^2 < \infty$,  
3. $\gamma(h) = 0$ for $|h| > 0$.
```


```{example, label="srw", name = "Random Walk"}
To evaluate the stationarity of this process, we first derive its properties:

1. We begin by calculating the expectation of the process:
\[
  \mathbb{E}\left[ {{X_t}} \right] = \mathbb{E}\left[ {{X_{t - 1}} + {W_t}} \right]
  = \mathbb{E}\left[ {\sum\limits_{i = 1}^t {{W_i}}  + {X_0}} \right] 
  = \mathbb{E}\left[ {\sum\limits_{i = 1}^t {{W_i}} } \right] + {c} 
  = c.  \] 

Observe that the mean obtained is constant since it depends only on the value
of the first term in the sequence.

2. Next, after finding the mean to be constant, we calculate the variance to check stationarity:
  \[\begin{aligned}
     \text{var}\left( {{X_t}} \right) &= \text{var}\left( {\sum\limits_{i = 1}^t {{W_t}}  + {X_0}} \right) 
     = \text{var}\left( {\sum\limits_{i = 1}^t {{W_i}} } \right) + \underbrace {\text{var}\left( {{X_0}} \right)}_{= 0} \\
     &= \sum\limits_{i = 1}^t {\text{var}\left( {{W_i}} \right)} 
     = t \sigma_w^2,
     \end{aligned}\] 
where $\sigma_w^2 = \text{var}(W_t)$. Therefore, the variance depends on time $t$, contradicting our
second property. Moreover, we have:
  \[\mathop {\lim }\limits_{t \to \infty } \; \text{var}\left(X_t\right) = \infty.\]
This process is therefore not weakly stationary.

3. Regarding the autocovariance of a random walk, we have:
  \[\begin{aligned}
     \gamma \left( h \right) &= \text{cov}\left( {{X_t},{X_{t + h}}} \right) 
     = \text{cov}\left( {\sum\limits_{i = 1}^t {{W_i}} ,\sum\limits_{j = 1}^{t + h} {{W_j}} } \right) 
     = \text{cov}\left( {\sum\limits_{i = 1}^t {{W_i}} ,\sum\limits_{j = 1}^t {{W_j}} } \right)\\ 
     &= \min \left( {t,t + h} \right)\sigma _w^2
     = \left( {t + \min \left( {0,h} \right)} \right)\sigma _w^2,
     \end{aligned} \]
which further illustrates the non-stationarity of this process.

Moreover, the autocorrelation of this process is given by

\[\rho (h) = \frac{t + \min \left( {0,h} \right)}{\sqrt{t}\sqrt{t+h}},\]

implying (for a fixed $h$) that

\[\mathop {\lim }\limits_{t \to \infty } \; \rho(h) = 1.\]

Note that using $\gamma (h)$ and $\rho (h)$ in this context is actually an abuse of notation since both of these quantites are here function of $h$ and $t$.
```

In the following simulated example, we illustrate the non-stationary feature of
such a process:
  
```{r RWsim, cache = TRUE, fig.cap = "Two hundred simulated random walks.", fig.align='center', fig.height = 5, fig.width = 7}
# Number of simulated processes
B = 200

# Length of random walks
n = 1000

# Output matrix
out = matrix(NA,B,n)

# Set seed for reproducibility
set.seed(6182)

# Simulate Data
for (i in seq_len(B)){
  # Simulate random walk
  Xt = gen_gts(n, RW(gamma = 1))
  
  # Store process
  out[i,] = Xt
}

# Plot random walks
plot(NA, xlim = c(1,n), ylim = range(out), xlab = "Time", ylab = " ")
grid()
color = sample(topo.colors(B, alpha = 0.5))
grid()
for (i in seq_len(B)){
  lines(out[i,], col = color[i])
}

# Add 95% confidence region
lines(1:n, 1.96*sqrt(1:n), col = 2, lwd = 2, lty = 2)
lines(1:n, -1.96*sqrt(1:n), col = 2, lwd = 2, lty = 2)
```

In the plot above, two hundred simulated random walks are plotted along with
theoretical 95% confidence intervals (red-dashed lines). The relationship
between time and variance can clearly be observed (i.e. the variance of the 
process increases with the time).

```{example, label="exma1", name = "Moving Average of Order 1"}

Similarly to our previous examples, we attempt to verify the stationary 
properties for the MA(1) model defined in the previous chapter:
  
1. \[ 
    \mathbb{E}\left[ {{X_t}} \right] = \mathbb{E}\left[ {{\theta_1}{W_{t - 1}} + {W_t}} \right] 
    = {\theta_1} \mathbb{E} \left[ {{W_{t - 1}}} \right] + \mathbb{E}\left[ {{W_t}} \right] 
    = 0. \] 
2. \[\text{var} \left( {{X_t}} \right) = \theta_1^2 \text{var} \left( W_{t - 1}\right) + \text{var} \left( W_{t}\right) = \left(1 + \theta^2 \right) \sigma^2_w.\]  
3. Regarding the autocovariance, we have 
\[\begin{aligned}
   \text{cov}\left( {{X_t},{X_{t + h}}} \right) &= \mathbb{E}\left[ {\left( {{X_t} - \mathbb{E}\left[ {{X_t}} \right]} \right)\left( {{X_{t + h}} - \mathbb{E}\left[ {{X_{t + h}}} \right]} \right)} \right] = \mathbb{E}\left[ {{X_t}{X_{t + h}}} \right] \\
   &= \mathbb{E}\left[ {\left( {{\theta}{W_{t - 1}} + {W_t}} \right)\left( {{\theta }{W_{t + h - 1}} + {W_{t + h}}} \right)} \right] \\
   &= \mathbb{E}\left[ {\theta^2{W_{t - 1}}{W_{t + h - 1}} + \theta {W_t}{W_{t + h}} + {\theta}{W_{t - 1}}{W_{t + h}} + {W_t}{W_{t + h}}} \right]. \\
   \end{aligned} \] 
It is easy to see that $\mathbb{E}\left[ {{W_t}{W_{t + h}}} \right] = {\boldsymbol{1}_{\left\{ {h = 0} \right\}}}\sigma _w^2$ and therefore, we obtain
\[\text{cov} \left( {{X_t},{X_{t + h}}} \right) = \left( {\theta^2{ \boldsymbol{1}_{\left\{ {h = 0} \right\}}} + {\theta}{\boldsymbol{1}_{\left\{ {h = 1} \right\}}} + {\theta}{\boldsymbol{1}_{\left\{ {h =  - 1} \right\}}} + {\boldsymbol{1}_{\left\{ {h = 0} \right\}}}} \right)\sigma _w^2\] 
implying the following autocovariance function:
  \[\gamma \left( h \right) = \left\{ {\begin{array}{*{20}{c}}
    {\left( {\theta^2 + 1} \right)\sigma _w^2}&{h = 0} \\ 
    {{\theta}\sigma _w^2}&{\left| h \right| = 1} \\ 
    0&{\left| h \right| > 1} 
    \end{array}} \right. .\]
Therefore, an MA(1) process is weakly stationary since both the mean and variance are constant over time and its covariance function is only a function of the lag $(h)$. Finally, we can easily obtain the autocorrelation for this process, which is given by
$$\rho \left( h \right) = \left\{ {\begin{array}{*{20}{c}}
  1&{h = 0} \\ 
  {\frac{{{\theta}\sigma _w^2}}{{\left( {\theta^2 + 1} \right)\sigma _w^2}} = \frac{{{\theta}}}{{\theta^2 + 1}}}&{\left| h \right| = 1} \\ 
  0&{\left| h \right| > 1} 
  \end{array}} \right. .$$
    Interestingly, we can note that $|\rho(1)| \leq 0.5$.
```

```{example, label="exar1", name = "Autoregressive of Order 1"}
  As another example, we shall verify the stationary properties for the AR(1)
  model defined in the previous chapter.
  
  Using the *backsubstitution* technique, we can rearrange an AR(1) process so 
  that it is written in a more compact form, i.e.
  
  \[\begin{aligned}
     {X_t} & =  {\phi }{X_{t - 1}} + {W_t} = \phi \left[ {\phi {X_{t - 2}} + {W_{t - 1}}} \right] + {W_t} 
     =  {\phi ^2}{X_{t - 2}} + \phi {W_{t - 1}} + {W_t}  \\
     &  \vdots  \\
     & =  {\phi ^k}{X_{t-k}} + \sum\limits_{j = 0}^{k - 1} {{\phi ^j}{W_{t - j}}} .
     \end{aligned} \]
  
  By taking the limit in $k$ (which is perfectly valid as we 
                              assume $t \in \mathbb{Z}$) and assuming $|\phi|<1$, we obtain
  
  \[\begin{aligned}
     X_t = \mathop {\lim }\limits_{k \to \infty} \; {X_t}  =  \sum\limits_{j = 0}^{\infty} {{\phi ^j}{W_{t - j}}} 
     \end{aligned} \]
  
  and therefore such process can be interpreted as a linear combination of the 
  white noise $(W_t)$ and corresponds (as we will observe later on) to an MA($\infty$). 
  In addition, the requirement $\left| \phi  \right| < 1$ turns out to be 
  extremely useful as the above formula is related to a **geometric series** which
  would diverge if $\phi \geq 1$ (for example when $\phi = 1$ we have a random walk). Indeed, remember that
  an infinite (converging) geometric series is given by
>>>>>>> cb89d168198f38b1736da9dce9b616b02713003d
  
  \[\sum\limits_{k = 0}^\infty  \, a{{r^k}}  = \frac{a}{{1 - r}}, \; {\text{ if }}\left| r \right| < 1.\]
  
  <!--
    The origin of the requirement comes from needing to ensure that the characteristic polynomial solution for an AR1 lies outside of the unit circle. Subsequently, stability enables the process to be stationary. If $\phi  \ge 1$, the process would not converge. Under the requirement, the process can represented as a
  -->
    
With this setup, we demonstrate how crucial this property is by calculating each of the requirements of a stationary process.
  
  1. First, we will check if the mean is stationary. 
  In this case, we choose to use limits in order to derive the expectation
  \[\begin{aligned}
     \mathbb{E}\left[ {{X_t}} \right] &= \mathop {\lim }\limits_{k \to \infty } \mathbb{E}\left[ {{\phi^k}{X_{t-k}} + \sum\limits_{j = 0}^{k - 1} {\phi^j{W_{t - j}}} } \right] \\
     &= \mathop {\lim }\limits_{k \to \infty } \, \underbrace {{\phi ^k}{\mathbb{E}[X_{t-k}]}}_{= 0} + \mathop {\lim }\limits_{k \to \infty } \, \sum\limits_{j = 0}^{k - 1} {\phi^j\underbrace {\mathbb{E}\left[ {{W_{t - j}}} \right]}_{ = 0}}
     = 0.
     \end{aligned} \] As expected, the mean is zero and, hence, the first criterion for weak stationarity is satisfied. 
  2. Next, we determine the variance of the process
  \[\begin{aligned}
     \text{var}\left( {{X_t}} \right) &= \mathop {\lim }\limits_{k \to \infty } \text{var}\left( {{\phi^k}{X_{t-k}} + \sum\limits_{j = 0}^{k - 1} {\phi^j{W_{t - j}}} } \right)
     = \mathop {\lim }\limits_{k \to \infty } \sum\limits_{j = 0}^{k - 1} {\phi ^{2j} \text{var}\left( {{W_{t - j}}} \right)}  \\
     &= \mathop {\lim }\limits_{k \to \infty } \sum\limits_{j = 0}^{k - 1} \sigma _W^2 \, {\phi ^{2j}}  =  
       \underbrace {\frac{\sigma _W^2}{{1 - {\phi ^2}}}.}_{\begin{subarray}{l} 
         {\text{Geom. Series}} 
         \end{subarray}}
     \end{aligned} \] Once again, the above result only holds because we are able to use the convergence of the geometric series as a result of $\left| \phi  \right| < 1$.
  3. Finally, we consider the autocovariance of an AR(1). For $h > 0$, we have
  \[\gamma \left( h \right) =  \text{cov}\left( {{X_t},{X_{t + h}}} \right) = \phi \text{cov}\left( {{X_t},{X_{t + h - 1}}} \right) = \phi \, \gamma \left( h-1 \right).\]
  Therefore, using the symmetry of autocovariance, we find that
  \[\gamma \left( h \right) = \phi^{|h|} \, \gamma(0).\]
  
  Both the mean and variance do not depend on time. In addition, the autocovariance function can be viewed as a function that only depends on the time lag $h$ and, thus, the AR(1) process is weakly stationary if $\left| \phi  \right| < 1$.  Lastly, we can obtain the autocorrelation for this process. Indeed, for $h > 0$, we have
  
  \[\rho \left( h \right) = \frac{{\gamma \left( h \right)}}{{\gamma \left( 0 \right)}} = \frac{{\phi \gamma \left( {h - 1} \right)}}{{\gamma \left( 0 \right)}} = \phi \rho \left( {h - 1} \right).\]
  
  After simplifying, we obtain
  
  \[\rho \left( h \right) = {\phi^{|h|}}.\]
  
  Thus, the autocorrelation function for an AR(1) exhibits a _geometric decay_,
  meaning that as $|\phi|$ gets smaller the autocorrelation reaches zero at a faster rate (on the contrary, if $|\phi|$ is close to 1 then the decay rate is slower).
```
  
## Estimation of Moments (Stationary Processes)
  
In this section, we discuss how moments and related quantities of stationary
process can be estimated. Informally speaking, the use of "averages" is 
meaningful for such processes suggesting that classical moments estimators can
be employed. Indeed, suppose that one is interested in 
estimating $\alpha \equiv \mathbb{E}[m (X_t)]$, where $m(\cdot)$ is a known
function of $X_t$. If $X_t$ is a strongly stationary process, we have
  
\[\alpha = \int m(x) \, f(x) dx\]
  
where $f(x)$ denotes the density of $X_t, \; \forall t$. Replacing $f(x)$ by
$f_n(x)$, the empirical density, we obtain the following estimator
  
\[\hat{\alpha} = \frac{1}{n} \sum_{i = 1}^n m\left(x_i\right).\]
  
In the next subsection, we examine how this simple idea can be used to estimate
the mean, autocovariance and autocorrelation functions. Moreover, we discuss
some of the properties of these estimators.
  
  
### Estimation of the Mean Function
  
If a time series is stationary, the mean function is constant and a possible
estimator of this quantity is, as discussed above, given by
  
\[\bar{X} = {\frac{1}{n}\sum\limits_{t = 1}^n {{X_t}} }.\]
  
Naturally, the $k$-th moment, say $\beta_k \equiv \mathbb{E}[X_t^k]$ can be
estimated by
  
\[\hat{\beta}_k = {\frac{1}{n}\sum\limits_{t = 1}^n {{X_t^k}} }, \;\; k \in \left\{x \in \mathbb{N} : \, 0 < x < \infty  \right\}.\]
  
The variance of such an estimator can be derived as follows:
    
\begin{equation}
  \begin{aligned}
  \text{var} \left( \hat{\beta}_k \right) &= \text{var} \left( {\frac{1}{n}\sum\limits_{t = 1}^n {{X_t^k}} } \right)  \\
  &= \frac{1}{{{n^2}}}\text{var} \left( {{{\left[ {\begin{array}{*{20}{c}}
    1& \cdots &1
    \end{array}} \right]}_{1 \times n}}{{\left[ {\begin{array}{*{20}{c}}
      {{X_1^k}} \\
      \vdots  \\
      {{X_n^k}}
      \end{array}} \right]}_{n \times 1}}} \right)  \\
  &= \frac{1}{{{n^2}}}{\left[ {\begin{array}{*{20}{c}}
    1& \cdots &1
    \end{array}} \right]_{1 \times n}} \, \boldsymbol{\Sigma}(k) \, {\left[ {\begin{array}{*{20}{c}}
      1 \\
      \vdots  \\
      1
      \end{array}} \right]_{n \times 1}}, 
  \end{aligned}
  (\#eq:chap2VarMoment)
\end{equation}
    
where $\boldsymbol{\Sigma}(k) \in \mathbb{R}^{n \times n}$ and its $i$-th, $j$-th
element is given by
    
\[ \left(\boldsymbol{\Sigma}(k)\right)_{i,j} = \text{cov} \left(X_i^k, X_j^k\right).\]
    
In the case $k = 1$, \@ref(eq:chap2VarMoment) can easily be further simplified.
Indeed, we have
    
\[\begin{aligned}
       \text{var} \left( {\bar X} \right) &= \text{var} \left( {\frac{1}{n}\sum\limits_{t = 1}^n {{X_t}} } \right)  \\
       &= \frac{1}{{{n^2}}}{\left[ {\begin{array}{*{20}{c}}
         1& \cdots &1
         \end{array}} \right]_{1 \times n}}\left[ {\begin{array}{*{20}{c}}
           {\gamma \left( 0 \right)}&{\gamma \left( 1 \right)}& \cdots &{\gamma \left( {n - 1} \right)} \\
           {\gamma \left( 1 \right)}&{\gamma \left( 0 \right)}&{}& \vdots  \\
           \vdots &{}& \ddots & \vdots  \\
           {\gamma \left( {n - 1} \right)}& \cdots & \cdots &{\gamma \left( 0 \right)}
           \end{array}} \right]_{n \times n}{\left[ {\begin{array}{*{20}{c}}
             1 \\
             \vdots  \\
             1
             \end{array}} \right]_{n \times 1}}  \\
       &= \frac{1}{{{n^2}}}\left( {n\gamma \left( 0 \right) + 2\left( {n - 1} \right)\gamma \left( 1 \right) + 2\left( {n - 2} \right)\gamma \left( 2 \right) +  \cdots  + 2\gamma \left( {n - 1} \right)} \right)  \\
       &= \frac{1}{n}\sum\limits_{h =  - n}^n {\left( {1 - \frac{{\left| h \right|}}{n}} \right)\gamma \left( h \right)} .  \\
\end{aligned} \]
    
Obviously, when $X_t$ is a white noise process, the above formula reduces to the
usual $\text{var} \left( {\bar X} \right) = \sigma^2_w/n$. In the following example,
we consider the case of an AR(1) process and discuss 
how $\text{var} \left( {\bar X} \right)$ can be obtained or estimated.
    
```{example, label="exactvbootstrap"}
For an AR(1), we have $\gamma(h) = \phi^h \sigma_w^2 \left(1 - \phi^2\right)^{-1}$. Therefore, we obtain (after some computations):
      
\begin{equation}
    \text{var} \left( {\bar X} \right) = \frac{\sigma_w^2 \left( n - 2\phi - n \phi^2 + 2 \phi^{n + 1}\right)}{n^2\left(1-\phi^2\right)\left(1-\phi\right)^2}.
\end{equation}
    
Unfortunately, deriving such an exact formula is often difficult when considering
more complex models. However, asymptotic approximations are often employed to
simplify the calculation. For example, in our case we have
    
\[\mathop {\lim }\limits_{n \to \infty } \; n \text{var} \left( {\bar X} \right) = \frac{\sigma_w^2}{\left(1-\phi\right)^2},\]
    
providing the following approximate formula:
      
\[\text{var} \left( {\bar X} \right) \approx \frac{\sigma_w^2}{n \left(1-\phi\right)^2}.\]
    
Alternatively, simulation methods can also be employed. For example, a possible
strategy would be parametric bootstrap.
```
  
```{example, label="parabootstrap", "Parametric Bootstrap"}
Parametric bootstrap can be implemented in the following manner:

1. Simulate a new sample under the postulated model, i.e. $X_t^* \sim F_{\boldsymbol{\theta}}$ (*note:* if $\boldsymbol{\theta}$ is unknown it can be replaced by $\hat{\boldsymbol{\theta}}$, a suitable estimator).
2. Compute the statistics of interest on the simulated sample $(X_t^*)$.
3. Repeat Steps 1 and 2 $B$ times where $B$ is sufficiently "large" (typically $100 \leq B \leq 10000$).
4. Compute the empirical variance of the statistics of interest based on the $B$ independent replications. 
```
    
In our example, we would consider $(X_t^*)$ to be ${\bar{X}^*}$ and seek to
obtain:
      
\[\hat{\sigma}^2_B = \frac{1}{B-1} \sum_{i = 1}^B \left(\bar{X}^*_i - \bar{X}^* \right)^2, \;\;\; \text{where} \;\;\; \bar{X}^* = \frac{1}{B} \sum_{i=1}^B \bar{X}^*_i,\]
    
where $\bar{X}^*_i$ denotes the value of the mean estimated on the $i$-th 
simulated sample.
    
The figure below generated by the following code compares these three methods
for $n = 10$, $B = 1000$, $\sigma^2 = 1$ and a grid of values for $\phi$ going
from $-0.95$ to $0.95$:
      
      
```{r estimXbar, cache = TRUE, fig.height = 5, fig.width = 7}
# Define sample size
n = 10

# Number of Monte-Carlo replications
B = 5000

# Define grid of values for phi
phi = seq(from = 0.95, to = -0.95, length.out = 30)

# Define result matrix
result = matrix(NA,B,length(phi))

# Start simulation
for (i in seq_along(phi)){
  # Define model
  model = AR1(phi = phi[i], sigma2 = 1)
  
  # Monte-Carlo
  for (j in seq_len(B)){
    # Simulate AR(1)
    Xt = gen_gts(n, model)
    
    # Estimate Xbar
    result[j,i] = mean(Xt)
  }
}

# Estimate variance of Xbar
var.Xbar = apply(result,2,var)

# Compute theoretical variance
var.theo = (n - 2*phi - n*phi^2 + 2*phi^(n+1))/(n^2*(1-phi^2)*(1-phi)^2)

# Compute (approximate) variance
var.approx = 1/(n*(1-phi)^2)

# Compare variance estimations
plot(NA, xlim = c(-1,1), ylim = range(var.approx), log = "y", 
     ylab = expression(paste("var(", bar(X), ")")),
     xlab= expression(phi), cex.lab = 1)
grid()
lines(phi,var.theo, col = "deepskyblue4")
lines(phi, var.Xbar, col = "firebrick3")
lines(phi,var.approx, col = "springgreen4")
legend("topleft",c("Theoretical variance","Bootstrap variance","Approximate variance"), 
       col = c("deepskyblue4","firebrick3","springgreen4"), lty = 1,
       bty = "n",bg = "white", box.col = "white", cex = 1.2)
```
    
It can be observed that the variance of $\bar{X}$ typically increases with 
$\phi$. As expected when $\phi = 0$, we have $\text{var}(\bar{X}) = 1/n$ — in 
this case the process is a white noise. Moreover, the bootstrap approach 
appears to well approximate the curve of (\@ref(eq:chap2_exAR1)), while the
asymptotic form provides a reasonable approximation when $\phi$ lies between
-0.5 and 0.5. Naturally, the quality of this approximation would be far better
for a larger sample size (here we consider $n = 10$, which is a little "extreme").
    
### Sample Autocovariance and Autocorrelation Functions
    
A natural estimator of the *autocovariance function* is given by:
      
\[\hat \gamma \left( h \right) = \frac{1}{n}\sum\limits_{t = 1}^{n - h} {\left( {{X_t} - \bar X} \right)\left( {{X_{t + h}} - \bar X} \right)} \]
    
leading to the following "plug-in" estimator of the *autocorrelation function*:
      
\[\hat \rho \left( h \right) = \frac{{\hat \gamma \left( h \right)}}{{\hat \gamma \left( 0 \right)}}.\]
    
A graphical representation of the autocorrelation function is often the first
step for any time series analysis (again assuming the process to be stationary).
Consider the following simulated example:
      
```{r basicACF, cache = TRUE, fig.height = 4.5, fig.width = 7}
# Set seed for reproducibility
set.seed(2241)

# Simulate 100 observation from a Gaussian white noise
Xt = gen_gts(100, WN(sigma2 = 1))

# Compute autocorrelation
acf_Xt = ACF(Xt)

# Plot autocorrelation
plot(acf_Xt, show.ci = FALSE)
```
    
In this example, the true autocorrelation is equal to zero at any 
lag $h \neq 0$, but obviously the estimated autocorrelations are random variables
and are not equal to their true values. It would therefore be useful to have
some knowledge about the variability of the sample autocorrelations (under some
conditions) to assess whether the data comes from a completely random series or
presents some significant correlation at certain lags. The following result
provides an asymptotic solution to this problem:

```{theorem, label="approxnormal"}
If $X_t$ is a strong white noise with finite fourth moment,
then $\hat{\rho}(h)$ is approximately normally distributed with mean $0$ and 
variance $n^{-1}$ for all fixed $h$.
```
    
The proof of this Theorem is given in Appendix \@ref(appendixa).
    
Using this result, we now have an approximate method to assess whether peaks in
the sample autocorrelation are significant by determining whether the observed
peak lies outside the interval $\pm 2/\sqrt{n}$ (i.e. an approximate 95%
confidence interval). Returning to our previous example and adding confidence
bands to the previous graph, we obtain:
      
```{r basicACF2, cache = TRUE, fig.height = 4.5, fig.width = 7}
# Plot autocorrelation with confidence bands 
plot(acf_Xt)
```
    
It can now be observed that most peaks lie within the 
interval $\pm 2/\sqrt{n}$ suggesting that the true data generating process 
is uncorrelated.
    
```{example, label = "acffeatures"}
To illustrate how the autocorrelation function can be used to reveal some
"features" of a time series, we download the level of the Standard & Poor's 500
index, often abbreviated as the S&P 500. This financial index is based on the
market capitalization of 500 large companies having common stock listed on
the New York Stock Exchange or the NASDAQ Stock Market. The graph below 
shows the index level and daily returns from 1990.
```
  

```{r GSPC, cache = TRUE, fig.height = 4, fig.width = 8, warnings = FALSE, message = FALSE}
# Load package
library(quantmod)

# Download S&P index
getSymbols("^GSPC", from="1990-01-01", to = Sys.Date())

# Compute returns
GSPC.ret = ClCl(GSPC)

# Plot index level and returns
par(mfrow = c(1,2))
plot(GSPC, main = " ", ylab = "Index level")
plot(GSPC.ret, main = " ", ylab = "Daily returns")
```

From these graphs, it is clear that the returns are not identically distributed
as the variance seems to change over time and clusters with either high or low 
volatility can be observed. These characteristics of financial time series are 
well known and further on in this book we will discuss how the variance of such 
processes can be approximated. Nevertheless, we compute the empirical
autocorrelation function of the S&P 500 return to evaluate the degree 
of "linear" dependence between observations. The graph below presents 
the empirical autocorrelation.

```{r GSPCacf, cache = TRUE, fig.height = 4.5, fig.width = 7, warning = FALSE}
sp500 = na.omit(GSPC.ret)
names(sp500) = paste("S&P 500 (1990-01-01 - ",Sys.Date(),")", sep = "")
plot(ACF(sp500))
```

As expected, the autocorrelation is small but it might be reasonable to believe
that this sequence is not purely uncorrelated. Unfortunately, Theorem \@ref(approxnormal) is based on an asymptotic argument and since the
confidence bands constructed are also asymptotic, there are no "exact" tools
that can be used in this case. To study the validity of these results when $n$ is
"small" we can perform a simulation study. In the latter, we simulate processes from a Gaussian white noise and examine the empirical distribution of
$\hat{\rho}(3)$ with different sample sizes (i.e. $n$ is set to 5, 10, 30 and
300). Intuitively, the "quality" of the approximation provided by Theorem \@ref(approxnormal)
should increase with the sample size $n$. The code below performs such a simulation
and compares the empirical distribution of $\sqrt{n} \hat{\rho}(3)$ with a
normal distribution with mean 0 and variance 1 (its asymptotic
distribution), which is depicted using a red line.

```{r simulationACF, cache = TRUE, fig.height = 5.5, fig.width = 7}
# Number of Monte Carlo replications
B = 10000

# Define considered lag
h = 3

# Sample size considered
N = c(5, 10, 30, 300)

# Initialisation
result = matrix(NA,B,length(N))

# Set seed
set.seed(1)

# Start Monte Carlo
for (i in seq_len(B)){
  for (j in seq_along(N)){
    # Simluate process
    Xt = rnorm(N[j])
    
    # Save autocorrelation at lag h
    result[i,j] = acf(Xt, plot = FALSE)$acf[h+1]
  }
}

# Plot results
par(mfrow = c(2,length(N)/2))
for (i in seq_along(N)){
  # Estimated empirical distribution
  hist(sqrt(N[i])*result[,i], col = "royalblue1", 
       main = paste("Sample size n =",N[i]), probability = TRUE,
       xlim = c(-4,4), xlab = " ")
  
  # Asymptotic distribution
  xx = seq(from = -10, to = 10, length.out = 10^3)
  yy = dnorm(xx,0,1)
  lines(xx,yy, col = "red", lwd = 2)
}
```

As expected, it can clearly be observed that the asymptotic approximation is
quite poor when $n = 5$ but as the sample size increases the approximation
improves and is very close when, for example, $n = 300$. Therefore, this simulation would
suggest that Theorem \@ref(approxnormal) provides a relatively "close" approximation of the
distribution of $\hat{\rho}(h)$, especially when the sample size is large enough.

### Robustness Issues
  
The data generating process delivers a theoretical autocorrelation
(autocovariance) function that, as explained in the previous section,
can then be estimated through the sample autocorrelation (autocovariance)
functions. However, in practice, the sample is often issued from a data 
generating process that is "close" to the true one, meaning that the sample
suffers from some form of small contamination. This contamination is typically
represented by a small amount of extreme observations that are called "outliers"
that come from a process that is different from the true data generating process.

The fact that the sample can suffer from outliers implies that the standard
estimation of the autocorrelation (autocovariance) functions through the sample
functions could be highly biased. The standard estimators presented in the
previous section are therefore not "robust" and can behave badly when the sample
suffers from contamination. To illustrate this limitation for a classical estimator,
we consider the following two processes:
  
\[ 
    \begin{aligned}
    X_t &= \phi X_{t-1} + W_t, \;\;\; W_t \sim \mathcal{N}(0,\sigma_w^2),\\
    Y_t &= \begin{cases}
    X_t       & \quad \text{with probability } 1 - \epsilon\\
    U_t  & \quad \text{with probability } \epsilon\\
    \end{cases}, \;\;\; U_t \sim \mathcal{N}(0,\sigma_u^2),
    \end{aligned}
\] 

where $\epsilon$ is "small" and $\sigma_u^2 \gg \sigma_w^2$. The process $(Y_t)$
can be interpreted as a "contaminated" version of $(X_t)$ and the figure below
represents one realization of the process $(Y_t)$ using the 
following setting: $n = 100$, $\sigma_u^2 = 10$, $\phi = 0.9$, $\sigma_w^2 = 1$ as 
well as $\alpha = 0.05$.

```{r Yt, cache = TRUE, fig.height = 8, fig.width = 7}
# Set seed for reproducibility
set.seed(2241)

# Define length of time series
N = 100

# Select observations from contamination distribution
epsilon = 0.05
index = sample(1:N, round(epsilon*N))
Ut = gen_gts(N, WN(sigma2 = 10))

# Simulate observations from Xt and Yt
Xt = gen_gts(N, AR1(phi = 0.9, sigma2 = 1))
Yt = Xt
Yt[index] = Ut[index]

# Plot time series
par(mfrow = c(1,1))
plot(Yt, main = "Contaminated Time Series Yt")
```

The first question we can ask ourselves looking at this figure is: where are the outliers? `r emo::ji("thinking_face")` You can probably spot a few but there are 5 outliers. If you're having difficulties detecting the outliers don't worry: it's a commonly known phenomenon in time series that detecting outliers is not always easy. Indeed, when looking at a time series for the first time it's not straightforward to understand if there's any form of contamination. Let's now compare $X_t$ and $Y_t$ in the following plots. 

```{r XtYt, cache = TRUE, fig.height = 8, fig.width = 7}
# Plot time series
par(mfrow = c(2,1))
plot(Xt, main = "Original Time Series Xt")
plot(Yt, main = "Contaminated Time Series Yt")
points(index-1, Yt[index], col = "red")
```

In this case it's more simple to detect the outliers (that are also highlighted with the red dots) since we can compare $Y_t$ with the original uncontaminated time series $X_t$. Having highlighted how exploratory analysis can provide limited information on the presence of contamination in an observed time series, we now consider a simulated example to highlight how the performance of a
"classical" autocorrelation estimator can deteriorate if the sample is contaminated (i.e.
what is the impact of $Y_t$ on the ACF estimator). In this simulation, we will use the setting presented above and
consider $B = 10^3$ bootstrap replications comparing the performance of the classical estimator when applied to an uncontaminated time series ($X_t$) and a contaminated time series ($Y_t$).

```{r simulationRobust, cache = TRUE, fig.height = 5.5, fig.width = 7}
B = 1000 # Number of simulations
N = 100 # Length of time series
epsilon = 0.05 # Amount of contamination
phi = 0.9 # AR(1) parameter value

# Store first 15 values of Empirical ACF (for Xt and Yt)
acf_xt = acf_yt = matrix(NA, B, 15)

for(i in 1:B) {
  
  # Set seed for reproducibility
  set.seed(i + 2241)

  # Select observations from contamination distribution
  index = sample(1:N, round(epsilon*N))
  Ut = gen_gts(N, WN(sigma2 = 10))

  # Simulate observations from Xt and Yt
  Xt = gen_gts(N, AR1(phi = 0.9, sigma2 = 1))
  Yt = Xt
  Yt[index] = Ut[index]
  
  # Store ACF values
  acf_xt[i, ] = as.numeric(ACF(Xt))[1:15]
  acf_yt[i, ] = as.numeric(ACF(Yt))[1:15]
  
}

# Compute the Theoretical ACF of an AR(1) model (up to lag 15)
true_acf = phi^(1:15)

# Make boxplots of Empirical ACF for both settings and compare with true ACF
par(mfrow = c(1,2))
boxplot(acf_xt[, 3], col = "grey80", main = "ACF at lag 3 (uncontaminated)")
abline(h = true_acf[3], col = "red")
boxplot(acf_yt[, 3], col = "grey80", main = "ACF at lag 3 (contaminated)")
abline(h = true_acf[3], col = "red")
```

The boxplots represent the empirical distribution of the ACF estimator for time-lag $h = 3$: the left boxplot shows how the standard autocorrelation estimator is
centered around the true value (red line) when the sample is not 
contaminated while the right boxplot shows how this estimate is considerably biased when the sample is contaminated. Indeed, it can 
be seen how the boxplot under contamination shows a lower value of autocorrelation indicating that it does not detect much dependence in the data although it should. The latter phenomenon is even more evident when analysing the empirical distributions at the larger lags. This is a known result in robustness, more specifically that outliers in the data can break the dependence structure and make it more difficult for the latter to be detected.

In order to limit this problem, different robust estimators exist for time
series problems which are designed to reduce the impact of contamination on 
the estimation procedure. Among these estimators, there are a few that estimate the
autocorrelation (autocovariance) functions in a robust manner. One of these 
estimators is provided in the ``robacf()`` function in the "robcor" package. 
The following simulated example shows how it limits bias from contamination. 
Unlike in the previous simulation, we shall only consider data issued 
from the contaminated model, $Y_t$, and compare the performance of two 
estimators (i.e. classical and robust autocorrelation estimators):
  
```{r simulationRobust2, cache = TRUE, fig.height = 5.5, fig.width = 7}
B = 1000 # Number of simulations
N = 100 # Length of time series
epsilon = 0.05 # Amount of contamination
phi = 0.9 # AR(1) parameter value

# Store first 15 values of Empirical ACF (classic and robust)
cl_acf = rob_acf = matrix(NA, B, 15)

for(i in 1:B) {
  
  # Set seed for reproducibility
  set.seed(i + 2241)

  # Select observations from contamination distribution
  index = sample(1:N, round(epsilon*N))
  Ut = gen_gts(N, WN(sigma2 = 10))

  # Simulate observations from  Yt
  Yt = gen_gts(N, AR1(phi = 0.9, sigma2 = 1))
  Yt[index] = Ut[index]
  
  # Store Classic and Robust ACF values
  cl_acf[i, ] = as.numeric(ACF(Yt))[1:15]
  rob_acf[i, ] = robacf(Yt, plot = FALSE)$acf[1:15]
  
}

# Compute the Theoretical ACF of an AR(1) model (up to lag 15)
true_acf = phi^(1:15)

# Make boxplots of both Classic and Robust Empirical ACF and compare with true ACF
par(mfrow = c(1,2))
boxplot(cl_acf[, 2], col = "grey80", main = "Classic ACF at lag 2")
abline(h = true_acf[2], col = "red")
boxplot(rob_acf[, 2], col = "grey80", main = "Robust ACF at lag 2")
abline(h = true_acf[2], col = "red")
```

In this case we see the empirical distributions of the two estimators (classic and robust) for the ACF at time-lag $h = 2$. As you can see, the robust estimator remains close to the true value represented by the red line
in the boxplots as opposed to the standard estimator. However, the price to pay in terms of bias reduction (under contamination) is a loss of efficiency of the robust estimator. Indeed it can often be observed that to reduce the bias induced by contamination in the sample, robust estimators pay a certain price in terms of efficiency.

To assess how much is "lost" by the robust estimator compared to the classical
one in terms of efficiency, we consider one last simulation where we examine 
the performance of two estimators on data issued from the uncontaminated model,
i.e. $(X_t)$. Therefore, the only difference between this simulation and the 
previous one is the value of $\epsilon$ set equal to $0$ (the code shall thus be omitted). For this reason we simply show a plot that represents the ratio of the variances of the classic and robust ACF estimators respectively. If they happened to have the same efficiency, we would expect this ratio to be roughly equal to 1 over all the considered lags (we omit this ratio at lag 1 since it is numerically impossible to represent due to the size of the empirical variances).
  
```{r simulationRobust3, cache = TRUE, echo = FALSE, fig.height = 5.5, fig.width = 7}
B = 1000 # Number of simulations
N = 100 # Length of time series
phi = 0.9 # AR(1) parameter value

# Store first 15 values of Empirical ACF (classic and robust)
cl_acf = rob_acf = matrix(NA, B, 15)

for(i in 1:B) {
  
  # Set seed for reproducibility
  set.seed(i + 2241)

  # Simulate observations from  Xt
  Xt = gen_gts(N, AR1(phi = 0.9, sigma2 = 1))
  
  # Store Classic and Robust ACF values
  cl_acf[i, ] = as.numeric(ACF(Xt))[1:15]
  rob_acf[i, ] = robacf(Xt, plot = FALSE)$acf[1:15]
  
}

# Compute the variance ratio (Classic/Robust) (from lags 2 to 15)
eff = apply(cl_acf[, 2:15], 2, var)/apply(rob_acf[, 2:15], 2, var)

# Plot the efficiency over the time lags 2 to 15
plot(2:15, eff, main = "Ratio of Empirical Variances of Classic and Robust ACF", ylim = c(0.5, 1.1), ylab = "Efficiency", xlab = "Time Lags (h)", type = "l")
abline(h = 1, lty = 2, col = "red")
```

As can be seen on the plot, the black line representing the above described ratio decreases steadily as the lags increase and moves further away from the red dotted line representing equality of variances. Hence, since the variance of the classic ACF estimator is in the numerator of this ratio, we can conclude that the variance of the robust ACF estimator is always larger and increases over the lags. This can partly be explained by the fact that the classic ACF estimator makes use of all the observations while the robust estimator only uses part of the information coming from more "extreme" observations. Moreover, the number of observations available to estimate the greater lags is smaller and therefore the robust ACF estimator pays a larger price in terms of "loss" of information.

<!--chapter:end:02-fundamental_rep.Rmd-->

`r if (knitr::is_html_output()) '# References {-}'`

<!--chapter:end:33-Ref.Rmd-->

# (APPENDIX) Appendix {-} 

<!--chapter:end:90-appendix.Rmd-->

# Proofs {#appendixa}

## Proof of Theorem 1

We let $X_t = W_t + \mu$, where $\mu < \infty$ and $(W_t)$ is a strong white noise process with variance $\sigma^2$ and finite fourth moment (i.e. $\mathbb{E} [W_t^4] < \infty$). 

Next, we consider the sample autocovariance function computed on $(X_t)$, i.e.

$$
\hat \gamma \left( h \right) = \frac{1}{n}\sum\limits_{t = 1}^{n - h} {\left( {{X_t} - \bar X} \right)\left( {{X_{t + h}} - \bar X} \right)}.
$$

For this equation, it is clear that $\hat \gamma \left( 0 \right)$ and $\hat \gamma \left( h \right)$ (with $h > 0$) are two statistics involving sums of different lengths. As we will see, this prevents us from using directly the multivariate central limit theorem on the vector $[ \hat \gamma \left( h \right) \;\;\; \hat \gamma \left( h \right) ]^T$. However, the lag $h$ is fixed and therefore the difference in the number of elements of both sums is asymptotically negligible. Therefore, we define a new statistic

$$\tilde{\gamma} \left( h \right) = \frac{1}{n}\sum\limits_{t = 1}^{n} {\left( {{X_t} - \mu} \right)\left( {{X_{t + h}} - \mu} \right)},
$$

which, as we will see, is easier to used and show that $\hat \gamma \left( h \right)$ and $\tilde{\gamma} \left( h \right)$ are asymptotically equivalent in the sense that:

$$
n^{\frac{1}{2}}[\tilde{\gamma} \left( h \right) - \hat \gamma \left( h \right)] = o_p(1).
$$

Therefore, assuming this results to be true, $\tilde{\gamma} \left( h \right)$ and $\hat \gamma \left( h \right)$ would have the same asymptotic distribution, it is sufficient to show the asymptotic distribution of $\tilde{\gamma} \left( h \right)$. So that before continuing the proof the Theorem 1 we first state and prove the following lemma:

**Lemma A1:** Let 

$$
X_t = \mu + \sum\limits_{j = -\infty}^{\infty} \psi_j W_{t-j},
$$
where $(W_t)$ is a strong white process with variance $\sigma^2$, and the coefficients satisfying $\sum \, |\psi_j| < \infty$. Then, we have

$$
n^{\frac{1}{2}}[\tilde{\gamma} \left( h \right) - \hat \gamma \left( h \right)] = o_p(1).
$$

*Proof:* By Markov inequality, we have

$$
\mathbb{P}\left( |n^{\frac{1}{2}}[\tilde{\gamma} \left( h \right) - \hat \gamma \left( h \right)]| \geq \epsilon \right) \leq \frac{\mathbb{E}|n^{\frac{1}{2}}[\tilde{\gamma} \left( h \right) - \hat \gamma \left( h \right)]|}{\epsilon},
$$
for any $\epsilon > 0$. Thus, it is enough to show that 

\[\mathop {\lim }\limits_{n \to \infty } \; \mathbb{E} \left[|n^{\frac{1}{2}}[\tilde{\gamma} \left( h \right) - \hat \gamma \left( h \right)]|\right] = 0\]

to prove Lemma A1.By the definitions of $\tilde{\gamma} \left( h \right)$ and $\hat \gamma \left( h \right)$, we have
  
$$
\begin{aligned}
n^{\frac{1}{2}}[\tilde{\gamma} \left( h \right) - \hat \gamma \left( h \right)] &= \frac{1}{\sqrt{n}} \sum_{t = n-h+1}^{n}(X_t - \mu)(X_{t+h} - \mu) \\
&+ \frac{1}{\sqrt{n}} \sum_{t = 1}^{n-h}\left[(X_t - \mu)(X_{t+h} - \mu) - (X_t - \bar{X})(X_{t+h} - \bar{X})\right]\\
&= \frac{1}{\sqrt{n}} \sum_{t = n-h+1}^{n}(X_t - \mu)(X_{t+h} - \mu)  
+ \frac{1}{\sqrt{n}} \sum_{t = 1}^{n-h}\left[(\bar{X} - \mu)(X_t + X_{t+h} - \mu - \bar{X})\right]\\
&= \frac{1}{\sqrt{n}} \sum_{t = n-h+1}^{n} (X_t - \mu)(X_{t+h} - \mu) + \frac{1}{\sqrt{n}} (\bar{X} - \mu)\sum_{t = 1}^{n-h}(X_t + X_{t+h} - \mu - \bar{X})\\
&= \frac{1}{\sqrt{n}} \sum_{t = n-h+1}^{n} (X_t - \mu)(X_{t+h} - \mu) + \frac{1}{\sqrt{n}} (\bar{X} - \mu)\left[\sum_{t = 1+h}^{n-h}X_t - (n-h)\mu + h\bar{X}\right]\\
&= \frac{1}{\sqrt{n}} \sum_{t = n-h+1}^{n} (X_t - \mu)(X_{t+h} - \mu)
+ \frac{1}{\sqrt{n}} (\bar{X} - \mu)\left[\sum_{t = 1+h}^{n-h}(X_t - \mu) - h(\mu - \bar{X})\right]\\
&= \frac{1}{\sqrt{n}} \sum_{t = n-h+1}^{n} (X_t - \mu)(X_{t+h} - \mu) + \frac{1}{\sqrt{n}} (\bar{X} - \mu)\sum_{t = 1+h}^{n-h}(X_t - \mu) + \frac{h}{\sqrt{n}} (\bar{X} - \mu)^2,
\end{aligned}
$$
where $\bar{X} = \frac{1}{n}\sum_{t=1}^n X_t = \mu + \frac{1}{n}\sum_{t=1}^n\sum_{j=-\infty}^{\infty} \psi_j W_{t-j} = \mu + \frac{1}{n} \sum_{j = -\infty}^{\infty} \sum_{t=1}^n \psi_j W_{t-j}$.

Then, we have
$$
\begin{aligned}
\mathbb{E}\left[\left|n^{\frac{1}{2}}[\tilde{\gamma} \left( h \right) - \hat \gamma \left( h \right)]\right|\right]
&\leq \frac{1}{\sqrt{n}} \sum_{t = n-h+1}^{n} \mathbb{E}\left[\left|(X_t - \mu) \, (X_{t+h} - \mu)\right|\right]\\
&+ \frac{1}{\sqrt{n}} \mathbb{E} \left[\left|(\bar{X} - \mu) \, \sum_{t = 1+h}^{n-h}(X_t - \mu)\right|\right] +  \frac{h}{\sqrt{n}}\mathbb{E} \left[ (\bar{X} - \mu)^2 \right].
\end{aligned}
$$

Next, we consider each term of the above equation. For the first term, since $(X_t - \mu)^2 = \left(\sum_{j = -\infty}^{\infty} \psi_j W_{t-j}\right)^2$, and $\mathbb{E}[W_iW_j] \neq 0$ only if $i = j$. By Cauchy–Schwarz inequality we have

$$
\mathbb{E}\left[|(X_t - \mu)(X_{t+h} - \mu)|\right] \leq \sqrt{\mathbb{E}\left[|(X_t - \mu)|^2\right] \mathbb{E}\left[|(X_{t+h} - \mu)|^2\right]} = \sigma^2 \sum_{i = -\infty}^{\infty}\psi_i^2.
$$

Then, we consider the third term, since it will be used in the second term

$$\mathbb{E}[(\bar{X} - \mu)^2] = \frac{1}{n^2} \sum_{t = 1}^{n} \sum_{i = -\infty}^{\infty} \psi_i^2 \mathbb{E}\left[ W_{t-i}^2 \right] = \frac{\sigma^2}{n} \sum_{i = -\infty}^{\infty}\psi_i^2.$$

Similarly, for the second term we have

$$\begin{aligned}
\mathbb{E}\left[\left|(\bar{X} - \mu) \sum_{t = 1+h}^{n-h}(X_t - \mu)\right|\right] &\leq \sqrt{\mathbb{E}\left[|(\bar{X} - \mu)|^2\right] \mathbb{E}\left[|\sum_{t = 1+h}^{n-h}(X_t - \mu)|^2\right]}\\
&= \sqrt{\mathbb{E}\left[(\bar{X} - \mu)^2\right] \mathbb{E}\left[\sum_{t = 1+h}^{n-h}\left(X_t - \mu \right)^2 + \sum_{t_1 \neq t_2}(X_{t_1} - \mu)(X_{t_2} - \mu) \right]}\\
&\leq \sqrt{\frac{\sigma^2}{n} \sum_{i = -\infty}^{\infty}\psi_i^2 \cdot (n-2h)\sigma^2 \left( \sum_{j = -\infty}^{\infty} |\psi_j| \right)^2}\\
&\leq \sqrt{\frac{n-2h}{n}}\sigma^2 \left(\sum_{i = -\infty}^{\infty}|\psi_i| \right)^2.
\end{aligned}
$$

Combining the above results we obtain

$$\begin{aligned}
\mathbb{E}|n^{\frac{1}{2}}[\tilde{\gamma} \left( h \right) - \hat \gamma \left( h \right)]|
&\leq \frac{1}{\sqrt{n}} h \sigma^2 \sum_{i = -\infty}^{\infty}\psi_i^2 + \sqrt{\frac{n-2h}{n^2}}\sigma^2 \left(\sum_{i = -\infty}^{\infty}|\psi_i| \right)^2 + \frac{h}{n\sqrt{n}}\sigma^2 \sum_{i = -\infty}^{\infty}\psi_i^2\\
&\leq \frac{1}{n\sqrt{n}} (nh + \sqrt{n - 2h} + h) \sigma^2 \left(\sum_{i = -\infty}^{\infty}|\psi_i|\right)^2,
\end{aligned}
$$

By the taking the limit in $n$ we have

$$\mathop {\lim }\limits_{n \to \infty } \; \mathbb{E} \left[|n^{\frac{1}{2}}[\tilde{\gamma} \left( h \right) - \hat \gamma \left( h \right)]|\right] \leq \sigma^2 \left(\sum_{i = -\infty}^{\infty}|\psi_i|\right)^2 \mathop {\lim }\limits_{n \to \infty } \; \frac{nh + \sqrt{n - 2h} + h}{n\sqrt{n}} = 0.
$$

We can therefore conclude that

$$\sqrt{n}[\tilde{\gamma} \left( h \right) - \hat \gamma \left( h \right)] = o_p(1),$$

which concludes the proof of Lemma A1. $\;\;\;\;\;\;\;\; \blacksquare$  
  
$\\$ 

$\\$ 

Returning to the proof of Theorem 1, since the process $(Y_t)$, where $Y_t = \left( {{X_t} - \mu} \right)\left( {{X_{t + h}} - \mu} \right)$, is iid, we can apply multivariate central limit theorem to the vector $[ \tilde \gamma \left( h \right) \;\;\; \tilde \gamma \left( h \right) ]^T$, and we obtain

$$\begin{aligned}
    \sqrt{n}\left\{
        \begin{bmatrix}
         \tilde{\gamma} \left( 0 \right) \\
         \tilde{\gamma} \left( h \right)
        \end{bmatrix}
    - \mathbb{E}\begin{bmatrix}
         \tilde{\gamma} \left( 0 \right) \\
         \tilde{\gamma} \left( h \right)
        \end{bmatrix} \right\} 
    &= \frac{1}{\sqrt{n}}\begin{bmatrix}
         \sum\limits_{t = 1}^{n}(X_t - \mu)^2 - n\mathbb{E}\left[ \tilde{\gamma} \left( 0 \right) \right]\\
         \sum\limits_{t = 1}^{n}\left( {{X_t} - \mu} \right)\left( {{X_{t + h}} - \mu} \right) - n\mathbb{E}\left[ \tilde{\gamma} \left( h \right) \right]
        \end{bmatrix} \\
       & \overset{\mathcal{D}}{\to} 
    \mathcal{N}\left(0, n \, \text{var} \left(\begin{bmatrix}
         \tilde{\gamma} \left( 0 \right) \\
         \tilde{\gamma} \left( h \right)
        \end{bmatrix} \right)\right)
        \end{aligned}
$$

Moreover, by Cauchy–Schwarz inequality and since $\text{var}(X_t) = \sigma^2$, we have

$$
\sum\limits_{t = 1}^{n} {\left( {{X_t} - \mu} \right)\left( {{X_{t + h}} - \mu} \right)} \leq \sqrt{\sum\limits_{t = 1}^{n} {\left( {{X_t} - \mu} \right)^2} \sum\limits_{t = 1}^{n} {\left( {{X_{t + h}} - \mu} \right)^2}} < \infty.
$$

Therefore, by bounded convergence theorem and $(W_t)$ is iid, we have

$$\begin{aligned}
    \mathbb{E}[\tilde{\gamma} \left( h \right)] &= \frac{1}{n}\mathbb{E}\left[\sum\limits_{t = 1}^{n} {\left( {{X_t} - \mu} \right)\left( {{X_{t + h}} - \mu} \right)}\right]\\
    &= \frac{1}{n}\left[\sum\limits_{t = 1}^{n} { \mathbb{E}\left( {{X_t} - \mu} \right)\mathbb{E}\left( {{X_{t + h}} - \mu} \right)}\right] =
    \begin{cases}
        \sigma^2, & \text{for } h = 0\\
        0, & \text{for } h \neq 0
    \end{cases}.
    \end{aligned}
$$


Next, we consider the variance of $\tilde{\gamma} \left( h \right)$ when $h \neq 0$,

$$
\begin{aligned}
        var[\tilde{\gamma} \left( h \right)] &= \frac{1}{n^2}\mathbb{E}\left\{\left[\sum\limits_{t = 1}^{n} {\left( {{X_t} - \mu} \right)\left( {{X_{t + h}} - \mu} \right)}\right]^2\right\}\\
        &= \frac{1}{n^2}\mathbb{E}\left\{\left[\sum\limits_{i = 1}^{n} {\left( {{X_i} - \mu} \right)\left( {{X_{i + h}} - \mu} \right)}\right] \left[\sum\limits_{j = 1}^{n} {\left( {{X_j} - \mu} \right)\left( {{X_{j + h}} - \mu} \right)}\right]\right\}\\
        &= \frac{1}{n^2}\mathbb{E}\left[\sum\limits_{i = 1}^{n}\sum\limits_{j = 1}^{n} {\left( {{X_i} - \mu} \right)\left( {{X_{i + h}} - \mu} \right)}{\left( {{X_j} - \mu} \right)\left( {{X_{j + h}} - \mu} \right)}\right].
    \end{aligned}
$$

Also by Cauchy–Schwarz inequality and the finite fourth moment assumption, we can use the bounded convergence theorem. Once again since $(W_t)$ is white noise process, we have 

$$
\mathbb{E}\left[{\left( {{X_i} - \mu} \right)\left( {{X_{i + h}} - \mu} \right)}{\left( {{X_j} - \mu} \right)\left( {{X_{j + h}} - \mu} \right)}\right] \neq 0
$$
only when $i = j$.

Therefore, we obtain

$$\begin{aligned}
        var[\tilde{\gamma} \left( h \right)] &= \frac{1}{n^2}\sum\limits_{i = 1}^{n} \mathbb{E}\left[ {\left( {{X_i} - \mu} \right)^2\left( {{X_{i + h}} - \mu} \right)^2}\right]\\
        &= \frac{1}{n^2}\sum\limits_{i = 1}^{n} \mathbb{E}{\left( {{X_i} - \mu} \right)^2\mathbb{E}\left( {{X_{i + h}} - \mu} \right)^2}
        = \frac{1}{n}\sigma^4.
    \end{aligned}
$$

Similarly, for $h = 0$, we have

$$
\begin{aligned}
        var[\tilde{\gamma} \left( 0 \right)] &= \frac{1}{n^2}\mathbb{E}\left\{\left[\sum\limits_{t = 1}^{n} {\left( {{X_t} - \mu} \right)^2}\right]^2\right\} - \frac{1}{n^2}\left[\mathbb{E}\sum\limits_{t = 1}^{n} {\left( {{X_t} - \mu} \right)^2}\right]^2
        = \frac{2}{n}\sigma^4.
    \end{aligned}
$$

Next, we consider the covariance between $\tilde{\gamma} \left( 0 \right)$ and $\tilde{\gamma} \left( h \right)$, for $h \neq 0$, and we obtain

$$
\begin{aligned}
        cov[\tilde{\gamma} \left( 0 \right), \tilde{\gamma} \left( h \right)] &= \mathbb{E}[\tilde{\gamma} \left( 0 \right) \tilde{\gamma} \left( h \right)] - \mathbb{E}[\tilde{\gamma} \left( 0 \right)] \mathbb{E}[\tilde{\gamma} \left( h \right)]
        = \mathbb{E}[\tilde{\gamma} \left( 0 \right) \tilde{\gamma} \left( h \right)]\\
        &= \mathbb{E}\left[\left[\sum\limits_{t = 1}^{n} {\left( {{X_t} - \mu} \right)^2}\right]\left[\sum\limits_{t = 1}^{n} {\left( {{X_t} - \mu} \right)\left( {{X_{t + h}} - \mu} \right)}\right]\right]
        = 0.
    \end{aligned}
$$

Therefore by Slutsky's Theorem we have,

$$
\begin{aligned}
    \sqrt{n}\left\{
        \begin{bmatrix}
         \hat{\gamma} \left( 0 \right) \\
         \hat{\gamma} \left( h \right)
        \end{bmatrix}
    - \begin{bmatrix}
         \sigma^2 \\
         0
        \end{bmatrix} \right\}
    &= \sqrt{n}\left\{
        \begin{bmatrix}
         \tilde{\gamma} \left( 0 \right) \\
         \tilde{\gamma} \left( h \right)
        \end{bmatrix}
    - \begin{bmatrix}
         \sigma^2 \\
         0
        \end{bmatrix} \right\}
    + \underbrace{\sqrt{n}\left\{
        \begin{bmatrix}
         \hat{\gamma} \left( 0 \right) \\
         \hat{\gamma} \left( h \right)
        \end{bmatrix}
    - \begin{bmatrix}
         \tilde{\gamma} \left( 0 \right) \\
         \tilde{\gamma} \left( h \right)
        \end{bmatrix} \right\}}_{\overset{p}{\to} 0}\\
    &\overset{\mathcal{D}}{\to} 
    \mathcal{N}\left(0, \begin{bmatrix}
         2\sigma^4 & 0\\
         0 & \sigma^4
        \end{bmatrix} \right).
    \end{aligned}
$$

Next, we define the function $g\left( \begin{bmatrix}
         a \\
         b
        \end{bmatrix} \right) = b/a$, where $a \neq 0$. For this function it is clear that

$$
\nabla g\left( \begin{bmatrix}
         a \\
         b
        \end{bmatrix} \right) = \begin{bmatrix}
         -\frac{b}{a^2} \\
         \frac{1}{a}
        \end{bmatrix}^{T} ,
$$

and thus using the Delta method, we have for $h \neq 0$

$$
\begin{aligned}
    \sqrt{n}\hat{\rho}(h) =
    \sqrt{n}\left\{g\left(
        \begin{bmatrix}
         \hat{\gamma} \left( 0 \right) \\
         \hat{\gamma} \left( h \right)
        \end{bmatrix} \right)
    - {\mu} \right\}
    &\overset{\mathcal{D}}{\to} 
    \mathcal{N}\left(0, \sigma_r^2 \right),
    \end{aligned}
$$

where 

$$
\begin{aligned}
{\mu} &= g\left(\begin{bmatrix}
         \sigma^2 & 0
        \end{bmatrix} \right) = 0,\\
\sigma_r^2 &= \nabla g\left(\begin{bmatrix}
         \sigma^2 \\
         0
        \end{bmatrix} \right) \begin{bmatrix}
         2\sigma^4 & 0\\
         0 & \sigma^4
        \end{bmatrix} \nabla g\left(\begin{bmatrix}
         \sigma^2 \\
         0
        \end{bmatrix} \right)^{T}
         = \begin{bmatrix}
         0 & \sigma^{-2}
        \end{bmatrix}  \begin{bmatrix}
         2\sigma^4 & 0\\
         0 & \sigma^4
        \end{bmatrix} \begin{bmatrix}
         0 \\
         \sigma^{-2}
        \end{bmatrix} = 1.
    \end{aligned}
$$

Thus, we have

$$
\sqrt{n}\hat{\rho}(h) \overset{\mathcal{D}}{\to} 
    \mathcal{N}\left(0, 1 \right),
$$

which concludes the proof the Theorem 1. $\;\;\;\;\;\;\;\; \blacksquare$



<!--chapter:end:91-appendix-a.Rmd-->

# Robust Regression Methods {#appendixb}

This appendix is largely based on the introduction to linear robust regression
presented in @ronchetti2006historical and @duncan2016ela. In these references
it is stated that the vast majority of the statistical models employed in 
different fields going from finance to biology and engineering, for example,
are parametric models. Based on these models, assumptions are made concerning
the properties of the variables of interest (and the models themselves) and
optimal procedures are derived under these assumptions. Among these procedures,
the least squares and maximum likelihood estimators are well known examples
that, however, are only optimal when the underlying statistical assumptions are
exactly satisfied. If the latter case does not hold, then these procedures can
become considerably biased and/or inefficient when there exist small deviations
from the model. The results obtained by classical procedures can therefore be
misleading when applied to real data (see e.g. @ronchetti2006historical and
@huber2009robust).

In order to address the problems arising from violated parametric assumptions,
robust statistics can be seen as an extension to classical parametric statistics
by directly considering the deviations from the models. Indeed, while parametric
models may be a good approximation of the true underlying situation, robust
statistics does not assume that the model is exactly correct. A robust procedure
as stated in @huber2009robust therefore should have the following features: 

- It should efficiently estimate the assumed model.
- It should be reliable and reasonably efficient under small deviations from the
  model (e.g. when the distribution lies in a neighborhood of the assumed model).
- Larger deviations from the model should not affect the estimation procedure
  excessively.

A robust estimation method is a compromise with respect to these three features.
This compromise is illustrated by @anscombe1960rejection using an insurance
metaphor: "sacrifice some efficiency at the model in order to insure against
accidents caused by deviations from the model".

It is often believed that robust procedures may be avoided by using the 
following two-step procedure:

- Clean the data using some rule for outlier rejection.
- Apply classical optimal procedures on the "clean" data. 

Unfortunately such procedures cannot replace robust methods as discussed in
 @huber2009robust for the following reasons:

- It is rarely possible to seperate the two steps. For example, in a parametric
  regression setting, outliers are difficult to recognize without reliable 
  (i.e. robust) estimates of the model's parameters.
-  The cleaned data will not correspond to the assumed model since there will be
  statistical errors of both kinds (false acceptance and false rejection). 
  Therefore in general the classical theory is not applicable to the cleaned 
  sample.
- Empirically, the best rejection procedures do not reach the performance of 
  the best robust procedures. The latter are apparently superior because they 
  can make a smoother transition between the full acceptance and full rejection
  of an observation using weighting procedures @hampel1987robust.
- Empirical studies have also shown that many of the classical rejection methods
  are unable to deal with multiple outliers. Indeed, it is possible that a
  second outlier "masks" the effect of the first so that neither are rejected.

Unfortunately the least squares estimator suffers from a dramatic lack of
robustness. A single outlier can have an arbitrarily large effect on the
estimated parameters. In order to assess the robustness of an estimator we first
need to introduce an important concept, namely the influence function. This 
concept was introduced in @hampel1968contributions and it formalizes the bias
caused by one outlier. The influence function of an estimator represents the
effect of an infinitesimal contamination at the point $x$ or ($\mathbf{x}$, $y$)
in the regression setting) on the estimate, standardized by the mass of
contamination. Mathematically, the influence function of the estimator $T$ for
the model $F$ is given by:

\[\text{IF}(x| T, F) = \lim_{\varepsilon \rightarrow 0} \frac{T\left((1-\varepsilon) F + \varepsilon \Delta_x \right) - T\left(F\right)}{\varepsilon}\]

where $\Delta_x$ is a probability measure which puts mass $1$ at the point $x$.	

## The Classical Least-Squares Estimator

The standard definition of the linear model is derived as follows. 
Let ${(\mathbf{x}_{i},y_{i}): i = 1, \ldots, n}$ be a sequence of independent 
identically distributed random variables such that:

\[y_{i} = \mathbf{x}_{i}^{T} {\boldsymbol{\beta}} + u_{i}\]

where $y_{i} \in \mathbb{R}$ is the $i$-th observation, $\mathbf{x}_{i} \in \mathbb{R}^{p}$ 
is the $i$-th row of the design matrix $\mathbf{X} \in \mathbb{R}^{n\times p}$, 
$\boldsymbol{\beta} \in \boldsymbol{\Theta} \subseteq \mathbb{R}$ is a *p*-vector of unknown
parameters, $u_{i} \in \mathbb{R}$ is the $i$-th error.   

The least-squares estimator $\hat{\boldsymbol{\beta}}_{LS}$ of $\boldsymbol{\beta}$ can be
expressed as an $M$-estimator^[$M$-estimators are obtained as the
minima of sums of functions of the data.] Least-squares estimators are an
example of the larger class of $M$-estimators. The definition of $M$-estimators
was motivated by robust statistics which delivered new types of $M$-estimators.
defined by the estimating equation:

\begin{equation}
  \sum_{i = 1}^{n} \left(y_{i} - \mathbf{x}_{i}^{T} \boldsymbol{\beta} \right)\mathbf{x}_{i} = 0.
	(\#eq:lsMestim2)
\end{equation}

This estimator is optimal under the following assumptions:

- $u_{i}$ are normally distributed.
- $\mathbb{E}[u_{i}] = 0$ for $i = 1, \ldots, n$.
- $Cov(u_{1}, \ldots, u_{n}) = \sigma^2 \, \mathbf{I}_{n}$ where $\mathbf{I}_{n}$
  denotes the identity matrix of size $n$.

In other words, least-squares estimation is only optimal when the errors are
normally distributed. Small departures from the normality assumption for the
errors results in considerable loss of efficiency of the least-squares estimator
(see @hampel1987robust, @huber1973robust and @hampel1973robust).

## Robust Estimators for Linear Regression Models

The "Huber estimator" introduced in @huber1973robust was one of the first robust
estimation methods applied to linear models. Basically, this estimator is a
weighted version of the least-squares estimate with weights of the form:

\[
w_{i} = \min \left(1,\frac{c}{|r_{i}|}\right)
\]

where $r_{i}$ is the $i$-th residual and $c$ is a positive constant which
controls the trade-off between robustness and efficiency.

Huber proposed an \textit{M}-estimator $\hat{\boldsymbol{\beta}}_{H}$ of $\boldsymbol{\beta}$ 
defined by the estimating equation:

\[
	\sum_{i = 1}^{n} \psi_{c}\left(y_{i} - \mathbf{x}_{i}^{T} \boldsymbol{\beta} \right)\mathbf{x}_{i} = 0
\]

where $\psi_{c}(\cdot)$ corresponds to Huber's weight function 

\begin{equation}
w\left({x}\right) = \begin{cases}
1, &\text{if } \left|{x}\right| \le k \\
\frac{k}{\left|{x}\right|}, &\text{if} \left|{x}\right| > k
\end{cases}
(\#eq:huberweight)
\end{equation}

and, thus, is defined as:

\[
\psi_{c}(r) = \left\{  
\begin{array}{l l}
  r & \quad \\
  c \cdot \text{sign}(r). & \quad \\
\end{array} \right. 
\]

However, the Huber estimator cannot cope with problems caused by outlying points
in the design (or covariate) matrix $X$. An estimator which was developed to
address this particular issue is the one proposed by Mallows which has the 
important property that the influence function is bounded also for the matrix
$X$ (see @krasker1980estimation for more details).

## Applications of Robust Estimation

Having rapidly highlighted the theory of robustness, we now focus on the 
application of robust techniques in practical settings. Over the next three 
examples, estimation between classical or traditional methods will be compared
with  robust methods to illustrate the usefulness of using the latter techniques
can have in specific scenarios. 

```{example, label = "slmrobust"}

Consider a simple linear model with Gaussian errors such as

\begin{equation}
		y_i = \alpha + \beta x_i + \varepsilon_i, \;\; \varepsilon_i \overset{iid}{\sim} \mathcal{N}(0,\sigma^2)
  (\#eq:exam)
\end{equation}

for $i = 1,...,n$. Next, we set the parameter values $\alpha = 5$, 
$\beta = 0.5$ and $\sigma^2 = 1$ in order to simulate 20 observations from
the above simple linear model where we define $x_i = i$ for $i = 1,...,20$. In the left panel of Figure \@ref(fig:slmrobex), we present the simulated observations 
together with the fitted regression lines obtained by least-squares and a
robust estimation method. It can be observed that both lines are very similar
and "close" to the true model given by $y_i = 5 + 0.5 i$. Indeed, although the
robust estimator pays a small price in terms of efficiency compared to the the
least-squares estimator, the two methods generally deliver very "similar" results
when the model assumption holds. On the other hand, the robust estimators provide 
(in general) far more reliable results when outliers are present in a data-set.
To illustrate this behavior we modify the last observation by 
setting $\varepsilon_{20} = -10$ (which is "extreme" under the assumption 
that $\varepsilon_i \overset{iid}{\sim} \mathcal{N}(0,1)$). 
The modified observations are presented in the right panel of 
Figure \@ref(fig:slmrobex) together with the fitted regression lines. 
In this case, the least-squares is strongly influenced by the outlier 
we introduced while the robust estimator remains stable and "close" to the
true model.
```


```{r slmrobex, cache = TRUE, fig.cap="Simulation Study Comparing Robust and Classical Regression Methodologies", fig.width = 13}
# Load robust library
library("robustbase")

# Set seed for reproducibility
set.seed(867)

# Sample size
n = 20      

# Model's parameters
alpha = 5    # Intercept 
beta = 0.5   # Slope 
sig2 = 1     # Residual variance

# Construct response variable y
x = 1:n 
y = alpha + beta*x + rnorm(n,0,sqrt(sig2))

# Construct "perturbed" verison of y
y.perturbed = y
y.perturbed[20] = alpha + beta*x[20] - 10

# Compute LS estimates
LS.y = lm(y ~ x)
LS.y.fit = coef(LS.y)[1] + coef(LS.y)[2]*x
LS.y.pert = lm(y.perturbed ~ x)
LS.y.pert.fit = coef(LS.y.pert)[1] + coef(LS.y.pert)[2]*x

# Compute robust estimates
RR.y = lmrob(y ~ x)
RR.y.fit = coef(RR.y)[1] + coef(RR.y)[2]*x
RR.y.pert = lmrob(y.perturbed ~ x)
RR.y.pert.fit = coef(RR.y.pert)[1] + coef(RR.y.pert)[2]*x

# Define colors
gg_color_hue <- function(n, alpha = 1) {
  hues = seq(15, 375, length = n + 1)
  hcl(h = hues, l = 65, c = 100, alpha = alpha)[1:n]
}
couleurs = gg_color_hue(6)

# Compare results based on y and y.perturbed
par(mfrow = c(1,2))
plot(NA, ylim = range(cbind(y,y.perturbed)), xlim = range(x),
     main = "Uncontaminated setting", xlab = "x", ylab = "y")
grid()
points(x,y, pch = 16, col = couleurs[4])
lines(x, alpha + beta*x, col = couleurs[3], lty = 2)
lines(x, LS.y.fit, col = couleurs[5])
lines(x, RR.y.fit, col = couleurs[1])

legend("topleft",c("Observations","Estimated model (least-squares)",
                   "Estimated model (robust)", "True model"), 
       lwd = c(NA,1,1,1), col = couleurs[c(4,5,1,3)],
       lty = c(NA,1,1,2), bty = "n", pch = c(16,NA,NA,NA))

plot(NA, ylim = range(cbind(y,y.perturbed)), xlim = range(x),
     main = "Contaminated setting", xlab = "x", ylab = "y")
grid()
points(x[1:19],y.perturbed[1:19], pch = 16, col = couleurs[4])
points(x[20], y.perturbed[20], pch = 15, col = couleurs[6])
lines(x, alpha + beta*x, col = couleurs[3], lty = 2)
lines(x, LS.y.pert.fit, col = couleurs[5])
lines(x, RR.y.pert.fit, col = couleurs[1])

legend("topleft",c("Uncontamined observations", "Contamined observation",
                   "Estimated model (least-squares)", "Estimated model (robust)",
                   "True model"), lwd = c(NA,NA,1,1,1), col = couleurs[c(4,6,5,1,3)],
       lty = c(NA,NA,1,1,2), bty = "n", pch = c(16,15,NA,NA,NA))
```

```{example, label="bslmrob", name = "Robust v. Classical Simulation Study."}
The next example presents a simulation study where the robust and classical
techniques will be compared in order to show that the previous example was not 
due to a "lucky" sample favorable to the robust approach. To do so, the 
simulation study will generate 100 realizations from the model in the previous 
example and use the same robust and classical estimators to retrieve the 
parameters for each of the 100 iterations. 
```

```{r lmbsrob, cache = TRUE, warning = FALSE, message = FALSE}
# Number of bootstrap replications
B = 10^3

# Initialisation
coef.LS.cont = coef.rob.cont = matrix(NA,B,2)
coef.LS.uncont = coef.rob.uncont = matrix(NA,B,2)

# Start Monte-carlo
for (j in 1:2){
  for (i in seq_len(B)) {
   # Control seed
    set.seed(2*j*B + i)
    
    # Uncontaminated case
    if (j == 1){
      y = alpha + beta*x + rnorm(n,0,sqrt(sig2))
      coef.LS.uncont[i,] = as.numeric(lm(y ~ x)$coef)
      coef.rob.uncont[i,] = as.numeric(lmrob(y ~ x)$coef)
    }
    
    # Contaminated case
    if (j == 2){
      y = alpha + beta*x + rnorm(n,0,sqrt(sig2))
      y[20] = alpha + beta*x[20] - 10
      coef.LS.cont[i,] = as.numeric(lm(y ~ x)$coef)
      coef.rob.cont[i,] = as.numeric(lmrob(y ~ x)$coef)
    }
  }
}

# Make graph
colors = gg_color_hue(6, alpha = 0.5)
names = c("LS - uncont","Rob - uncont","LS - cont","Rob - cont")

par(mfrow = c(1,2))
boxplot(coef.LS.uncont[,1],coef.rob.uncont[,1],coef.LS.cont[,1],
        coef.rob.cont[,1], main = expression(alpha), 
        col = colors[c(5,1,5,1)], 
        cex.main = 1.5, xaxt = "n")
axis(1, labels = FALSE)
text(x = seq_along(names), y = par("usr")[3] - 0.15, srt = 45, adj = 1,
     labels = names, xpd = TRUE)

abline(h = alpha, lwd = 2)

boxplot(coef.LS.uncont[,2],coef.rob.uncont[,2],coef.LS.cont[,2],
        coef.rob.cont[,2], main = expression(beta), 
        col = colors[c(5,1,5,1)], 
        cex.main = 1.5, xaxt = "n")
axis(1, labels = FALSE)
text(x = seq_along(names), y = par("usr")[3] - 0.015, srt = 45, adj = 1,
     labels = names, xpd = TRUE)
abline(h = beta, lwd = 2)
```

It can be seen that, as underlined earlier, that the estimations resulting from
the two methods appear to be quite similar, with the robust estimator performing
slightly less efficiently than the classical estimator. However, under the
contaminated setting it is evident how the robust estimation procedure is not 
affected much by the outliers in the simulated data while the classical 
techniques appear to be highly biased. Therefore, if there appear to be outliers
in the data, a robust estimation procedure may considered preferable. In fact, 
the results of the two estimations could be compared to understand if there
appears to be some deviation from the model assumptions.

```{example, dsbook}
A practical example which underlines the usefulness of robust estimation
procedures is given by the dataset that contains the properties of 47 stars
from the Hertzsprung-Russell diagram of the star cluster CYG OB1 in the
direction of Cygnus. From the plot it can be observed that there appears to be
a cluster of four stars on the upper left hand-side of the plot. The rest of
the data however appears to have a reasonably good linear behavior.
```

```{r robstarcluster, cache = TRUE, fig.cap="Comparison of Estimation Methodologies on 47 observations from the Hertzsprung-Russell diagram of the star cluster CYG OB1 in the direction of Cygnus."}
colors = gg_color_hue(6)
data(starsCYG, package = "robustbase")
par(mfrow = c(1,1))
plot(NA, xlim = range(starsCYG$log.Te) + c(-0.1,0.1),
     ylim = range(starsCYG$log.light),
     xlab = "Temperature at the surface of the star (log)",
     ylab = "Light intensity (log)")
grid()  
points(starsCYG, col = colors[4], pch = 16)

LS = lm(starsCYG$log.light ~ starsCYG$log.Te)
rob = lmrob(starsCYG$log.light ~ starsCYG$log.Te)
x = seq(from = min(starsCYG$log.Te)-0.3, 
        to = max(starsCYG$log.Te)+0.3, length.out = 4)
fit.LS = coef(LS)[1] + x*coef(LS)[2]
fit.rob = coef(rob)[1] + x*coef(rob)[2]

lines(x, fit.LS, col = colors[5])
lines(x, fit.rob, col = colors[1])


legend("topright",c("Observations","Estimated model (least-squares)",
                    "Estimated model (robust)"), lwd = c(NA,1,1), 
       col = colors[c(4,5,1)], lty = c(NA,1,1), 
       bty = "n", pch = c(16,NA,NA))

```

From Figure \@ref(fig:robstarcluster) it can be seen that classical linear
regression is considerably affected by the cloud of apparent outliers on the
left hand side of the plot. As a result, the regression line has a negative 
slope when the majority of the data appears to have a clear positive linear
trend. The latter is however the case when using a robust approach which 
indeed detects a positive linear trend. Having said this, there are in fact
two distinct populations in the data consisting in "giants" (upper left corner) 
and "main sequencers" (right handside). Thus, when observing data in general it 
is always necessary to understand if we are dealing with real 
outliers (unexplained outlying data) or simply with data that can be explained
by other factors (e.g. a different population as in this example).

```{r estimatesreg, cache = TRUE}
summary(LS)
summary(rob)
```

<!--chapter:end:91-appendix-b.Rmd-->

