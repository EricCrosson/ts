# The Family of Autoregressive Moving Average Models

```{r, echo = FALSE, message = FALSE, warning = FALSE}
library(simts)
library(gmwm2)
library(ggplot2)
library(robcor)
```

In this chapter we introduce a class of time series models that is considerably flexible and among the most commonly used to describe stationary time series. This class is represented by the Seasonal AutoRegressive Integrated Moving Average (SARIMA) models which, among others, combine and include the autoregressive and moving average models seen in the previous chapter. To introduce this class of models, we start by describing a sub-class called AutoRegressive Moving Average (ARMA) models which represent the backbone on which the SARIMA class is built. The importance of ARMA models resides in their flexibility as well as their capacity of describing (or closely approximating) almost all the features of a stationary time series. The autoregressive parts of these models describe how consecutive observations in time influence each other while the moving average parts capture some possible unobserved shocks thereby allowing to model different phenomena which can be observed in various fields going from biology to finance.

With this premise, the first part of this chapter introduces and explains the class of ARMA models in the following manner. First of all we will discuss the class of linear processes, which ARMA models belong to, and we will then proceed to a detailed description of autoregressive models in which we review their definition, explain their properties, introduce the main estimation methods for their parameters and highlight the diagnostic tools which can help understand if the estimated models appear to be appropriate or sufficient to well describe the observed time series. Once this is done, we will then use most of the results given for the autoregressive models to further describe and discuss moving average models, for which we underline the property of invertibility, and finally the ARMA models. Indeed, the properties and estimation methods for the latter class are directly inherited from the discussions on the autoregressive and moving average models.

The second part of this chapter introduces the general class of SARIMA models, passing through the class of ARIMA models. These models allow to apply the ARMA modeling framework also to time series that have particular non-stationary components to them such as, for example, linear and/or seasonal trends. Extending ARMA modeling to these cases allows SARIMA models to be an extremely flexible class of models that can be used to describe a wide range of phenomena.


## Linear Processes

In order to discuss the classes of models mentioned above, we first present the class of linear processes which underlie many of the most common time series models.

```{definition, label="lp", name="Linear Process"}
A time series, $(X_t)$, is defined to be a linear process if it can be expressed
as a linear combination of white noise as follows:

\[{X_t} = \mu + \sum\limits_{j =  - \infty }^\infty  {{\psi _j}{W_{t - j}}} \]

where $W_t \sim WN(0, \sigma^2)$ and $\sum\limits_{j =  - \infty }^\infty  {\left| {{\psi _j}} \right|}  < \infty$.
```

Note, the latter assumption is required to ensure that the series has a limit. Furthermore, the set of coefficients \[{( {\psi _j}) _{j =  - \infty , \cdots ,\infty }}\] can be viewed as a linear filter. These coefficients do not have to be all equal nor symmetric as later examples will show. Generally, the properties of a linear process related to mean and variance are given by:

\[\begin{aligned}
\mu_{X} &= \mu \\
\gamma_{X}(h) &= \sigma _W^2\sum\limits_{j =  - \infty }^\infty  {{\psi _j}{\psi _{h + j}}}
\end{aligned}\]

The latter is derived from 

\[\begin{aligned}
  \gamma \left( h \right) &= Cov\left( {{x_t},{x_{t + h}}} \right) \\
   &= Cov\left( {\mu  + \sum\limits_{j =  - \infty }^\infty  {{\psi _j}{w_{t - j}}} ,\mu  + \sum\limits_{j =  - \infty }^\infty  {{\psi _j}{w_{t + h - j}}} } \right) \\
   &= Cov\left( {\sum\limits_{j =  - \infty }^\infty  {{\psi _j}{w_{t - j}}} ,\sum\limits_{j =  - \infty }^\infty  {{\psi _j}{w_{t + h - j}}} } \right) \\
   &= \sum\limits_{j =  - \infty }^\infty  {{\psi _j}{\psi _{j + h}}Cov\left( {{w_{t - j}},{w_{t - j}}} \right)}  \\
   &= \sigma _w^2\sum\limits_{j =  - \infty }^\infty  {{\psi _j}{\psi _{j + h}}}  \\ 
\end{aligned} \]

Within the above derivation, the key is to realize that 
$Cov\left( {{w_{t - j}},{w_{t + h - j}}} \right) = 0$ if $t - j \ne t + h - j$.

Lastly, another convenient way to formalize the definition of a linear process is through the use of the **backshift operator** (or lag operator) which is itself defined as follows:

\[B\,X_t = X_{t-1}.\]

The properties of the backshift operator allow us to create composite functions of the type 

$$B^2 \, X_t = B (B \, X_t) = B \, X_{t-1} = X_{t-2}$$
which allows to generalize as follows

$$B^k \, X_t = X_{t-k}.$$
Moreover, we can apply the inverse operator to it (i.e. $B^{-1} \, B = 1$) thereby allowing us to have, for example:

$$X_t = B^{-1} \, B X_t = B^{-1} X_{t-1}$$

```{example, label="backdiff", name="d-order Differences"}
We can re-express $X_t - X_{t-1}$ as
$$\delta X_t = (1 - B) X_t$$
or a second order difference as
$$\delta^2 X_t = (1 - B)^2 X_t$$
thereby generalizing to a d-order difference as follows:
$$\delta^d X_t = (1 - B)^d X_t.$$
```

Having defined the backshift operator, we can now provide an alternative definition of a linear process as follows:

\[{X_t} = \mu + \psi \left( B \right){W_t}\]

where $\psi ( B )$ is a polynomial function in $B$ whose coefficients are given by the linear filters $(\psi_j)$ (we'll describe these polynomials further on).

```{example, label="lpwn", name="Linear Process of White Noise"}

The white noise process $(X_t)$, defined in \@ref(wn),
can be expressed as a linear process as follows:

\[\psi _j = \begin{cases}
      1 , &\mbox{ if } j = 0\\
      0 , &\mbox{ if } |j| \ge 1
\end{cases}.\]

and $\mu = 0$.

Therefore, $X_t = W_t$, where $W_t \sim WN(0, \sigma^2_W)$
```

```{example, label="lpma1", name="Linear Process of Moving Average Order 1"}

Similarly, consider $(X_t)$ to be a MA(1) process, 
given by \@ref(ma1). The process can be expressed linearly through the following filters: 

\[\psi _j = \begin{cases}
      1, &\mbox{ if } j = 0\\
      \theta , &\mbox{ if } j = 1 \\
      0, &\mbox{ if } j \ge 2
\end{cases}.\]

and $\mu = 0$.

Thus, we have: $X_t = W_t + \theta W_{t-1}$
```

```{example, label="lpsma", name="Linear Process and Symmetric Moving Average"}

Consider a symmetric moving average given by:

\[{X_t} = \frac{1}{{2q + 1}}\sum\limits_{j =  - q}^q {{W_{t + j}}} \]

Thus, $(X_t)$ is defined for $q + 1 \le t \le n-q$. The above process
would be a linear process since:

\[\psi _j = \begin{cases}
      \frac{1}{{2q + 1}} , &\mbox{ if } -q \le j \le q\\
      0 , &\mbox{ if } |j| > q
\end{cases}.\]

and $\mu = 0$.

In practice, if $q = 1$, we would have:

\[{X_t} = \frac{1}{3}\left( {{W_{t - 1}} + {W_t} + {W_{t + 1}}} \right)\]
```


```{example, label="lpar1", name="Autoregressive Process of Order 1"}
If $\left\{X_t\right\}$ follows an AR(1) model defined in \@ref(ar1), the linear filters are a function of the time lag:

\[\psi _j = \begin{cases}
      \phi^j , &\mbox{ if } j \ge 0\\
      0 , &\mbox{ if } j < 0
\end{cases}.\]

and $\mu = 0$. We would require the condition that $\left| \phi \right| < 1$ in order to respect the condition on the filters (i.e. $\sum\limits_{j =  - \infty }^\infty  {\left| {{\psi _j}} \right|}  < \infty$).
```


## Autoregressive Models - AR(p)

The class of autoregressive models is based on the idea that previous values in the time series are needed to explain current values in the series. For this class of models, we assume that the $p$ previous observations are needed for this purpose and we therefore denote this class as AR($p$). In the previous chapter, the model we introduced was an AR(1) in which only the immediately previous observation is needed to explain the following one and therefore represents a particular model which is part of the more general class of AR(p) models.

```{definition, label="arp", name="Autoregressive Models of Order p"}
The AR(p) models can be formally represented as follows
$$(X_t) = {\phi_1}{X_{t - 1}} + ... + {\phi_p}{X_{t - p}} + {W_t},$$
where $\phi_i \neq 0$ (for $i = 1, ..., p$) and $W_t$ is a (Gaussian) white noise process with variance $\sigma^2$. 
```

As earlier in this book, we will assume that the expectation of the process $({X_t})$, as well as that of the following ones in this chapter, is zero. The reason for this simplification is that if $\mathbb{E} [ X_t ] = \mu$, we can define an AR process *around* $\mu$ as follows:

$$X_t - \mu = \sum_{i = 1}^p \phi_i \left(X_{t-i} - \mu \right) + W_t,$$

which is equivalent to 

$$X_t  = \mu^{\star} +  \sum_{i = 1}^p \phi_i X_{t-i}  + W_t,$$

where $\mu^{\star} = \mu (1 - \sum_{i = 1}^p \phi_i)$. Therefore, to simplify the notation we will generally consider only zero mean processes, since adding means (as well as other deterministic trends) is easy.

A useful way of representing AR(p) processes is through the backshift operator introduced in the previous section and is as follows

\[\begin{aligned}
  {X_t} &= {\phi_1}{X_{t - 1}} + ... + {\phi_p}{X_{t - p}} + {W_t} \\
   &= {\phi_1}B{X_t} + ... + {\phi_p}B^p{X_t} + {W_t} \\
   &= ({\phi_1}B + ... + {\phi_p}B^p){X_t} + {W_t} \\ 
\end{aligned},\]

which finally yields

$$(1 - {\phi _1}B - ... - {\phi_p}B^p){X_t} = {W_t},$$

which, in abbreviated form, can be expressed as

$$\phi(B){X_t} = W_t.$$

We will see that $\phi(B)$ is important to establish the stationarity of these processes and is called the *autoregressive* operator. Moreover, this quantity is closely related to another important property of AR(p) processes called *causality*. Before formally defining this new property we consider the following example which provides an intuitive illustration of its importance.

**Example:** Consider a classical AR(1) model with $|\phi| > 1$. Such a model could be expressed as

$$X_t = \phi^{-1} X_{t+1} - \phi^{-1} W_t = \phi^{-k} X_{t+k} - \sum_{i = 1}^{k-1} \phi^{-i} W_{t+i}.$$

Since $|\phi| > 1$, we obtain

$$X_t = - \sum_{j = 1}^{\infty} \phi^{-j} W_{t+j},$$

which is a linear process and therefore is stationary. Unfortunately, such a model is useless because we need the future to predict the future. These processes are called non-causal.

### Properties of AR(p) models

In this section we will describe the main property of the AR(p) model which has already been mentioned in the previous paragraphs and therefore let us now introduce the property of causality in a more formal manner.

**Definition:** An AR(p) model is *causal* if the time series $(X_t)_{-\infty}^{\infty}$ can be written as a one-sided linear process:
\begin{equation}
    X_t = \sum_{j = 0}^{\infty} \psi_j W_{t-j} = \frac{1}{\phi(B)} W_t = \psi(B) W_t,
(\#eq:causal)
\end{equation}
where $\phi(B) = \sum_{j = 0}^{\infty} \phi_j B^j$, and $\sum_{j=0}^{\infty}|\phi_j| < \infty$ and setting $\phi_0 = 1$.

As discussed earlier this condition implies that only the past values of the time series can explain the future values of it and not viceversa. Moreover, given the expression of the linear filters given by
$$\frac{1}{\phi(B)}$$
it is obvious that a solution exists only when $\phi(B) = \sum_{j = 0}^{\infty} \phi_j B^j \neq 0$ (thereby implying causality). A condition for this to be respected is for the roots of $\phi(B) = 0$ to lie outside the unit circle.

<!-- However, it might be difficult and not obvious to show the causality of an AR(p) process by using the above definitions directly, thus the following properties are useful in practice.  -->

<!-- **Causality** -->
<!-- If an AR(p) model is causal, then the coefficients of the one-sided linear process given in \@ref(eq:causal) can be obtained by solving -->
<!-- \begin{equation*} -->
<!--     \psi(z) = \frac{1}{\sum_{j=0}^{\infty} \phi_j z^j} = \frac{1}{\phi(z)}, \mbox{ } |z| \leq 1. -->
<!-- \end{equation*} -->

<!-- It can be seen how there is no solution to the above equation if $\phi(z) = 0$ and therefore an AR(p) is causal if and only if $\phi(z) \neq 0$. A condition for this to be respected is for the roots of $\phi(z) = 0$ to lie outside the unit circle. -->

```{example, label="AR2asLP", name="Transform an AR(2) into a Linear Process"}
Consider an AR(2) process $$X_t = 1.3 X_{t-1} - 0.4 X_{t-2} + W_t,$$ which we would like to transform into a linear process. This can be done using the following approach:

- Step 1:
The autoregressive operator of this model can be expressed as 
$$
\phi(B) = 1-1.3B+0.4B^2 = (1-0.5B)(1-0.8B),
$$
and has roots 2 and 1.25, both $>1$. Thus, we should be able to convert it into a linear process.

- Step 2: We know that if an AR(p) process has all its roots outside the unit circle, then we can write $X_t = \frac{1}{\phi(B)} W_t$. By applying the partial fractions trick, we can inverse the autoregressive operator $\phi(B)$ as follows:
\[ \begin{aligned}
\phi^{-1}(B) &= \frac{1}{(1-0.5B)(1-0.8B)} = \frac{c_1}{(1-0.5B)} + \frac{c_2}{(1-0.8B)} \\
&= \frac{c_2(1-0.5B) + c_1(1-0.8B)}{(1-0.5B)(1-0.8B)} = \frac{(c_1 + c_2)-(0.8c_1+0.5c_2)B}{(1-0.5B)(1-0.8B)}.
\end{aligned} \]

To solve for $c_1$ and $c_2$:
\[ \begin{cases}
      c_1 + c_2 &=1\\
      0.8c_1+0.5c_2 &=0
\end{cases} \to 
\begin{cases}
      c_1 &= -5/3\\
      c_2 &= 8/3.
\end{cases} \]

So we obtain 
$$
\phi^{-1}(B) = \frac{-5}{3(1-0.5B)} + \frac{8}{3(1-0.8B)}.
$$

- Step 3: Using the Geometric series, i.e. $a\sum_{j=0}^{\infty} r^j = \frac{a}{1-r}$ if $|r| <1$, we have
\[ \begin{cases}
      \frac{-5}{3(1-0.5B)} = -\frac{5}{3} \sum_{j=0}^\infty 0.5^j B^j, &\mbox{ if } |B| < 2 \\
      \frac{8}{3(1-0.8B)} = \frac{8}{3} \sum_{j=0}^\infty 0.8^j B^j, &\mbox{ if } |B| < 1.25.
\end{cases} \]

So we can express $\phi^{-1}(B)$ as 
$$
\phi^{-1}(B) = \sum_{j=0}^\infty \Big[ -\frac{5}{3} (0.5)^j  + \frac{8}{3} (0.8)^j \Big] B^j, \;\;\; \text{if  } |B|<1.25.
$$

- Step 4: Finally, we obtain
\[ \begin{aligned}
X_t &= \phi(B)^{-1} W_t = \sum_{j=0}^\infty \Big[ -\frac{5}{3} (0.5)^j  + \frac{8}{3} (0.8)^j \Big] B^j W_t \\
&= \sum_{j=0}^\infty \Big[ -\frac{5}{3} (0.5)^j  + \frac{8}{3} (0.8)^j \Big] W_{t-j},
\end{aligned} \]
which verifies that the AR(2) is causal, and therefore is stationary.
```


```{example, label="AR2causalcond", name="Causal Conditions for an AR(2) Process"}
We already know that an AR(1) is causal with the simple condition $|\phi_1|<1$. It seems natural to believe that an AR(2) should be causal (and therefore stationary) with the condition that $|\phi_i| <1, \; i=1,2$. However, this is actually not the case as we illustrate below.

We can express an AR(2) process as 
$$
X_t = \phi_1 X_{t-1} + \phi_2 X_{t-2} + W_t = \phi_1 BX_t + \phi_2 B^2 X_t + W_t,
$$
thereby delivering the following autoregressive operator:
$$
\phi(B) = 1-\phi_1 B - \phi_2 B^2 = \Big( 1-\frac{B}{\lambda_1} \Big) \Big( 1-\frac{B}{\lambda_2} \Big)
$$
where $\lambda_1$ and $\lambda_2$ are the roots of $\phi(B)$ such that 
\[ \begin{aligned}
\phi_1 &=  \frac{1}{\lambda_1} + \frac{1}{\lambda_2}, \\
\phi_2 &= - \frac{1}{\lambda_1} \frac{1}{\lambda_2}.
\end{aligned} \]

That is, 
\[\begin{aligned}
\lambda_1 &= \frac{\phi_1 + \sqrt{\phi_1^2 + 4\phi_2}}{-2\phi_2}, \\
\lambda_2 &= \frac{\phi_1 - \sqrt{\phi_1^2 + 4\phi_2}}{-2\phi_2}.
\end{aligned} \]

In order to ensure the causality of the model, we need the roots of $\phi(B)$, i.e. $\lambda_1$ and $\lambda_2$, to lie outside the unit circle.

\[ \begin{cases}
|\lambda_1| &> 1 \\
|\lambda_2| &> 1,
\end{cases} \]
if and only if 
\[ \begin{cases}
\phi_1 + \phi_2 &< 1 \\
\phi_2 - \phi_1 &< 1 \\
|\phi_2| &<1.
\end{cases} \]

We can show the *if* part of the statement as follows:
\[ \begin{aligned}
& \phi_1 + \phi_2 = \frac{1}{\lambda_1} + \frac{1}{\lambda_2} - \frac{1}{\lambda_1 \lambda_2} = \frac{1}{\lambda_1} \Big(1-\frac{1}{\lambda_2} \Big) + \frac{1}{\lambda_2} < 1 - \frac{1}{\lambda_2} + \frac{1}{\lambda_2} = 1 \;\; \text{since } 1-\frac{1}{\lambda_2} > 0, \\
& \phi_2 - \phi_1 = -\frac{1}{\lambda_1 \lambda_2} - \frac{1}{\lambda_1} - \frac{1}{\lambda_2} = -\frac{1}{\lambda_1} \Big( \frac{1}{\lambda_2} +1 \Big) - \frac{1}{\lambda_2} < \frac{1}{\lambda_2}+1-\frac{1}{\lambda_2} = 1 \;\; \text{since } \frac{1}{\lambda_2}+1 > 0, \\
& |\phi_2| = \frac{1}{|\lambda_1| |\lambda_2|} < 1.
\end{aligned} \]

We can also show the *only if* part of the statement as follows:

Since $\lambda_1 = \frac{\phi_1 + \sqrt{\phi_1^2 + 4\phi_2}}{-2\phi_2}$ and $\phi_2 - 1 < \phi_1 < 1- \phi_2$, we have 
$$
\lambda_1^2 = \frac{(\phi_1 + \sqrt{\phi_1^2 + 4\phi_2})^2}{4\phi_2^2} < \frac{\Big( (1-\phi_2)+ \sqrt{(1-\phi_2)^2 + 4\phi_2} \Big)^2}{4\phi_2^2} = \frac{4}{4\phi_2^2} \leq 1. 
$$

Since $\lambda_2 = \frac{\phi_1 - \sqrt{\phi_1^2 + 4\phi_2}}{-2\phi_2}$ and $\phi_2 - 1 < \phi_1 < 1- \phi_2$, we have 
$$
\lambda_2^2 = \frac{(\phi_1 - \sqrt{\phi_1^2 + 4\phi_2})^2}{4\phi_2^2} < \frac{\Big( (\phi_2-1)+ \sqrt{(\phi_2-1)^2 + 4\phi_2} \Big)^2}{4\phi_2^2} = \frac{4\phi_2^2}{4\phi_2^2} = 1. 
$$

Finally, the causal region of an AR(2) is demonstrated as
```

```{r correxample2, cache = TRUE, echo = FALSE, fig.cap="Causal Region for Parameters of an AR(2) Process", fig.align = 'center'}
knitr::include_graphics("images/causal_AR2.png")
```

### Estimation of AR(p) models

Given the above defined properties of AR(p) models, we will now discuss how these models can be estimated, more specifically how the $p+1$ parameters can be obtained from an observed time series. Indeed, a reliable estimation of these models is necessary in order to intepret and describe different natural phenomena and/or forecast possible future values of the time series.

A first approach builds upon the earlier definition of AR(p) models being a linear process. Recall that
\begin{equation}
    X_t = \sum_{j = 1}^{p} \phi_j X_{t-j}
\end{equation}
which delivers the following autocovariance function
\begin{equation}
    \gamma(h) = \text{cov}(X_{t+h}, X_t) = \text{cov}\left(\sum_{j = 1}^{p} \phi_j X_{t+h-j}, X_t\right) = \sum_{j = 1}^{p} \phi_j \gamma(h-j), \mbox{ } h \geq 1.
\end{equation}
Rearranging the above expressions we obtain the following general equations
\begin{equation}
    \gamma(h) - \sum_{j = 1}^{p} \phi_j \gamma(h-j) = 0, \mbox{ } h \geq 1
\end{equation}
and, recalling that $\gamma(h) = \gamma(-h)$,
\begin{equation}
    \gamma(0) - \sum_{j = 1}^{p} \phi_j \gamma(j) = \sigma_w^2.
\end{equation}
We can now define the Yule-Walker equations.

**Definition:** The Yule-Walker equations are given by
\begin{equation}
    \gamma(h) = \phi_1 \gamma(h-1) + ... + \phi_p \gamma(h-p), \mbox{ } h = 1,...,p
\end{equation}
and
\begin{equation}
    \sigma_w^2 = \gamma(0) - \phi_1 \gamma(1) - ... - \phi_p \gamma(p).
\end{equation}
which in matrix notation can be defined as follows
\begin{equation}
    \Gamma_p \mathbf{\phi} = \mathbf{\gamma}_p \,\, \text{and} \,\, \sigma_w^2 = \gamma(0) - \mathbf{\phi}'\mathbf{\gamma}_p
\end{equation}
where $\Gamma_p$ is the $p\times p$ matrix containing the autocovariances $\gamma(k-j)$, where $j,k = 1, ...,p$, while $\mathbf{\phi} = (\phi_1,...,\phi_p)'$ and $\mathbf{\gamma}_p = (\gamma(1),...,\gamma(p))'$ are $p\times 1$ vectors.

Considering the Yule-Walker equations, it is possible to use a method of moments approach and simply replace the theoretical quantities given in the previous definition with their empirical (estimated) counterparts that we saw in the previous chapter. This gives us the following Yule-Walker estimators
\begin{equation}
    \hat{\mathbf{\phi}} = \hat{\Gamma}_p^{-1}\hat{\mathbf{\gamma}}_p \,\, \text{and} \,\, \hat{\sigma}_w^2 = \hat{\gamma}(0) - \hat{\mathbf{\gamma}}_p'\hat{\Gamma}_p^{-1}\hat{\mathbf{\gamma}}_p .
\end{equation}

These estimators have the following asymptotic properties.

**Consistency and Asymptotic Normality of Yule-Walker estimators:**
The Yule-Walker estimators for a causal AR(p) model have the following asymptotic properties:

\begin{equation*}
\sqrt{T}(\hat{\mathbf{\phi}}- \mathbf{\phi}) \xrightarrow{\mathcal{D}} \mathcal{N}(\mathbf{0},\sigma_w^2\Gamma_p^{-1}) \,\, \text{and} \,\, \hat{\sigma}_w^2 \xrightarrow{\mathcal{P}} \sigma_w^2 .
\end{equation*}

Therefore the Yule-Walker estimators have an asymptotically normal distribution and the estimator of the innovation variance is consistent. Moreover, these estimators are also optimal for AR(p) models, meaning that they are also efficient. However, there is also another method which allows to achieve this efficiency (also for general ARMA models that will be tackled further on) and this is the Maximum Likelihood Estimation (MLE) method. Considering an AR(1) model as an example, and assuming without loss of generality that its expectation is zero, we have the following representation of the AR(1) model
\begin{equation*}
X_t = \phi X_{t-1} + W_t
\end{equation*}
where $|\phi|<1$ and $W_t \overset{iid}{\sim} \mathcal{N}(0,\sigma_w^2)$. Supposing we have observations $(x_t)_{t=1,...,T}$ issued from this model, then the likelihood function for this setting is given by
\begin{equation*}
L(\phi,\sigma_w^2) = f(\phi,\sigma_w^2|x_1,...,x_T)
\end{equation*}
which, for an AR(1) model, can be rewritten as follows
\begin{equation*}
L(\phi,\sigma_w^2) = f(x_1)f(x_2|x_1)\cdot \cdot \cdot f(x_T|x_{T-1}).
\end{equation*}
If we define $\Omega_t^p$ as the information contained in the previous $p$ observations (before time $t$), the above expression can be generalized for an AR(p) model as follows
\begin{equation*}
L(\phi,\sigma_w^2) = f(x_1,...,x_p)f(x_{p+1}|\Omega_{p+1}^p)\cdot \cdot \cdot f(x_T|\Omega_{T}^p)
\end{equation*}
where $f(x_1,...,x_p)$ is the joint probability distribution of the first $p$ observations. Going back to the AR(1) setting, based on our assumption on $(W_t)$ we know that $x_t|x_{t-1} \sim \mathcal{N}(\phi x_{t-1},\sigma_w^2)$ and therefore we have that
\begin{equation*}
f(x_t|x_{t-1}) = f_w(x_t - \phi x_{t-1})
\end{equation*}
where $f_w(\cdot)$ is the distribution of $W_t$. This rearranges the likelihood function as follows
\begin{equation*}
L(\phi,\sigma_w^2) = f(x_1)\prod_{t=2}^T f_w(x_t - \phi x_{t-1})
\end{equation*}
where $f(x_1)$ can be found through the causal representation
\begin{equation*}
x_1 = \sum_{j=0}^{\infty} \phi^j w_{1-j} 
\end{equation*}
which implies that $x_1$ follows a normal distribution with zero expectation and a variance given by $\frac{\sigma_w^2}{(1-\phi^2)}$. Based on this, the likelihood function of an AR(1) finally becomes
\begin{equation*}
L(\phi,\sigma_w^2) = (2\pi \sigma_w^2)^{-\frac{T}{2}} (1 - \phi^2)^{\frac{1}{2}} \exp \left(-\frac{S(\phi)}{2 \sigma_w^2}\right)
\end{equation*}
with $S(\phi) = (1-\phi^2) x_1^2 + \sum_{t=2}^T (x_t -\phi x_{t-1})^2$. Once the derivative of the logarithm of the likelihood is taken, the minimization of the negative of this function is usually done numerically. However, if we condition on the initial values, the AR(p) models are linear and, for example, we can then define the conditional likelihood of an AR(1) as
\begin{equation*}
L(\phi,\sigma_w^2|x_1) = (2\pi \sigma_w^2)^{-\frac{T-1}{2}} \exp \left(-\frac{S_c(\phi)}{2 \sigma_w^2}\right)
\end{equation*}
where
\begin{equation*}
S_c(\phi) = \sum_{t=2}^T (x_t -\phi x_{t-1})^2 .
\end{equation*}
The latter is called the conditional sum of squares and $\phi$ can be estimated as a straightforward linear regression problem. Once an estimate $\hat{\phi}$ is obtained, this can be used to obtain the conditional maximum likelihood estimate of $\sigma_w^2$
\begin{equation*}
\hat{\sigma}_w^2 = \frac{S_c(\hat{\phi})}{(T-1)} .
\end{equation*}

The estimation methods presented so far are standard for these kind of models. Nevertheless, if the data suffers from some form of contamination, these methods can become highly biased. For this reason, some robust estimators are available to limit this problematic if there are indeed outliers in the observed time series. One of these methods relies on the estimator proposed in Kunsch (1984) who underlines that the MLE score function of an AR(p) is given by
\begin{equation*}
 \kappa(\mathbf{\theta}|x_j,...x_{j+p}) = \frac{\partial}{\partial \mathbf{\theta}} (x_{j+p} - \sum_{k=1}^p \phi_k x_{j+p-k})^2
\end{equation*}
where $\theta$ is the parameter vector containing, in the case of an AR(1)
model, the two parameters $\phi$ and $\sigma_w^2$ 
(i.e. $\theta = [\phi \,\, \sigma_w^2]$). This delivers the estimating equation
\begin{equation*}
\sum_{j=1}^{n-p} \kappa (\hat{\mathbf{\theta}}|x_j,...x_{j+p}) = 0 .
\end{equation*}
The score function $\kappa(\cdot)$ is clearly not bounded, in the sense that if
we arbitrarily move a value of $(x_t)$ to infinity then the score function also 
goes to infinity thereby delivering a biased estimation procedure. To avoid that
outlying observations bias the estimation excessively, a bounded score function
can be used to deliver an M-estimator given by
\begin{equation*}
\sum_{j=1}^{n-p} \psi (\hat{\mathbf{\theta}}|x_j,...x_{j+p}) = 0,
\end{equation*}
where $\psi(\cdot)$ is a function of bounded variation. When conditioning on the
first $p$ observations, this problem can be brought back to a linear regression
problem which can be applied in a robust manner using the robust regression
tools available in `R` such as `rlm` or `lmrob`. However, another 
available tool in `R` which does not require a strict specification of the distribution function (also for general ARMA models) is the `gmwm` function (in the `gmwm` package) in which it is possible to specify the option `robust = TRUE`. This function makes use of a quantity called the wavelet variance (denoted as $\boldsymbol{\nu}$) which is estimated robustly and then used to retrieve the parameters $\theta$ of the time series model. The robust estimate is obtained by solving the following minimization problem
\begin{equation*}
\hat{\boldsymbol{\theta}} = \underset{\boldsymbol{\theta} \in \boldsymbol{\Theta}}{\text{argmin}} (\hat{\boldsymbol{\nu}} - \boldsymbol{\nu}(\boldsymbol{\theta}))^T\boldsymbol{\Omega}(\hat{\boldsymbol{\nu}} - \boldsymbol{\nu}({\boldsymbol{\theta}})),
\end{equation*}
where $\hat{\boldsymbol{\nu}}$ is the robustly estimated wavelet variance, $\boldsymbol{\nu}({\boldsymbol{\theta}})$ is 
the theoretical wavelet variance (implied by the model we want to estimate) and $\boldsymbol{\Omega}$ is a positive definite weighting matrix. Below we show some simulation studies where we present the results of the above estimation procedures in absence and in presence of contamination in the data. As a reminder, so far we have mainly discussed three estimators for the parameters of AR($p$) models (i.e.  Yule-Walker, maximum likelihod, and RGMWM estimators). 

In `R` the first three estimators can be computed as follows:

```{r, eval=FALSE}
mod = ar(Xt, order.max = p, method = select_method, demean = TRUE, aic = FALSE)
```

In the above sample code `Xt` denotes the time series (a vector of length $T$), `p` is the order of the AR($p$) and `demean = TRUE` indicates that the mean of the process should be estimated (if this is not the case, then use `demean = FALSE`). The `select_method` input can be (among others) `"mle"` for the maximum likelihood and `"yule-walker"` for the Yule-Walker estimator. For example, if you would like to estimate a zero mean AR(3) with the MLE you can use the code:

```{r, eval = FALSE}
mod = ar(Xt, order.max = 3, method = "mle", demean = FALSE, aic = FALSE)
```

On the other hand, the RGMWM is implemented in the `gmwm` R package which can be downloaded and installed as follows:

```{r, eval = FALSE}
devtools::install_github("smac-group/gmwm")
```

Once this package is installed, you can estimate robust AR models using the following syntax:

```{r, eval = FALSE}
mod = gmwm(AR(p), Xt, robust = TRUE)
```

This syntax assumes that `Xt` is a zero mean process so if this is not case simply remove the empirical mean (or median) before using the `gmwm` function. For example, to estimate a robust AR(3) you can use the code:

```{r, eval=FALSE}
mod = gmwm(AR(3), Xt, robust = TRUE)
```

We now have the necessary `R` functions to deliver the above mentioned estimators and we can now proceed to the simulation study. In particular, we simulate three different processes $X_t, Y_t, Z_t$ by using the first as an uncontaminated process defined as \[X_t = 0.5 X_{t-1} - 0.25 X_{t-2} + W_t,\] with $W_t \overset{iid}{\sim} N(0, 1)$. This first process $(X_t)$ is uncontaminated while the other two processes are contaminated versions of the first that can often be observed in practice. The first type of contamination can be seen in $(Y_t)$ and is delivered by replacing a portion of the original process with a process defined as \[U_t = 0.90 U_{t-1} - 0.40 U_{t-2} + V_t,\] where $V_t \overset{iid}{\sim} N(0, 9)$. The second form of contamination can be seen in $(Z_t)$ and consists in the so-called point-wise contamination where randomly selected points from $X_t$ are replaced with $N_t \overset{iid}{\sim} N(0, 9)$.

The code below performs the simulation study where it can be seen how the contaminated processes $(Y_t)$ and $(Z_t)$ are generated. Once this is done, for each simultation the code estimates the parameters of the AR(2) model using the three different estimation methods.

```{r simuAR2study, eval = TRUE, cache = TRUE}
# Load gmwm
library(gmwm)

# Number of bootstrap iterations
B = 250

# Sample size
n = 500

 # Proportion of contamination
eps = 0.05       

# Number of contaminated observations
cont = round(eps*n)   

# Simulation storage
res.Xt.MLE = res.Xt.YW = res.Xt.RGMWM = matrix(NA, B, 3)
res.Yt.MLE = res.Yt.YW = res.Yt.RGMWM = matrix(NA, B, 3)
res.Zt.MLE = res.Zt.YW = res.Zt.RGMWM = matrix(NA, B, 3)
  
# Begin bootstrap
for (i in seq_len(B)){
  # Set seed for reproducibility
  set.seed(1982 + i)
  
  # Generate processes
  Xt = gen_gts(n, AR(phi = c(0.5, 0.25), sigma2 = 1))
  Yt = Zt = Xt
  
  # Generate Ut contamination process that replaces a portion of original signal
  index_start = sample(1:(n-cont-1), 1)
  index_end = index_start + cont - 1
  Yt[index_start:index_end] = gen_gts(cont, AR(phi = c(0.9,-0.4), sigma2 = 9))
  
  # Generate Nt contamination that inject noise at random
  Zt[sample(n, cont, replace = FALSE)] = gen_gts(cont, WN(sigma2 = 9))
  
  # Fit Yule-Walker estimators on the three time series
  mod.Xt.YW = ar(Xt, order.max = 2, method = "yule-walker", 
                 demean = FALSE, aic = FALSE)
  mod.Yt.YW = ar(Yt, order.max = 2, method = "yule-walker", 
                 demean = FALSE, aic = FALSE)
  mod.Zt.YW = ar(Zt, order.max = 2, method = "yule-walker", 
                 demean = FALSE, aic = FALSE)
  
  # Store results
  res.Xt.YW[i, ] = c(mod.Xt.YW$ar, mod.Xt.YW$var.pred)
  res.Yt.YW[i, ] = c(mod.Yt.YW$ar, mod.Yt.YW$var.pred)
  res.Zt.YW[i, ] = c(mod.Zt.YW$ar, mod.Zt.YW$var.pred)
  

  # Fit MLE on the three time series
  mod.Xt.MLE = ar(as.vector(Xt), order.max = 2, method = "mle", 
                  demean = FALSE, aic = FALSE)
  mod.Yt.MLE = ar(as.vector(Yt), order.max = 2, method = "mle", 
                  demean = FALSE, aic = FALSE)
  mod.Zt.MLE = ar(as.vector(Zt), order.max = 2, method = "mle", 
                  demean = FALSE, aic = FALSE)
  
  # Store results
  res.Xt.MLE[i, ] = c(mod.Xt.MLE$ar, mod.Xt.MLE$var.pred)
  res.Yt.MLE[i, ] = c(mod.Yt.MLE$ar, mod.Yt.MLE$var.pred)
  res.Zt.MLE[i, ] = c(mod.Zt.MLE$ar, mod.Zt.MLE$var.pred)
  
  # Fit RGMWM on the three time series
  res.Xt.RGMWM[i, ] = gmwm(AR(2), Xt, robust = TRUE)$estimate
  res.Yt.RGMWM[i, ] = gmwm(AR(2), Yt, robust = TRUE)$estimate
  res.Zt.RGMWM[i, ] = gmwm(AR(2), Zt, robust = TRUE)$estimate
}
```

Having performed the estimation, we should now have `r B` estimates for each AR(2) parameter and each estimation method. The code below takes the results of the simulation and shows them in the shape of boxplots along with the true values of the parameters. The estimation methods that are denoted as follows: -->

- **YW**: Yule-Walker estimator
- **MLE**: Maximum Likelihood Estimator
- **RGMWM**: the robust version of the GMWM estimator


```{r, echo=FALSE, fig.height = 6, fig.width = 8, cache = TRUE, fig.align='center'}
# Define colors
hues = seq(15, 375, length = 4)
cols = hcl(h = hues, l = 65, c = 100, alpha = 0.8)[1:3]

# True values
theta0 = c(0.5, 0.25, 1)

# Define windows params
par(mfrow = c(3, 3), oma = c(5,4,0,0) + 1.5, mar = c(0,0,1,1) + 1.5)

# Make boxplots - Xt
main = c(expression(paste("Simulated process ", X[t], " - ", phi[1])),
         expression(paste("Simulated process ", X[t], " - ", phi[2])),
         expression(paste("Simulated process ", X[t], " - ", sigma^2)))

for (i in 1:3){
  boxplot(res.Xt.YW[,i], res.Xt.MLE[,i], res.Xt.RGMWM[,i], names = c("YW", "MLE", "RGMWM"), col = cols, main = main[i])
  abline(h = theta0[i], col = 2, lwd = 2)
}

# Make boxplots - Yt
main = c(expression(paste("Simulated process ", Y[t], " - ", phi[1])),
         expression(paste("Simulated process ", Y[t], " - ", phi[2])),
         expression(paste("Simulated process ", Y[t], " - ", sigma^2)))

for (i in 1:3){
  boxplot(res.Yt.YW[,i], res.Yt.MLE[,i], res.Yt.RGMWM[,i], names = c("YW", "MLE", "RGMWM"), col = cols, main = main[i])
  abline(h = theta0[i], col = 2, lwd = 2)
}

# Make boxplots - Yt
main = c(expression(paste("Simulated process ", Z[t], " - ", phi[1])),
         expression(paste("Simulated process ", Z[t], " - ", phi[2])),
         expression(paste("Simulated process ", Z[t], " - ", sigma^2)))

for (i in 1:3){
  boxplot(res.Zt.YW[,i], res.Zt.MLE[,i], res.Zt.RGMWM[,i], names = c("YW", "MLE", "RGMWM"), col = cols, main = main[i])
  abline(h = theta0[i], col = 2, lwd = 2)
}
```

It can be seen how all methods appear to properly estimate the true parameter values on average when they are applied to the simulated time series from the uncontaminated process $(X_t)$. However, the MLE appears to be slightly more efficient (less variable) compared to the other methods and, in addition, the robust method (RGMWM) appears to be less efficient than the other two estimators. The latter is a known result since robust estimators usually pay a price in terms of efficiency (as an insurance against bias).

On the other hand, when checking the performance of the same methods when applied to the two contaminated processes $(Y_t)$ and $(Z_t)$ it can be seen that the standard estimators appear to be (highly) biased for most of the estimated parameters (with one exception) while the robust estimator remains close (on average) to the true parameter values that we are aiming to estimate. Therefore, when there's a suspicion that there could be some (small) contamination in the observed time series, it may be more appropriate to use a robust estimator.

<!-- In particular, we simulate three different processes $X_t, Y_t, Z_t$ from \[X_t = 0.5 X_{t-1} - 0.25 X_{t-2} + W_t,\] with $W_t \overset{iid}{\sim} N(0, 1)$. The first process $(X_t)$ is uncontaminated while the other two processes are contaminated versions of the first that can often be observed in practice. The first type of contamination can be seen in $(Y_t)$ and is delivered by replacing a portion of the original process with a process defined as \[U_t = 0.90 U_{t-1} - 0.40 U_{t-2} + V_t,\] where $V_t \overset{iid}{\sim} N(0, 9)$. The second form of contamination can be seen in $(Z_t)$ and consists in the so-called point-wise contamination where randomly selected points from $X_t$ are replaced with $N_t \overset{iid}{\sim} N(0, 9)$. An example of how to simulate a time series from each of these processes is in the code below followed by the relative plots. -->

<!-- ``` {r simuAR2data, cache = TRUE} -->
<!-- n         = 1000           # Sample Size (T) -->
<!-- eps       = 0.05           # Proportion of contamination -->
<!-- cont      = round(eps*n)   # Number of contaminated observations -->

<!-- set.seed(19)               # Set seed for reproducibility -->

<!-- # Generate time series -->
<!-- Xt = gen_gts(n, AR(phi = c(0.5,0.25), sigma2 = 1)) -->
<!-- Yt = gen_gts(n, AR(phi = c(0.5,0.25), sigma2 = 1)) -->
<!-- Zt = gen_gts(n, AR(phi = c(0.5,0.25), sigma2 = 1)) -->

<!-- # Contaminate a portion of Yt with a process -->
<!-- index_start = sample(1:(n-cont-1), 1) -->
<!-- index_end = index_start + cont - 1 -->
<!-- Yt[index_start:index_end,] = gen_gts(cont, AR(phi = c(0.9,-0.4), sigma2 = 9)) -->

<!-- # Contaminate at random Zt with noise -->
<!-- Zt[sample(1:n, cont, replace = FALSE),] = gen_gts(cont, WN(sigma2 = 9)) -->
<!-- ``` -->

<!-- ```{r visualizesimuAR2data, cache = TRUE, echo = FALSE, fig.height = 9, fig.width = 6, fig.cap = "Contaminated AR(2) Processes", fig.align='center'} -->
<!-- # Graph points -->
<!-- par(mfrow = c(3,1)) -->
<!-- plot(Xt, main = expression(paste("Simulated process ",X[t])), ylim = range(c(Xt, Yt, Zt))) -->
<!-- plot(Yt, main = expression(paste("Simulated process ",Y[t])), ylim = range(c(Xt, Yt, Zt))) -->
<!-- plot(Zt, main = expression(paste("Simulated process ",Z[t])), ylim = range(c(Xt, Yt, Zt))) -->
<!-- ``` -->

<!-- Having highlighted the kind of processes we will be simulating from, we will now create a code that allows to apply the different methods in order to estimate the parameters of an AR(2) model from a given time series. The result of this code therefore will consist in the parameter estimates for the five methods discussed above (i.e. MLE, Yule-Walker (standard and robust), GMWM (standard and robust)) on an observed (simulated) time series. The code that follows this function allows to deliver separate boxplots for each method in order to assess the performance of these approaches within the simulation procedure (which will be described further on).  -->

<!-- ```{r simengine, cache = TRUE} -->
<!-- apply_methods = function(Xt){ -->
<!--   # Estimate ARIMA parameters using MLE -->
<!--   mod = arima(Xt, order = c(2, 0, 0), include.mean  = FALSE) -->

<!--   # Extract MLE Parameters (including sigma) -->
<!--   res.MLE = c(mod$coef, mod$sigma) -->

<!--   # Calculate ACF -->
<!--   autocorr = as.numeric(acf(Xt, lag.max = 2, plot = FALSE)$acf) -->
<!--   X = matrix(1,2,2) -->
<!--   X[1,2] = X[2,1] = autocorr[2] -->
<!--   y = autocorr[2:3] -->

<!--   # Compute Least Squares on ACF -->
<!--   svmat = solve(X) -->
<!--   phi.LS = svmat%*%y -->
<!--   sig2.LS = var(Xt)*(1 - t(y)%*%svmat%*%y) -->
<!--   res.LS = c(phi.LS, sig2.LS) -->

<!--   # Calculate RACF -->
<!--   rob.ccf = as.numeric(robcor::robacf(Xt, plot=FALSE, type = "covariance")$acf) -->
<!--   X[1,2] = X[2,1] = rob.ccf[2]/rob.ccf[1] -->
<!--   y = rob.ccf[2:3]/rob.ccf[1] -->

<!--   # Compute Least Squares on RACF -->
<!--   svmat = solve(X) -->
<!--   phi.RR = svmat%*%y -->
<!--   sig2.RR = rob.ccf[1]*(1 - t(y)%*%svmat%*%y) -->
<!--   res.RR = c(phi.RR, sig2.RR) -->

<!--   # Compute the GMWM Estimator -->
<!--   res.GMWM = gmwm2::gmwm(ARMA(2,0), Xt)$estimate -->
<!--   res.RGMWM = gmwm2::gmwm(ARMA(2,0), Xt, robust = TRUE)$estimate -->

<!--   # Return results -->
<!--   list(res.MLE = res.MLE, res.LS = res.LS, res.RR = res.RR, -->
<!--        res.GMWM = res.GMWM, res.RGMWM = res.RGMWM) -->
<!-- }  -->
<!-- ``` -->

<!-- ```{r simgraphcreator} -->
<!-- sim_study_graph = function(res.MLE, res.LS, res.RR, res.GMWM, res.RGMWM, -->
<!--                            theta = c(0.5, 0.25, 1), process_name = "NA") { -->

<!--   labels = c("MLE", "LS", "RLS", "GMWM", "RGMWM") -->
<!--   title = c(expression(phi [1]), expression(phi [2]), expression(sigma)) -->

<!--   par(mfrow=c(2,2), oma=c(0,0,2,0)) -->

<!--   for(i in 1:length(theta)) { -->

<!--     boxplot(res.MLE[, i], res.LS[, i], res.RR[, i], res.GMWM[, i], res.RGMWM[, i], col = "grey80", main = title[i], names = labels, las = 3) -->
<!--     abline(h = theta[i], col = "red") -->

<!--   } -->

<!--   title(paste("Estimations on process", process_name), outer=TRUE) -->


<!-- } -->
<!-- ``` -->

<!-- Having defined two functions that allow to estimate and summarize the results of the considered estimation techniques, we can now perform the simulation study. For this purpose we simulate 250 time series from the processes $(X_t)$, $(Y_t)$ and $(Z_t)$ and, for each simulation, we apply the five different methods thereby saving the results of the estimations for each simulated time series. By doing so, we will verify the finite sample behaviour of the methods and understand how they behave under the different settings discussed earlier (contaminated and uncontaminated observations). -->


<!-- ``` {r simuAR2study, cache = TRUE, fig.height = 4.5, fig.width = 9} -->
<!-- # Number of bootstrap iterations -->
<!-- B = 250 -->

<!-- # Simulation storage -->
<!-- res.xt.MLE = res.xt.LS = res.xt.RR = res.xt.GMWM = res.xt.RGMWM = matrix(NA, B, 3) -->
<!-- res.yt.MLE = res.yt.LS = res.yt.RR = res.yt.GMWM = res.yt.RGMWM = matrix(NA, B, 3) -->
<!-- res.zt.MLE = res.zt.LS = res.zt.RR = res.zt.GMWM = res.zt.RGMWM = matrix(NA, B, 3) -->

<!-- # Begin bootstrap -->
<!-- for (i in seq_len(B)){ -->

<!--   # Set seed for reproducibility -->
<!--   set.seed(i) -->

<!--   # Generate processes -->
<!--   Xt = gen_gts(n, AR(phi = c(0.5, 0.25), sigma2 = 1)) -->
<!--   Yt = gen_gts(n, AR(phi = c(0.5, 0.25), sigma2 = 1)) -->
<!--   Zt = gen_gts(n, AR(phi = c(0.5, 0.25), sigma2 = 1)) -->

<!--   # Generate Ut contamination process that replaces a portion of original signal -->
<!--   index_start = sample(1:(n-cont-1), 1) -->
<!--   index_end = index_start + cont - 1 -->
<!--   Yt[index_start:index_end] = gen_gts(cont, AR(phi = c(0.9,-0.4), sigma2 = 9)) -->

<!--   # Generate Nt contamination that inject noise at random -->
<!--   Zt[sample(n, cont, replace = FALSE)] = gen_gts(cont, WN(sigma2 = 9)) -->

<!--   # Compute estimates and store in the appropriate matrix -->
<!--   res = apply_methods(Xt) -->
<!--   res.xt.MLE[i,]  = res$res.MLE; res.xt.LS[i,] = res$res.LS; res.xt.RR[i,] = res$res.RR -->
<!--   res.xt.GMWM[i,] = res$res.GMWM; res.xt.RGMWM[i,] = res$res.RGMWM -->

<!--   res = apply_methods(Yt) -->
<!--   res.yt.MLE[i,]  = res$res.MLE; res.yt.LS[i,] = res$res.LS; res.yt.RR[i,] = res$res.RR -->
<!--   res.yt.GMWM[i,] = res$res.GMWM; res.yt.RGMWM[i,] = res$res.RGMWM -->

<!--   res = apply_methods(Zt) -->
<!--   res.zt.MLE[i,]  = res$res.MLE; res.zt.LS[i,] = res$res.LS; res.zt.RR[i,] = res$res.RR -->
<!--   res.zt.GMWM[i,] = res$res.GMWM; res.zt.RGMWM[i,] = res$res.RGMWM -->
<!-- } -->
<!-- ``` -->

<!-- Once we run this simulation (which can be time-demanding), we can now represent the results of the estimation methods that are denoted as follows: -->

<!-- - **MLE**: Maximum Likelihood Estimator -->
<!-- - **LS**: Yule-Walker estimator (based on a least-squares distance) -->
<!-- - **RLS**: the robust version of the Yule-Walker estimator -->
<!-- - **GMWM**: the standard version of the GMWM estimator -->
<!-- - **RGMWM**: the robust version of the GMWM estimator -->

<!-- The boxplots representing the distributions of estimated values on the process $(X_t)$ (i.e. uncontaminated) are shown below. -->

<!-- ```{r, dependson="simuAR2study"} -->
<!-- sim_study_graph(res.xt.MLE, res.xt.LS, res.xt.RR, res.xt.GMWM, res.xt.RGMWM, process_name = "Xt") -->
<!-- ``` -->

<!-- It can be seen how all methods appear to properly estimate the true parameter values on average. However, the MLE appears to be slightly more efficient (less variable) compared to the other methods and, in addition, the robust methods (RLS and RGMWM) appear to be less efficient than their standard counterparts. Now let us check the performance of the same methods when applied to the two contaminated processes $(Y_t)$ and $(Z_t)$. -->

<!-- ```{r, dependson="simuAR2study"} -->
<!-- sim_study_graph(res.yt.MLE, res.yt.LS, res.yt.RR, res.yt.GMWM, res.yt.RGMWM, process_name = "Yt") -->
<!-- sim_study_graph(res.zt.MLE, res.zt.LS, res.zt.RR, res.zt.GMWM, res.zt.RGMWM, process_name = "Zt") -->
<!-- ``` -->

<!-- It can be seen that for these two contaminated processes, the standard estimators appear to be (highly) biased for most of the estimated parameters (with one exception) while the robust estimators allow to remain close (on average) to the true parameter values that we are aiming to estimate. As you can observe, the type of contamination can be important in the choice of the robust estimator to be used as underlined by the different behaviour of the RLS and RGMWM estimators on $(Y_t)$ and $(Z_t)$. However, this discussion is out of the scope of this book. -->

To conclude this section on estimation, we now compare the above studied estimators in different applied settings where we can highlight how to assess which estimator is more appropriate according to the type of setting. For this purpose, let us start with an example we have already checked in the previous chapter when discussing standard and robust estimators of the ACF, more specifically the data on monthly precipitations. As mentioned before when discussing this example, the importance of modelling precipitation data lies in the fact that its usually used to successively model the entire water cycle. Common models for this purpose are either the white noise (WN) model or the AR(1) model. Let us compare the standard and robust ACF again to understand which of these two models seems more appropriate for the data at hand.

```{r}
compare_acf(hydro)
```

As we had underlined in the previous chapter, the standard ACF estimates would suggest that there appears to be no correlation among lags and consequently, the WN model would be the most appropriate. However, the robust ACF estimates depict an entirely different picture where it can be seen that there appears to be a significant autocorrelation over different lags which exponentially decay. Although there appears to be some seasonality in the plot, we will assume that the correct model for this data is an AR(1) since that's what hydrology theory suggests. Let us therefore estimate the parameters of this model by using a standard estimator (MLE) and a robust estimator (RGMWM). The estimates for the MLE are the following:

```{r}
mle_hydro = ar(as.vector(hydro), order.max = 1, method = "mle", demean = TRUE, aic = FALSE)

# MLE Estimates
c(mle_hydro$ar, mle_hydro$var.pred)
```

From these estimates it would appear that the autocorrelation between lagged variables (i.e. lags of order 1) is extremely low and that (as suggested by the standard ACF plot) a WN model may be more appropriate. Considering the robust ACF however, it is possible that the MLE estimates may not be reliable in this setting. Hence, let us use the RGMWM to estimate the same parameters.

```{r}
rgmwm_hydro = gmwm(AR(1), hydro, robust = TRUE)$estimate

# RGMWM Estimates
t(rgmwm_hydro)
```

In this case, we see how the autocorrelation between lagged values is much higher (0.4 compared to 0.06) indicating that there is a stronger dependence in the data than what is suggested by standard estimators. Moreover, the innovation variance is smaller compared to that of the MLE. This is also a known phenomenon when there's contamination in the data since it leads to less dependence and more variability being detected by non-robust estimators. This estimate of the variance also has a considerable impact on forecast precision (as we'll see in the next section).

A final applied example that highlights the (potential) difference between estimators according to the type of setting is given by the "Recruitment" data set (in the "astsa" library). This data refers to the presence of new fish in the population of the Pacific Ocean and is often linked to the currents and temperatures passing through the ocean. As for the previous data set, let us take a look at the data itself and then analyse the standard and robust estimations of the ACF.

```{r}
library(astsa)

# Format data
fish = gts(rec, start = 1950, freq = 12, unit_time = 'month', name_ts = 'Recruitment')

# Plot data
par(mfrow = c(2, 1))
plot(fish)
compare_acf(fish)
```

```{r, echo=FALSE}
par(mfrow = c(1,1))
```

We can see that there appears to be a considerable dependence between the lagged variables which decays exponentially (as an AR(p) process ACF generally does). Also in this case we see a seasonality in the data but we won't consider this for the purpose of this example. Given that there doesn't appear to be any significant contamination in the data, let us consider the Yule-Walker and MLE estimators. The MLE highly depends on the assumed parametric distribution of the time series (i.e. usually Gaussian) and, if this is not respected, the resulting estimations could be unreliable. Hence, a first difference of the time series can often give an idea of the marginal distribution of the time series.

```{r}
# Take first differencing of the recruitment data
diff_fish = gts(diff(rec), start = 1950, freq = 12, unit_time = 'month', name_ts = 'Recruitment')

# Plot first differencing of the recruitment data
plot(diff_fish)
```

From the plot we can see that observations appear to be collected around a constant value and fewer appear to be further from this value (as would be the case for a normal distribution). However, various of these "more extreme" observations appear to be quite frequent suggesting that the underlying distribution may have a heavier tail compared to the normal distribution. With this in mind, let us estimate an AR(2) model for this data using the Yule-Walker and MLE.

```{r}
# MLE of Recruitment data
yw_fish = ar(rec, order.max = 2, method = "yule-walker", demean = TRUE, aic = FALSE)

# MLE of Recruitment data
mle_fish = ar(rec, order.max = 2, method = "mle", demean = TRUE, aic = FALSE)

# Compare estimates
# Yule-Walker Estimation
c(yw_fish$ar, yw_fish$var.pred)

# MLE Estimation
c(mle_fish$ar, mle_fish$var.pred)
```

It can be seen that, in this setting, the two estimators deliver very similar results (at least in terms of the $\phi_1$ and $\phi_2$ coefficients). Indeed, there doesn't appear to be need for robust estimation and the choice of a standard estimator is justified by the will to obtain efficient estimations. The only slight difference between the two estimations is in the innovation variance parameter $\sigma^2$ and this could be (evenutally) due to the normality assumption that the MLE estimator upholds in this case. If there is therefore a doubt on the fact that the Gaussian assumption does not hold for this data, then it is probably more convenient to use the Yule-Walker estimates.

Until now we have focussed on estimation based on an assumed model. However, how do we choose a model? How can we make inference on the models and their parameters? To perform all these tasks we will need to compute residuals (as, for example, in the linear regression framework). In order to obtain residuals, we need to be able to predict (forecast) values of the time series and, consequently, the next section focuses on forecasting time series.

### Forecasting AR(p) Models

One of the most interesting things in time series analysis is to predict the future unobserved values based on the values that have been observed up to now. However, this is not possible if the underlying (parametric) model is unknown, thus in this section we assume the time series $X_t$ is stationary and its model is known. In particular, we denote forecasts by $X^{T}_{T+m}$, where $n$ represents the data points collected (e.g. $\mathbf{X} = (X_{1}, X_{2}, \cdots , X_{T-1}, X_T)$) and $m$ represents the amount of points in the future we wish to predict. So, $X^{T}_{T+1}$ represents a one-step-ahead prediction $X_{T+1}$ given data $(X_{1}, X_{2}, \cdots, X_{T-1}, X_{T})$.

To obtain forecasts, we rely upon the best linear prediction (BLP). Recall that the BLP Definition \@ref(def:blp) is mentioned when we calculate the PACF of AR model. In general, the best approach to finding the BLP is to use the Theorem \@ref(thm:projtheo).

```{theorem, label="projtheo", name="Projection Theorem"}
Let $\mathcal{M} \subset \mathcal{L}_2$ be a closed linear subspace of Hibert space. 
For every $y \in \mathcal{L}_2$, there exists a unique element $\hat{y} \in \mathcal{M}$ that
minimizes $||y - l||^2$ over $l \in \mathcal{M}$. This element is uniquely 
determined by the requirements

1. $\hat{y} \in \mathcal{M}$ and 
2. $(y - \hat{y}) \perp \mathcal{M}$.
```

<!-- \mbox{(trivial for time series with zero mean)} -->
  
  The projection theorem naturally leads to an equivalent way to find the best linear 
predictor by solving the prediction equations,
\[ \mathbb{E}(X_{t+h} - \hat{X}_{t+h}) = 0,\]
and
\[ \mathbb{E} [(X_{t+h} - \hat{X}_{t+h})X_j ] = 0, \mbox{ for } i = 1, \dots, t.\]

If we denote $\mathbb{E}(X_{i}, X_{j})$ as $\gamma(|i - j|)$, the predition equations can
be represented in the following form

\begin{equation}
\begin{aligned}
\begin{pmatrix}
\gamma(0) & \gamma(1) & \cdots & \gamma(T-1) \\
\gamma(1) & \gamma(0) & \cdots & \gamma(T-2) \\
\vdots & \vdots & \ddots & \vdots \\
\gamma(T-1) & \gamma(T-2) & \cdots &\gamma(0)
\end{pmatrix}_{T \times T}
\begin{pmatrix}
\alpha_1 \\
\vdots \\
\alpha_T
\end{pmatrix}_{T \times 1}
&=
  \begin{pmatrix}
\gamma(T+h-1)  \\
\vdots \\
\gamma(h)
\end{pmatrix}_{T \times 1} \\
\Gamma_T \mathbf{\alpha}_T  &= \mathbf{\gamma}_T
\end{aligned}
\end{equation}.

Assuming that $\Gamma_T$ is non-singular, then the values of $\mathbf{\alpha}_T$ are
given as:
  
  $$\mathbf{\alpha}_T  = \Gamma^{-1}_T\mathbf{\gamma}_T$$