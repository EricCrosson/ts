# Fundamental Properties of Time Series {#fundtimeseries}


>
> "*One of the first things taught in introductory statistics textbooks is that correlation is not causation. It is also one of the first things forgotten.*" â€“ Thomas Sowell
>

```{block2,  type='rmdimportant'}
To make use of the R code within this chapter you will need to install (if not already done) and load the following libraries:

  - [quantmod](https://cran.r-project.org/web/packages/quantmod/index.html);
  - [simts](http://simts.smac-group.com/);
  - [astsa](https://cran.r-project.org/web/packages/astsa/index.html).
```

```{r, echo=FALSE, include=FALSE}
library(astsa)
library(mgcv)
library(simts)
library(imudata)
library(timeDate)
```

In this chapter we will discuss and formalize how knowledge about $X_{t-1}$ (or
more generally about all the information from the past, $\Omega_t$) can provide 
us with some information about the properties of $X_t$. In particular, we will 
consider the correlation (or covariance) of $X_t$ at different times such 
as $\text{corr} \left(X_t, X_{t+h}\right)$. This "form" of correlation (covariance) is
called the *autocorrelation* (*autocovariance*) and is a very useful tool in
time series analysis. However, if we do not assume that a time series is 
characterized by a certain form of "stability", it would be rather difficult 
to estimate $\text{corr} \left(X_t, X_{t+h}\right)$ as this quantity would depend on 
both $t$ and $h$ leading to more parameters to estimate than observations 
available. Therefore, the concept of *stationarity* is convenient in this
context as it allows (among other things) to assume that

\[\text{corr} \left(X_t, X_{t+h}\right) = \text{corr} \left(X_{t+j}, X_{t+h+j}\right), \;\;\; \text{for all $j$},\]

implying that the autocorrelation (or autocovariance) is only a function of the
lag between observations, rather than time itself. We will first discuss the concept of autocorrelation in time series, then we will discuss stationarity which will then allow us to adequately define and study estimators of the autocorrelation functions. Before 
moving on, it is helpful to remember that correlation (or autocorrelation) is
only appropriate to measure a very specific kind of dependence, i.e. linear
dependence. There are many other forms of dependence as illustrated in the
bottom panels of the graph below, which all have a (true) zero correlation:

```{r correxample, cache = TRUE, echo = FALSE, fig.cap="Different forms of dependence and their Pearson's r values", fig.align = 'center'}
knitr::include_graphics("images/corr_example.png")
```

Several other metrics have been introduced in the literature to assess the
degree of "dependence" of two random variables, however this goes beyond the
material discussed in this chapter.

 
## The Autocorrelation and Autocovariance Functions 

We will introduce the autocorrelation function by first defining the **autocovariance function**.


```{definition, label="acvf"}
The *autocovariance function* of a series $(X_t)$ is defined as 

\[{\gamma_x}\left( {t,t+h} \right) \equiv \text{cov} \left( {{X_t},{X_{t+h}}} \right),\]
```

where the definition of covariance is given by:
  
  \[
    \text{cov} \left( {{X_t},{X_{t+h}}} \right) \equiv \mathbb{E}\left[ {{X_t}{X_{t+h}}} \right] - \mathbb{E}\left[ {{X_t}} \right]\mathbb{E}\left[ {{X_{t+h}}} \right].
    \]

Similarly, the above expectations are defined as:
  
  \[\begin{aligned}
     \mathbb{E}\left[ {{X_t}} \right] &\equiv \int\limits_{ - \infty }^\infty  {x \cdot {f_t}\left( x \right)dx},  \\
     \mathbb{E}\left[ {{X_t}{X_{t+h}}} \right] &\equiv \int\limits_{ - \infty }^\infty  {\int\limits_{ - \infty }^\infty  {{x_1}{x_2} \cdot f_{t,t+h}\left( {{x_1},{x_2}} \right)d{x_1}d{x_2}} } ,
     \end{aligned} \]

where ${f_t}\left( x \right)$ and $f_{t,t+h}\left( {{x_1},{x_2}} \right)$ denote,
respectively, the density of $X_t$ and the joint density of the 
pair $(X_t, X_{t+h})$. Considering the notation used above, it should be clear that $X_t$ is assumed to be a continous random variable. Since we generally consider stochastic processes with constant zero mean, we often have

\[{\gamma_x}\left( {t,t+h} \right) = \mathbb{E}\left[X_t X_{t+h} \right]. \]

In addition, in the context of this book we will normally drop the subscript referring to the time series (i.e. $x$ in this case) if it is clear from the context which time
series the autocovariance refers to. For example, we generally use ${\gamma}\left( {t,t+h} \right)$ instead of ${\gamma_x}\left( {t,t+h} \right)$. Moreover, the notation is even further simplified when the covariance of $X_t$ and $X_{t+h}$ is the same as that of $X_{t+j}$ and $X_{t+h+j}$ (for all $j$), i.e. the covariance depends only 
on the time between observations and not on the specific time $t$. This is an 
important property called *stationarity*, which will be discussed in the next
section. In this case, we simply use to following notation:

  \[\gamma \left( {h} \right) = \text{cov} \left( X_t , X_{t+h} \right). \]

This is the definition of autocovariance that will be used from this point onwards and therefore this notation will generally be used throughout the text thereby implying 
certain properties for the process $(X_t)$ (i.e. stationarity) . 
With this in mind, several remarks can be made on the autocovariance function:
  
  1. The autocovariance function is *symmetric*. 
That is, ${\gamma}\left( {h} \right) = {\gamma}\left( -h \right)$ 
  since $\text{cov} \left( {{X_t},{X_{t+h}}} \right) = \text{cov} \left( X_{t+h},X_{t} \right)$.
2. The autocovariance function "contains" the variance of the process 
as $\text{var} \left( X_{t} \right) = {\gamma}\left( 0 \right)$.
3. We have that $|\gamma(h)| \leq \gamma(0)$ for all $h$. The proof of this 
inequality is direct and follows from the Cauchy-Schwarz inequality, i.e.
\[ \begin{aligned}
  \left(|\gamma(h)| \right)^2 &= \gamma(h)^2 = \left(\mathbb{E}\left[\left(X_t - \mathbb{E}[X_t] \right)\left(X_{t+h} - \mathbb{E}[X_{t+h}] \right)\right]\right)^2\\
  &\leq \mathbb{E}\left[\left(X_t - \mathbb{E}[X_t] \right)^2 \right] \mathbb{E}\left[\left(X_{t+h} - \mathbb{E}[X_{t+h}] \right)^2 \right] =  \gamma(0)^2. 
  \end{aligned}
  \] 
4. Just as any covariance, ${\gamma}\left( {h} \right)$ is "scale dependent"
since ${\gamma}\left( {h} \right) \in \mathbb{R}$, 
or $-\infty \le {\gamma}\left( {h} \right) \le +\infty$. We therefore have:
  - if $\left| {\gamma}\left( {h} \right) \right|$ is "close" to zero, 
then $X_t$ and $X_{t+h}$ are "weakly" (linearly) dependent;
- if $\left| {\gamma}\left( {h} \right) \right|$ is "far" from zero, 
then the two random variable present a "strong" (linear) dependence. 
However it is generally difficult to asses what "close" and "far" from 
zero means in this case. 
5. ${\gamma}\left( {h} \right)=0$ does not imply that $X_t$ and $X_{t+h}$ are 
independent but simply $X_t$ and $X_{t+h}$ are uncorrelated. 
The independence is only implied by ${\gamma}\left( {h} \right)=0$ in
the jointly Gaussian case.

As hinted in the introduction, an important related statistic is the correlation
of $X_t$ with $X_{t+h}$ or *autocorrelation*, which is defined as

$$\rho \left(  h \right) = \text{corr}\left( {{X_t},{X_{t + h}}} \right) = \frac{{\text{cov}\left( {{X_t},{X_{t + h}}} \right)}}{{{\sigma _{{X_t}}}{\sigma _{{X_{t + h}}}}}} = \frac{\gamma(h) }{\gamma(0)}.$$
  
Similarly to $\gamma(h)$, it is important to note that the above notation 
implies that the autocorrelation function is only a function of the 
lag $h$ between observations. Thus, autocovariances and autocorrelations are one
possible way to describe the joint distribution of a time series. Indeed, the 
correlation of $X_t$ with $X_{t+1}$ is an obvious measure of how *persistent* a
time series is. 

Remember that just as with any correlation:
  
1. $\rho \left( h \right)$ is "scale free" so it is much easier to interpret 
than $\gamma(h)$.
2. $|\rho \left( h \right)| \leq 1$ since $|\gamma(h)| \leq \gamma(0)$.
3. **Causation and correlation are two very different things!**
  
### A Fundamental Representation
  
Autocovariances and autocorrelations also turn out to be very useful tools as
they are one of the *fundamental representations* of time series. Indeed, if we
consider a zero mean normally distributed process, it is clear that its joint
distribution is fully characterized by the 
autocovariances $\mathbb{E}[X_t X_{t+h}]$ (since the joint probability density only depends of these covariances). Once we know the autocovariances we
know *everything* there is to know about the process and therefore:
  
> if two processes have the same autocovariance function, then they are the same process.
  
### Admissible Autocorrelation Functions `r emo::ji("scream")`
  
Since the autocorrelation function is one of the fundamental representations of time
series, it implies that one might be able to define a stochastic process by 
picking a set of autocorrelation values (assuming for example that $\text{var}(X_t) = 1$). 
However, it turns out that not every collection of
numbers, say $\{\rho_1, \rho_2, ...\}$, can represent the autocorrelation of a
process. Indeed, two conditions are required to ensure the validity of an
autocorrelation sequence:
  
1. $\operatorname{max}_j \; \left| \rho_j \right| \leq 1$.
2. $\text{var} \left[\sum_{j = 0}^\infty \alpha_j X_{t-j} \right] \geq 0 \;$ for all $\{\alpha_0, \alpha_1, ...\}$.

The first condition is obvious and simply reflects the fact 
that $|\rho \left( h \right)| \leq 1$ but the second is far more difficult to
verify. To further our understanding of the latter we let $\alpha_j = 0$ for $j > 1$ and see that in this case the second condition implies that

\[\text{var} \left[ \alpha_0 X_{t} + \alpha_1 X_{t-1}  \right] = \gamma_0 \begin{bmatrix}
   \alpha_0 & \alpha_1
   \end{bmatrix}   \begin{bmatrix}
   1 & \rho_1\\
   \rho_1 & 1
   \end{bmatrix} \begin{bmatrix}
   \alpha_0 \\
   \alpha_1
   \end{bmatrix} \geq 0. \]

Thus, the matrix 

\[ \boldsymbol{A}_1 = \begin{bmatrix}
  1 & \rho_1\\
  \rho_1 & 1
  \end{bmatrix}, \]

must be positive semi-definite. Taking the determinant we have 

\[\operatorname{det} \left(\boldsymbol{A}_1\right) = 1 - \rho_1^2, \]

implying that the condition $|\rho_1| \leq 1$ must be respected. 
Now, let $\alpha_j = 0$ for $j > 2$, then we must verify that:
  
  \[\text{var} \left[ \alpha_0 X_{t} + \alpha_1 X_{t-1}  + \alpha_2 X_{t-2} \right] = \gamma_0 \begin{bmatrix}
     \alpha_0 & \alpha_1 &\alpha_2
     \end{bmatrix}   \begin{bmatrix}
     1 & \rho_1 & \rho_2\\
     \rho_1 & 1 & \rho_1 \\
     \rho_2 & \rho_1 & 1
     \end{bmatrix} \begin{bmatrix}
     \alpha_0 \\
     \alpha_1 \\
     \alpha_2
     \end{bmatrix} \geq 0. \]

Again, this implies that the matrix

\[ \boldsymbol{A}_2 = \begin{bmatrix}
  1 & \rho_1 & \rho_2\\
  \rho_1 & 1 & \rho_1 \\
  \rho_2 & \rho_1 & 1
  \end{bmatrix}, \]

must be positive semi-definite and it is easy to verify that

\[\operatorname{det} \left(\boldsymbol{A}_2\right) = \left(1 - \rho_2 \right)\left(- 2 \rho_1^2 + \rho_2 + 1\right). \]

Thus, this implies that 

\[\begin{aligned} &- 2 \rho_1^2 + \rho_2 + 1 \geq 0 \Rightarrow 1 \geq \rho_2 \geq 2 \rho_1^2 - 1 \\
   &\Rightarrow 1 - \rho_1^2 \geq \rho_2 - \rho_1^2 \geq -(1 - \rho_1^2)\\
   &\Rightarrow 1 \geq \frac{\rho_2 - \rho_1^2 }{1 - \rho_1^2} \geq -1.
   \end{aligned}\]

Therefore, $\rho_1$ and $\rho_2$ must lie in a parabolic shaped region defined
by the above inequalities as illustrated in Figure \@ref(fig:admissibility).

```{r admissibility, cache = TRUE, echo = FALSE, fig.cap = "Admissible autocorrelation functions", fig.align='center', fig.height = 5, fig.width = 7}
plot(NA, xlim = c(-1.1,1.1), ylim = c(-1.1,1.1), xlab = expression(rho[1]),
     ylab = expression(rho[2]), cex.lab = 1.5)
grid()

# Adding boundary of constraint |rho_1| < 1
abline(v = c(-1,1), lty = 2, col = "darkgrey")

# Adding boundary of constraint |rho_2| < 1
abline(h = c(-1,1), lty = 2, col = "darkgrey")

# Adding boundary of non-linear constraint
rho1 = seq(from = -1, to = 1, length.out = 10^3)
rho2 = (rho1^2 - 1) + rho1^2 
lines(rho1, rho2, lty = 2, col = "darkgrey")

# Adding admissible region
polygon(c(rho1,rev(rho1)),c(rho2,rep(1,10^3)),
        border = NA, col= rgb(0,0,1, alpha = 0.1))

# Adding text
text(0,0, c("Admissible Region"))
```

From our derivation, it is clear that the restrictions on the autocorrelation are
very complicated, thereby justifying the need for other forms of fundamental 
representation which we will explore later in this text. Before moving on to
the estimation of the autocorrelation and autocovariance functions, we must 
first discuss the stationarity of $(X_t)$, which will provide a convenient 
framework in which $\gamma(h)$ and $\rho(h)$ can be used (rather that $\gamma(t,t+h)$ 
for example) and (easily) estimated.


## Stationarity {#stationary}

There are two kinds of stationarity that are commonly used. They are defined 
as follows:
  
```{definition, label="strongstationarity"}
A process $(X_t)$ is *strongly stationary* or *strictly stationary* if the joint probability distribution of $(X_{t-h}, ..., X_t, ..., X_{t+h})$ is independent of $t$ for all $t,h \in \mathbb{Z}$.
```
<br>

```{definition, label="weakstationarity"}
A process $(X_t)$ is *weakly stationary*, *covariance stationary* or *second order stationary* if $\mathbb{E}[X_t]$ and $\mathbb{E}[X_t^2]$ are finite and $\mathbb{E}[X_t X_{t-h}]$ depends only on $h$ and not on $t$ for all $t,h \in \mathbb{Z}$.
```
<br>

These types of stationarity are *not equivalent* and the presence of one kind 
of stationarity does not imply the other. That is, a time series can be strongly
stationary but not weakly stationary and vice versa. In some cases, a time 
series can be both strongly and weakly stationary and this occurs, for 
example, in the (jointly) Gaussian case. Stationarity of $(X_t)$ matters
because *it provides the framework in which averaging dependent data makes
sense*, thereby allowing us to easily obtain estimates for certain quantities
such as autocorrelation.

Several remarks and comments can be made on these definitions:
  
- As mentioned earlier, strong stationarity *does not imply* weak stationarity. For example, an $iid$ Cauchy process is strongly but not weakly stationary (why? `r emo::ji("thinking_face")`).


- Weak stationarity *does not imply* strong stationarity. For example, consider the following weak white noise process:
  \begin{equation*}
X_t = \begin{cases}
U_{t}      & \quad \text{if } t \in \{2k:\, k\in \mathbb{Z} \}, \\
V_{t}      & \quad \text{if } t \in \{2k+1:\, k\in \mathbb{Z} \},\\
\end{cases}
\end{equation*}
where ${U_t} \mathop \sim \limits^{iid} N\left( {1,1} \right)$ and ${V_t}\mathop \sim \limits^{iid} \mathcal{E}\left( 1 \right)$ is a weakly stationary process that is *not* strongly stationary.

- Strong stationarity combined with bounded values of 
  $\mathbb{E}[X_t^2]$ *implies* weak stationarity.
- Weak stationarity combined with normally distributed processes *implies* 
  strong stationarity.
  
```{exercise, moment, name = "Existence of moments"}
It is important to note that, for a given value of $r$ and a random variable $X$, the expected $\mathbb{E}[X^r]$ may be infinite, or may not exist. If there exist an $r \in \mathbb{N}^+$ such that $\mathbb{E}[|X|^r]$ exist and is bounded, then we have that $\mathbb{E}[X^j] < \infty$ for all $j = 1,...,r$. This result is implied by Jensenâ€™s inequality. Indeed, the function $f(x) = x^k$ is convex for $t > 0$ and $m > 1$, so we have
$$\mathbb{E}\left[|X |^j\right]^{\frac{r}{j}} \leq \mathbb{E}\left[(|X |^j)^{\frac{r}{j}}\right] = \mathbb{E}\left[|X|^r\right] < \infty.$$
Therefore, we obtain $\mathbb{E}[X^j] \leq \mathbb{E}[|X|^j] < \infty$.
```

### Assessing Weak Stationarity of Time Series Models

It is important to understand how to verify if a postulated model is (weakly)
stationary. In order to do so, we must ensure that our model satisfies the
following three properties:
  
1. $\mathbb{E}\left[X_t \right] = \mu_t = \mu < \infty$,
2. $\text{var}\left[X_t \right] = \sigma^2_t = \sigma^2 < \infty$,
3. $\text{cov}\left(X_t, X_{t+h} \right) = \gamma \left(h\right)$ (i.e. the autocovariance only depends on $h$ and not on $t$).

In the following examples, we evaluate the stationarity of the processes
introduced in Section \@ref(basicmodels).

```{example, label="gwn", name = "Gaussian White Noise"} 
It is easy to verify that this process is stationary. Indeed, we have:
  
1. $\mathbb{E}\left[ {{X_t}} \right] = 0$,
2. $\gamma(0) = \sigma^2 < \infty$,  
3. $\gamma(h) = 0$ for $|h| > 0$.
```


```{example, label="srw", name = "Random Walk"}
To evaluate the stationarity of this process, we first derive its properties:

1. We begin by calculating the expectation of the process:
\[
  \mathbb{E}\left[ {{X_t}} \right] = \mathbb{E}\left[ {{X_{t - 1}} + {W_t}} \right]
  = \mathbb{E}\left[ {\sum\limits_{i = 1}^t {{W_i}}  + {X_0}} \right] 
  = \mathbb{E}\left[ {\sum\limits_{i = 1}^t {{W_i}} } \right] + {c} 
  = c.  \] 

Observe that the mean obtained is constant since it depends only on the value
of the first term in the sequence.

2. Next, after finding the mean to be constant, we calculate the variance to check stationarity:
  \[\begin{aligned}
     \text{var}\left( {{X_t}} \right) &= \text{var}\left( {\sum\limits_{i = 1}^t {{W_t}}  + {X_0}} \right) 
     = \text{var}\left( {\sum\limits_{i = 1}^t {{W_i}} } \right) + \underbrace {\text{var}\left( {{X_0}} \right)}_{= 0} \\
     &= \sum\limits_{i = 1}^t {\text{var}\left( {{W_i}} \right)} 
     = t \sigma_w^2,
     \end{aligned}\] 
where $\sigma_w^2 = \text{var}(W_t)$. Therefore, the variance depends on time $t$, contradicting our
second property. Moreover, we have:
  \[\mathop {\lim }\limits_{t \to \infty } \; \text{var}\left(X_t\right) = \infty.\]
This process is therefore not weakly stationary.

3. Regarding the autocovariance of a random walk, we have:
  \[\begin{aligned}
     \gamma \left( h \right) &= \text{cov}\left( {{X_t},{X_{t + h}}} \right) 
     = \text{cov}\left( {\sum\limits_{i = 1}^t {{W_i}} ,\sum\limits_{j = 1}^{t + h} {{W_j}} } \right) 
     = \text{cov}\left( {\sum\limits_{i = 1}^t {{W_i}} ,\sum\limits_{j = 1}^t {{W_j}} } \right)\\ 
     &= \min \left( {t,t + h} \right)\sigma _w^2
     = \left( {t + \min \left( {0,h} \right)} \right)\sigma _w^2,
     \end{aligned} \]
which further illustrates the non-stationarity of this process.

Moreover, the autocorrelation of this process is given by

\[\rho (h) = \frac{t + \min \left( {0,h} \right)}{\sqrt{t}\sqrt{t+h}},\]

implying (for a fixed $h$) that

\[\mathop {\lim }\limits_{t \to \infty } \; \rho(h) = 1.\]

Note that using $\gamma (h)$ and $\rho (h)$ in this context is actually an abuse of notation since both of these quantites are here function of $h$ and $t$.
```


