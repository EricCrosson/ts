
# Stationarity {#stationary}

In this chapter we discuss one of the most important properties of a time series: *stationarity*. Before entering the technical definitions and details behing this property, let us give an idea of what this property implies. Let us assume we have a stochastic process from which we randomly choose two distinct indices $i$ and $j$ thereby forming a vector of random variables $(X_i, X_j)$ which follow a bivariate distribution $F_{\mathbf{\theta}}$ (in our case the indices $i$ and $j$ can refer to two distinct time points). In order to compute probabilities of certain events concerning the joint behaviour of the two random variables we can use the properties of $F_{\mathbf{\theta}}$ to compute them. Now let us assume that we want to study the probabilistic behaviour of other two random variables from the stochastic process but indexed by $X_{i+h}$ and $X_{j+h}$, where $h \in \mathbb{N}^+$. The latter selection implies that we select another couple of random variables that preserve the same distance from each other but at another set of indices (or future time points). If we were interested in ``predicting'' the joint behaviour of these two new random variables, then we would use the information from the previous couple given by $F_{\mathbf{\theta}}$ assuming that the news ones would follow the same behaviour (i.e. follow the same bivariate distribution $F_{\mathbf{\theta}}$). This assumption is often (unknowingly) used in practice to make predictions of time series but it is a strong assumption to use in many cases. 

Indeed, there are many cases where the joint probabilistic behaviour of random variables changes over time and this is defined as *non-stationarity*. Consider the $\texttt{djia}$ data set made available through the $\texttt{xts}$ package which collects the Dow Jones Industrial Average (DJIA) between April 2006 and April 2016. In the figure below we can see the log-returns of this financial index where it is possible to observe a stable mean-trend through time (i.e. the time series appears to fluctuate around zero).

```{r, echo=FALSE}
library(astsa)
library(xts)
djiar = diff(log(djia$Close))[-1]
plot(djiar, main="DJIA Returns", type="n")
lines(djiar)
```

However, would you say that there is the same probability of observing a log-return larger than 0.05 between 2008 and 2010 as there is between 2012 and 2014? It appears that the probabilistic behaviour of this time series changes over time although the mean seems to remain the same. This is a form of non-stationarity which is often observed, for example, in financial time series where the volatility (variance) of a stock changes over time according to the stability of the financial markets.

Having given an idea of what non-stationarity implies, in the rest of this chapter we will more formally define and describe the concept of stationary time series.


## Definitions

Let us now formally introduce the concept of stationarity starting from the strictest definition which is that of *strong stationarity*:
  
```{definition, label="strongstationarity"}
A process $(X_t)$ is *strongly stationary* or *strictly stationary* if the joint probability distribution of $(X_{t-h}, ..., X_t, ..., X_{t+h})$ is independent of $t$ for all $h$.
```

This means that the joint distribution of $(X_t, ..., X_{t+j})$, for $j \in \mathbb{N}^+$, is the same as the joint distribution of $(X_{t+h}, ..., X_{t+j+h})$, for any $h \in \mathbb{N}^+$. Hence, random variables that maintain a fixed lag between each other will preserve the same joint probabilistic behaviour no matter the point in time. ALthough this is the strict definition of stationarity, it is impossible to understand if this assumption holds for an observed time series. Therefore, another definition of stationarity is given below:

```{definition, label="weakstationarity"}
A process $(X_t)$ is *weakly stationary*, *covariance stationary* or *second order stationary* if $\mathbb{E}[X_t]$, $\mathbb{E}[X_t^2]$ are finite and $\mathbb{E}[X_t X_{t-h}]$ depends only on $h$ and not on $t$.
```

The above definition is less strong than the previous one and can more easily be assumed for different observed time series. Hence, these types of stationarity are *not equivalent* and the presence of one kind 
of stationarity does not imply the other. That is, a time series can be strongly
stationary but not weakly stationary and vice versa. In some cases, a time 
series can be both strongly and weakly stationary and this is occurs, for 
example, in the (jointly) Gaussian case. Stationarity of $(X_t)$ matters
because *it provides the framework in which averaging dependent data makes
sense*, thereby allowing us to easily obtain estimates for certain quantities
such as autocorrelation.

Several remarks and comments can be made on these definitions:
  
- As mentioned earlier, strong stationarity *does not imply* weak stationarity. 

```{example, label="strongnotweak"}
an $iid$ Cauchy process is strongly but not weakly stationary.
```

- Weak stationarity *does not imply* strong stationarity. 

```{example, label="weaksplit"}
Consider the following weak white noise process:
  \begin{equation*}
X_t = \begin{cases}
U_{t}      & \quad \text{if } t \in \{2k:\, k\in \mathbb{Z} \}, \\
V_{t}      & \quad \text{if } t \in \{2k+1:\, k\in \mathbb{Z} \},\\
\end{cases}
\end{equation*}
where ${U_t} \mathop \sim \limits^{iid} N\left( {1,1} \right)$ and ${V_t}\mathop \sim \limits^{iid} \mathcal{E}\left( 1 \right)$ is a weakly stationary process that is *not* strongly stationary.
```

- Strong stationarity combined with bounded values $\mathbb{E}[X_t]$ 
  and $\mathbb{E}[X_t^2]$ *implies* weak stationarity.
- Weak stationarity combined with normally distributed processes *implies* 
  strong stationarity.


### Assessing Weak Stationarity of Time Series Models

It is important to understand how to verify if a postulated model is (weakly)
stationary. In order to do so, we must ensure that our model satisfies the
following three properties:
  
1. $\mathbb{E}\left[X_t \right] = \mu_t = \mu < \infty$,
2. $\text{var}\left[X_t \right] = \sigma^2_t = \sigma^2 < \infty$,
3. $\text{cov}\left(X_t, X_{t+h} \right) = \gamma \left(h\right)$.

In the following examples, we evaluate the stationarity of the processes
introduced in Section \@ref(basicmodels).

```{example, label="gwn", name = "Gaussian White Noise"} 
It is easy to verify that this process is stationary. Indeed, we have:
  
1. $\mathbb{E}\left[ {{X_t}} \right] = 0$,
2. $\gamma(0) = \sigma^2 < \infty$,  
3. $\gamma(h) = 0$ for $|h| > 0$.

```

```{example, label="srw", name = "Random Walk"}
To evaluate the stationarity of this process, we first derive its properties:

1. We begin by calculating the expectation of the process:
\[
  \mathbb{E}\left[ {{X_t}} \right] = \mathbb{E}\left[ {{X_{t - 1}} + {W_t}} \right]
  = \mathbb{E}\left[ {\sum\limits_{i = 1}^t {{W_i}}  + {X_0}} \right] 
  = \mathbb{E}\left[ {\sum\limits_{i = 1}^t {{W_i}} } \right] + {c} 
  = c.  \] 

Observe that the mean obtained is constant since it depends only on the value
of the first term in the sequence.

2. Next, after finding the mean to be constant, we calculate the variance to check stationarity:
  \[\begin{aligned}
     \text{var}\left( {{X_t}} \right) &= \text{var}\left( {\sum\limits_{i = 1}^t {{W_t}}  + {X_0}} \right) 
     = \text{var}\left( {\sum\limits_{i = 1}^t {{W_i}} } \right) + \underbrace {\text{var}\left( {{X_0}} \right)}_{= 0} \\
     &= \sum\limits_{i = 1}^t {\text{var}\left( {{W_i}} \right)} 
     = t \sigma_w^2,
     \end{aligned}\] 
where $\sigma_w^2 = \text{var}(W_t)$. Therefore, the variance depends on time $t$, contradicting our
second property. Moreover, we have:
  \[\mathop {\lim }\limits_{t \to \infty } \; \text{var}\left(X_t\right) = \infty.\]
This process is therefore not weakly stationary.

3. Regarding the autocovariance of a random walk, we have:
  \[\begin{aligned}
     \gamma \left( h \right) &= \text{cov}\left( {{X_t},{X_{t + h}}} \right) 
     = \text{cov}\left( {\sum\limits_{i = 1}^t {{W_i}} ,\sum\limits_{j = 1}^{t + h} {{W_j}} } \right) 
     = \text{cov}\left( {\sum\limits_{i = 1}^t {{W_i}} ,\sum\limits_{j = 1}^t {{W_j}} } \right)\\ 
     &= \min \left( {t,t + h} \right)\sigma _w^2
     = \left( {t + \min \left( {0,h} \right)} \right)\sigma _w^2,
     \end{aligned} \]
which further illustrates the non-stationarity of this process.

Moreover, the autocorrelation of this process is given by

\[\rho (h) = \frac{t + \min \left( {0,h} \right)}{\sqrt{t}\sqrt{t+h}},\]

implying (for a fixed $h$) that

\[\mathop {\lim }\limits_{t \to \infty } \; \rho(h) = 1.\]
```

In the following simulated example, we illustrate the non-stationary feature of
such a process:
  
```{r RWsim, cache = TRUE, fig.cap = "Two hundred simulated random walks.", fig.align='center', fig.height = 5, fig.width = 7}
# Number of simulated processes
B = 200

# Length of random walks
n = 1000

# Output matrix
out = matrix(NA,B,n)

# Set seed for reproducibility
set.seed(6182)

# Simulate Data
for (i in seq_len(B)){
  # Simulate random walk
  Xt = gen_gts(n, RW(gamma = 1))
  
  # Store process
  out[i,] = Xt
}

# Plot random walks
plot(NA, xlim = c(1,n), ylim = range(out), xlab = "Time", ylab = " ")
grid()
color = sample(topo.colors(B, alpha = 0.5))
grid()
for (i in seq_len(B)){
  lines(out[i,], col = color[i])
}

# Add 95% confidence region
lines(1:n, 1.96*sqrt(1:n), col = 2, lwd = 2, lty = 2)
lines(1:n, -1.96*sqrt(1:n), col = 2, lwd = 2, lty = 2)
```

In the plot above, two hundred simulated random walks are plotted along with
theoretical 95% confidence intervals (red-dashed lines). The relationship
between time and variance can clearly be observed (i.e. the variance of the 
process increases with the time).

```{example, label="exma1", name = "Moving Average of Order 1"}

Similarly to our previous examples, we attempt to verify the stationary 
properties for the MA(1) model defined in Section \@ref(ma1):
  
1. \[ 
    \mathbb{E}\left[ {{X_t}} \right] = \mathbb{E}\left[ {{\theta_1}{W_{t - 1}} + {W_t}} \right] 
    = {\theta_1} \mathbb{E} \left[ {{W_{t - 1}}} \right] + \mathbb{E}\left[ {{W_t}} \right] 
    = 0. \] 
2. \[\text{var} \left( {{X_t}} \right) = \theta_1^2 \text{var} \left( W_{t - 1}\right) + \text{var} \left( W_{t}\right) = \left(1 + \theta^2 \right) \sigma^2_w.\]  
3. Regarding the autocovariance, we have 
\[\begin{aligned}
   \text{cov}\left( {{X_t},{X_{t + h}}} \right) &= \mathbb{E}\left[ {\left( {{X_t} - \mathbb{E}\left[ {{X_t}} \right]} \right)\left( {{X_{t + h}} - \mathbb{E}\left[ {{X_{t + h}}} \right]} \right)} \right] = \mathbb{E}\left[ {{X_t}{X_{t + h}}} \right] \\
   &= \mathbb{E}\left[ {\left( {{\theta}{W_{t - 1}} + {W_t}} \right)\left( {{\theta }{W_{t + h - 1}} + {W_{t + h}}} \right)} \right] \\
   &= \mathbb{E}\left[ {\theta^2{W_{t - 1}}{W_{t + h - 1}} + \theta {W_t}{W_{t + h}} + {\theta}{W_{t - 1}}{W_{t + h}} + {W_t}{W_{t + h}}} \right]. \\
   \end{aligned} \] 
It is easy to see that $\mathbb{E}\left[ {{W_t}{W_{t + h}}} \right] = {\boldsymbol{1}_{\left\{ {h = 0} \right\}}}\sigma _w^2$ and therefore, we obtain
\[\text{cov} \left( {{X_t},{X_{t + h}}} \right) = \left( {\theta^2{ \boldsymbol{1}_{\left\{ {h = 0} \right\}}} + {\theta}{\boldsymbol{1}_{\left\{ {h = 1} \right\}}} + {\theta}{\boldsymbol{1}_{\left\{ {h =  - 1} \right\}}} + {\boldsymbol{1}_{\left\{ {h = 0} \right\}}}} \right)\sigma _w^2\] 
implying the following autocovariance function:
  \[\gamma \left( h \right) = \left\{ {\begin{array}{*{20}{c}}
    {\left( {\theta^2 + 1} \right)\sigma _w^2}&{h = 0} \\ 
    {{\theta}\sigma _w^2}&{\left| h \right| = 1} \\ 
    0&{\left| h \right| > 1} 
    \end{array}} \right. .\]
Therefore, an MA(1) process is weakly stationary since both the mean and variance are constant over time and its covariance function is only a function of the lag $(h)$. Finally, we can easily obtain the autocorrelation for this process, which is given by
$$\rho \left( h \right) = \left\{ {\begin{array}{*{20}{c}}
  1&{h = 0} \\ 
  {\frac{{{\theta}\sigma _w^2}}{{\left( {\theta^2 + 1} \right)\sigma _w^2}} = \frac{{{\theta}}}{{\theta^2 + 1}}}&{\left| h \right| = 1} \\ 
  0&{\left| h \right| > 1} 
  \end{array}} \right. .$$
    Interestingly, we can note that $|\rho(1)| \leq 0.5$.
```

```{example, label="exar1", name = "Autoregressive of Order 1"}
  As another example, we shall verify the stationary properties for the AR(1)
  model defined in Section \@ref(ar1).
  
  Using the *backsubstitution* technique, we can rearrange an AR(1) process so 
  that it is written in a more compact form, i.e.
  
  \[\begin{aligned}
     {X_t} & =  {\phi }{X_{t - 1}} + {W_t} = \phi \left[ {\phi {X_{t - 2}} + {W_{t - 1}}} \right] + {W_t} 
     =  {\phi ^2}{X_{t - 2}} + \phi {W_{t - 1}} + {W_t}  \\
     &  \vdots  \\
     & =  {\phi ^k}{X_{t-k}} + \sum\limits_{j = 0}^{k - 1} {{\phi ^j}{W_{t - j}}} .
     \end{aligned} \]
  
  By taking the limit in $k$ (which is perfectly valid as we 
                              assume $t \in \mathbb{Z}$) and assuming $|\phi|<1$, we obtain
  
  \[\begin{aligned}
     X_t = \mathop {\lim }\limits_{k \to \infty} \; {X_t}  =  \sum\limits_{j = 0}^{\infty} {{\phi ^j}{W_{t - j}}} 
     \end{aligned} \]
  
  and therefore such process can be interpreted as a linear combination of the 
  white noise $(W_t)$ and corresponds (as we will observe later on) to an MA($\infty$). 
  In addition, the requirement $\left| \phi  \right| < 1$ turns out to be 
  extremely useful as the above formula is related to a **geometric series**, which
  would diverge if $\phi \geq 1$. Indeed, remember that
  an infinite (converging) geometric series is given by
  
  \[\sum\limits_{k = 0}^\infty  \, a{{r^k}}  = \frac{a}{{1 - r}}, \; {\text{ if }}\left| r \right| < 1.\]
  
  <!--
    The origin of the requirement comes from needing to ensure that the characteristic polynomial solution for an AR1 lies outside of the unit circle. Subsequently, stability enables the process to be stationary. If $\phi  \ge 1$, the process would not converge. Under the requirement, the process can represented as a
  -->
    
With this setup, we demonstrate how crucial this property is by calculating each of the requirements of a stationary process.
  
  1. First, we will check if the mean is stationary. 
  In this case, we opt to use limits to derive the expectation
  \[\begin{aligned}
     \mathbb{E}\left[ {{X_t}} \right] &= \mathop {\lim }\limits_{k \to \infty } \mathbb{E}\left[ {{\phi^k}{X_{t-k}} + \sum\limits_{j = 0}^{k - 1} {\phi^j{W_{t - j}}} } \right] \\
     &= \mathop {\lim }\limits_{k \to \infty } \, \underbrace {{\phi ^k}{\mathbb{E}[X_{t-k}]}}_{= 0} + \mathop {\lim }\limits_{k \to \infty } \, \sum\limits_{j = 0}^{k - 1} {\phi^j\underbrace {\mathbb{E}\left[ {{W_{t - j}}} \right]}_{ = 0}}
     = 0.
     \end{aligned} \] As expected, the mean is zero and, hence, the first criteria for weak stationarity is satisfied. 
  2. Next, we opt to determine the variance of the process
  \[\begin{aligned}
     \text{var}\left( {{X_t}} \right) &= \mathop {\lim }\limits_{k \to \infty } \text{var}\left( {{\phi^k}{X_{t-k}} + \sum\limits_{j = 0}^{k - 1} {\phi^j{W_{t - j}}} } \right)
     = \mathop {\lim }\limits_{k \to \infty } \sum\limits_{j = 0}^{k - 1} {\phi ^{2j} \text{var}\left( {{W_{t - j}}} \right)}  \\
     &= \mathop {\lim }\limits_{k \to \infty } \sum\limits_{j = 0}^{k - 1} \sigma _W^2 \, {\phi ^{2j}}  =  
       \underbrace {\frac{\sigma _W^2}{{1 - {\phi ^2}}}.}_{\begin{subarray}{l} 
         {\text{Geom. Serie}} 
         \end{subarray}}
     \end{aligned} \] Once again, the above result only holds because we are able to use the geometric series convergence as a result of $\left| \phi  \right| < 1$.
  3. Finally, we consider the autocovariance of an AR(1). For $h > 0$, we have
  \[\gamma \left( h \right) =  \text{cov}\left( {{X_t},{X_{t + h}}} \right) = \phi \text{cov}\left( {{X_t},{X_{t + h - 1}}} \right) = \phi \, \gamma \left( h-1 \right).\]
  Therefore, using the symmetry of autocovariance, we find that
  \[\gamma \left( h \right) = \phi^{|h|} \, \gamma(0).\]
  
  Both the mean and variance do not depend on time. In addition, the autocovariance function can be viewed as a function dependent entirely on lags and, thus, the AR(1) process is weakly stationary if $\left| \phi  \right| < 1$.  Lastly, we can obtain the autocorrelation for this process. Indeed, for $h > 0$, we have
  
  \[\rho \left( h \right) = \frac{{\gamma \left( h \right)}}{{\gamma \left( 0 \right)}} = \frac{{\phi \gamma \left( {h - 1} \right)}}{{\gamma \left( 0 \right)}} = \phi \rho \left( {h - 1} \right).\]
  
  After fully simplifying, we obtain
  
  \[\rho \left( h \right) = {\phi^{|h|}}.\]
  
  Thus, the autocorrelation function for an AR(1) exhibits a _geometric decay_,
  meaning, the smaller the $|\phi|$, the faster the autocorrelation reaches zero. 
  If $|\phi|$ is close to 1, then the decay rate is slower.
```

## Joint Stationarity

The notion of joint stationarity implies that the time series under
investigation is bivariate in nature, e.g. $\left(X_t \right)$ and 
$\left(Y_t\right)$ are two distinct time series with matching time points.
In order to fulfill bivariate stationarity, both processes
must be considered *weakly stationary* (with constant mean and autocovariance
dependent on lag $h$) and there must exist a cross-covariance function between them, 
depenent only on lag $h$.
The ideas discussed next can extended beyond the bivariate case to a general 
multivariate setting.

```{definition, label="crosscov"}
The **cross-covariance** function of two jointly stationary processes
$\left\{X_t\right\}$ and $\left\{Y_t\right\}$ is given by:
  
  \[{\gamma _{XY}}\left( {t + h,t} \right) = \text{cov}\left( {{X_{t+h}},{Y_{t}}} \right) =
      \e{\left( {{X_{t + h}} - {\mu _X}} \right)\left( {{Y_t} - {\mu _Y}} \right)} = {\gamma _{XY}}\left( h \right)\]
```

Unlike the symmetry found in the autocovariance function of a stationary
process $\left\{X_t\right\}$, e.g. $\gamma_X(h) = \gamma_X(-h)$, the cross-covariance
function is only equal when $\gamma_{XY}(h) = \gamma_{YX}(-h)$. Notice the
switch in indices and negative lag change.

```{definition, label="crosscorr"}
The *cross-correlation* function for two jointly stationary time series
$\left\{X_t\right\}$ and $\left\{Y_t\right\}$ can be expressed as:
  
  \[{\rho _{XY}}\left( {t+h,t} \right) = \frac{{{\gamma _{XY}}\left( {t + h,t} \right)}}{{\sqrt {{\gamma _X}\left( 0 \right)} \sqrt {{\gamma _Y}\left( 0 \right)} }} = \frac{{{\gamma _{XY}}\left( h \right)}}{{{\sigma _{{X_{t+h}}}}{\sigma _{{Y_{t}}}}}} = {\rho _{XY}}\left( h \right)\]
```

Due to the previously discussed symmetry regarding the cross-covariance function,
note that the cross-correlation function also only has equality when 
$\rho_{XY}(h) = \rho_{YX}(-h)$. Thus, note that $\rho_{XY}(h) \neq \rho_{XY}(-h)$.

```{example, label="ccfjs"}
Consider two time series processes given by:
  
  \[\begin{aligned}
     {X_t} &= {W_t} - {W_{t-1}} \\
     {Y_t} &= {W_t} + {W_{t-1}} \\ 
     \end{aligned} \]

where $W_t \sim WN(0, \sigma^2_W)$.

First check to see if $\left\{X_t\right\}$ and $\left\{Y_t\right\}$ are weakly stationary.

The means of both time series are evident to be
\[\begin{aligned}
   E\left[ {{X_t}} \right] &= E\left[ {{Y_t}} \right] = 0 \\
   \end{aligned} \]

The autocovariance for $\left\{X_t\right\}$ is given as:
  
  \[\begin{aligned}
     {\gamma _X}\left( h \right) &= \text{cov} \left( {{X_{t + h}},{X_t}} \right) \\
     &= \text{cov} \left( {{W_{t + h}} - {W_{t + h - 1}},{W_t} - {W_{t - 1}}} \right) \\
     &= {1_{\left\{ {h = 0} \right\}}}2{\sigma ^2} + {1_{\left\{ {h =  - 1} \right\}}}{-\sigma ^2} + {1_{\left\{ {h = 1} \right\}}}{-\sigma ^2} \\
     &= \begin{cases}
     2\sigma^2_W, &\text{if} h = 0 \\
     -\sigma^2_W, &\text{if} \left|h\right| = 1 \\
     0, &\text{if} \left|h\right| \ge 1 
     \end{cases} 
     \end{aligned} \]

Similarly, the autocovariance for $\left\{Y_t\right\}$ is calculated to be:
  
  \[\begin{aligned}
     {\gamma _Y}\left( h \right) &= \text{cov} \left( {{Y_{t + h}},{Y_t}} \right) \\
     &= \text{cov} \left( {{W_{t + h}} + {W_{t + h - 1}},{W_t} + {W_{t - 1}}} \right) \\
     &= {1_{\left\{ {h = 0} \right\}}}2{\sigma ^2} + {1_{\left\{ {h =  - 1} \right\}}}{\sigma ^2} + {1_{\left\{ {h = 1} \right\}}}{\sigma ^2} \\ 
     &= \begin{cases}
     2\sigma^2_W, &\text{if} h = 0 \\
     \sigma^2_W, &\text{if} \left|h\right| = 1 \\
     0, &\text{if} \left|h\right| \ge 1 
     \end{cases} 
     \end{aligned} \]

Next, the cross-covariance for $\left\{X_t\right\}$ and $\left\{Y_t\right\}$:
  
  \[\begin{aligned}
     {\gamma _{XY}}\left( h \right) &= \text{cov} \left( {{X_{t + h}},{Y_t}} \right) \\
     &= \text{cov} \left( {{W_{t + h}} - {W_{t + h - 1}},{W_t} + {W_{t-1}}} \right)  \\ 
     &= \begin{cases}
     0, &\text{if } h = 0 \\
     -\sigma^2_W, &\text{if } h = 1 \\
     \sigma^2_W, &\text{if } h = -1 \\
     0, &\text{if } h \ge 2
     \end{cases}
     \end{aligned}\]

Therefore, based on obtain the weak stationarity for each process and obtain a
cross-covariance function that only depends on the lag $h$, the process is
joint stationary. 

Furthermore, we also have the cross-correlation function as:
  
  \[{\rho _{XY}}\left( {{X_{t + h}},{Y_t}} \right) = \frac{{{\gamma _{XY}}\left( h \right)}}{{{\sigma _X}{\sigma _Y}}} = \frac{{{\gamma _{XY}}\left( h \right)}}{{\sqrt {{\gamma _x}\left( 0 \right)} \sqrt {{\gamma _y}\left( 0 \right)} }} = \frac{{{\gamma _{XY}}\left( h \right)}}{{{\gamma _X}\left( 0 \right)}} = \begin{cases}
    0, &\text{if } h = 0 \\
    -\frac{1}{2}, &\text{if } h = 1 \\
    \frac{1}{2}, &\text{if } h = -1 \\
    0, &\text{if } h \ge 2
    \end{cases}\]
```