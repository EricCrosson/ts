--- 
title: "Applied Time Series Analysis with R"
author: "Stéphane Guerrier, Roberto Molinari and Haotian Xu"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output:
  bookdown::gitbook:
    lib_dir: assets
    split_by: section
    config:
      toolbar:
        position: static
  bookdown::pdf_book:
    keep_tex: yes
  bookdown::html_book:
    css: toc.css
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: "acm"
link-citations: yes
github-repo: SMAC-Group/app_ts
description: ""
favicon: "favicon.ico"
---

# Preface

Welcome to Applied Time Series Analysis with `R` 


## About This Book

This book is intended as a support for the course of 463 (Applied Time Series). It contains an overview of the basic procedures to adequately approach a time series analysis with insight to more advanced analysis of time series. It firstly introduces the basic concepts and theory to appropriately use the applied tools that are presented in the second (and main) part of the book. In the latter part the reader will learn how to use descriptive analysis to identify the important characteristics of a time serues and then employ modelling and inference techniques (made available through R funtions) that allow to describe a time series and make predictions. The last part of the book will give introductory notions on more advanced analysis of time series where the reader will achieve a basic understanding of the tools available to anayse more complex characteristic of time series. 

This document is under active development and as a result is likely to contains
many errors. As Montesquieu puts it:

>
> "*La nature semblait avoir sagement pourvu à ce que les sottises des hommes 
> fussent passagères, et les livres les immortalisent.*"
>

## Contents

This book is structured as follows:

- Basic Elements of Time Series
    - Wold representation deterministic + random
    - Examples of deterministic components (trend + seasonality)
    - Random components: basic time series models
- Stationarity of Time Series
    - Stationarity vs Non-Stationarity
    - Linear operators and processes
    - Weak and Strong Stationarity
- Fundamental Representations
    - Conditions for fundamental representations (e.g. gaussian)
    - AutoCovariance and AutoCorrelation Functions
    - Spectral Density
    - Estimators: Empirical ACF
- SARIMA Models
    - AR(p) Models
    - MA(q) Models
    - ARMA(p,q) Models
    - ARIMA(p,d,q) Models
    - SARIMA(p,d,q)(P,D,Q) Models
- Descriptive Analysis
    - Raw Data
    - ACF plots
    - Identifying models
    - Other representations: SDF and WV
- Inference
    - Estimation
    - Inference
    - Model Selection
- Advanced Topics
    - GARCH
    - State-Space Models
    - Multivariate (VAR) Models

## Bibliographic Note 

This text is heavily inspired by the following three execellent references:

1. "*Time Series Analysis and Its Applications*", Fourth Edition, Robert H. Shumway & David S. Stoffer.
2. "*Time Series for Macroeconomics and Finance*", John H. Cochrane.
3. "*Cours de Séries Temporelles: Théorie et Applications*", Volume 1, Arthur Charpentier.


## Acknowledgements

The text has been developed in the open and has benefited greatly from many
people being able to alert the authors to problematic areas. We are greatful
for the corrections, suggestions, or requests ofclarity from the following:

- [Ziying Wang](https://github.com/zionward)
- [Haoxian Zhong](https://github.com/Lyle-Haoxian)
- [Zhihan Xiong](https://www.linkedin.com/in/zhihan-xiong-988152114)
- [Nathanael Claussen](https://github.com/Nathanael-Claussen)
- [Justin Lee](https:://github.com/munsheet)
- [James Balamuta](https::/github.com/coatless)


## License 

![This work is licensed under a [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License](http://creativecommons.org/licenses/by-nc-sa/4.0/).](images/license/cc.png)

This book is aimed at providing the procedures (and relative statistical theory) to allow the reader to appropriately analyse a time series 


<!--chapter:end:index.Rmd-->

```{r, echo=FALSE, include=FALSE}
library(astsa)
library(mgcv)
library(simts)
library(imudata)
```

# Basic Elements of Time Series 

We can start the discussion on the basic elements of time series by using a practical example from real data made available through the R software. The data represent the global mean land–ocean temperature shifts from 1880 to 2015 (with base index being the average temperatures from 1951 to 1980) and this time series is represented in the plot below.

```{r, echo=FALSE}
plot(globtemp, type="l", ylab="Global Temperature Deviations")
```

These data have been used as a support in favour of the argument that the global temperatures are increasing and that global warming has occured over the last half of the twentieth century. The first approach that one would take is to try and measure the average increase by fitting a (linear) model and testing if the (positive) slope is significant. In order to do so, we would require the residuals from the fitted model to independently and identically distributed (iid). Let us fit a model with the years (time) as explanatory variable and check the residuals from this fit.

```{r, echo=FALSE}
time <- 1:length(globtemp)
fit <- gam(globtemp ~ s(time))
resids <- resid(fit)

layout(matrix(c(1, 2, 3, 4), 2, 2, byrow = T))
plot(resids, type="l", ylab="Global Temperature Residuals")
plot(resids, ylab="Global Temperature Residuals")
hist(resids)
qqnorm(resids)
qqline(resids)
```

It can be seen from the upper left plot that the trend appears to be removed and, if looking at the residuals as one would usually do in a (linear) regression framework, the residual plots seem to suggest that the modelling has done a relatively good job since no particular pattern seems to emerge and their distribution is quite close to being Gaussian.

However, is it possible to conclude from the plots that the data are *iid*? More specifically, can we assume that the residuals are independent? This is a fundamental question in order for inference procedures to be carried out in an appropriate manner and to limit false conclusions. Let us provide an example through a simulated data set where we know that there is an upward trend through time (i.e. the slope $\beta = 0.01$) and our goal would be to show that this trend exists. Considering this, we simulate two cases where, in the first, the residuals are actually Gaussian iid while, in the second, the residuals are Gaussian but are dependent over time. The first case is shown below.

```{r}
set.seed(9)

# Simulate time series with iid residuals
y.ind <- cumsum(rep(0.01, 100)) + rnorm(100)

# Simulate time series with dependent residuals
y.dep <- cumsum(rep(0.01, 100)) + arima.sim(n = 100, list(ar = c(0.8897, -0.4858)))

# Define explanatory variable (time)
time <- 1:100

# Fit a linear model to estimate the slope (for the iid setting)
fit.ind <- lm(y.ind ~ time)
summary(fit.ind)

# Fit a linear model to estimate the slope (for the dependent setting)
fit.dep <- lm(y.dep ~ time)
summary(fit.dep)
```

As can be seen, the estimated slope ($\approx$ `r round(fit.ind$coefficients[2], 3)`) is close to the true slope (0.01) and is significant (i.e. the p-value is smaller than the common rejection level 0.05). Hence, from this inference procedure we can conclude that the slope is significant and is roughly equal to 0.01 (which corresponds to the truth). However, let us explore the same analysis when the residuals are not independent.

```{r}
set.seed(9)

# Simulate time series with dependent residuals
y.dep <- cumsum(rep(0.01, 100)) + arima.sim(n = 100, list(ar = c(0.8897, -0.4858)))

# Define explanatory variable (time)
time <- 1:100

# Fit a linear model to estimate the slope (for the dependent setting)
fit.dep <- lm(y.dep ~ time)
summary(fit.dep)
```

In this case we can observe that the p-value is greater than 0.05 and consequently the slope does not appear to be significant (although it is in reality). Therefore, the inference procedures can be misleading when not taking into account other possible significant variables or, in this case, forms of dependence that can hide true underlying effects.

The above examples therefore highlight how the approach to analysing time series does not only rely on finding an appropriate model that describes the evolution of a variable as a function of time (which is deterministic). Indeed, the main focus of time series analysis consists in modelling the dependence structure that describes how random variables impact each other as a function of time. In other words, a time series is a collection of random variables whose interaction and dependence structure is indexed by time.

## The Wold Decomposition

The previous discussion highlighted how a time series can be decomposed into a deterministic component and a random component. Leaving aside technical rigour, this characteristic of time series was put forward in Wold's Decomposition Theorem who postulated that a time series $(Y_t)$ (where $t = 1,...,n$ represents the time index) can be very generically represented as follows:

$$Y_t = D_t + W_t,$$

where $D_t$ represents the deterministic part (or *signal*) that can be modelled through the standard modelling techniques (e.g. linear regression) and $W_t$ that, restricting ourselves to a general class of processes, represents the random part (*noise*) that requires the analytical and modelling approaches that will be tackled in this book.
 
Typically, we have $\mathbb{E}[Y_t] \neq 0$ while $\mathbb{E}[W_t] = 0$ (although we may have
$\mathbb{E}[W_t | W_{t-1}, ..., W_1] \neq 0$). Such models impose some parametric
structure which represents a convenient and flexible way of studying time series
as well as a means to evaluate *future* values of the series through forecasting.
As we will see, predicting future values is one of the main aspects of time
series analysis. However, making predictions is often a daunting task or as
famously stated by Nils Bohr:

> 
> "*Prediction is very difficult, especially about the future.*"
>

There are plenty of examples of predictions that turned out to be completely
erroneous. For example, three days before the 1929 crash, Irving Fisher,
Professor of Economics at Yale University, famously predicted:

>
> "*Stock prices have reached what looks like a permanently high plateau*". 
>

Another example is given by Thomas Watson, president of IBM, who said in 1943:

>
> "*I think there is a world market for maybe five computers.*"
>

Let us now briefly discuss the two components of a time series.

### The deterministic component (Signal)

Before shifting our focus to the random component of time series, we will first just briefly underline the main features that should be taken into account for the deterministic component. The first feature that should be analysed is the *trend* that characterises the time series, more specifically the behaviour of the variable of interest as a specific function of time (as the global temperature time series seen earlier). Let us consider another example of time series based on real data, i.e. the quarterly earnings of Johnson & Johnson between 1960 and 1980 represented below.

```{r, echo=FALSE}
plot(jj, type="l", ylab="Quarterly Earnings per Share")
```

As can be seen from the plot, the earnings appear to grow over time, therefore we can imagine fitting a line to this data to describe its behaviour (see red line below).

```{r, echo=FALSE}
time_jj <- 1:84
quarters <- as.factor(rep(1:4, 21))
fit_jj1 <- lm(as.vector(jj) ~ time_jj)
plot(time_jj, as.vector(jj), type="l", xlab="Quarters", ylab="Quarterly Earnings per Share")
abline(fit_jj1, col = "red")
```

Although the line captures a part of the behaviour, it is quite clear that the trend of the time series is not linear. It could therefore be more appropriate to define another function of time to describe it and, consequently, we add a quadratic term of time to obtain the following fit.

```{r, echo=FALSE}
fit_jj2 <- gam(as.vector(jj) ~ s(time_jj))
plot(time_jj, as.vector(jj), type="l", xlab="Quarters", ylab="Quarterly Earnings per Share")
lines(time_jj, fit_jj2$fitted.values, col = "red")
```

We can see now that the quadratic function of time allows to better fit the observed time series and closely follow the observations. However, there still appears to be a pattern in the data that isn't captured by this quadratic model. This pattern appears to be repeated over time: peaks and valleys that seem to occur at regular intervals along the time series. This behaviour is known as *seasonality* which, in this case, can be explained by the effect of a specific quarter on the behaviour of the earnings. Indeed, it is reasonable to assume that the seasons have impacts on different variables measured over time (e.g. temperatures, earnings linked to sales that vary with seasons, etc.). Let us therefore take the quarters as an explanatory variable and add it to the previous quadratic model (see fit below).

```{r, echo=FALSE}
fit_jj3 <- gam(as.vector(jj) ~ quarters + s(time_jj))
plot(time_jj, as.vector(jj), type="l", xlab="Quarters", ylab="Quarterly Earnings per Share")
lines(time_jj, fit_jj3$fitted.values, col = "red")
```

This final fit appears to well describe the behaviour of the earnings. Hence, *trend* and *seasonality* are the main features that characterize the deterministic component of a time series. However, as discussed earlier, these deterministic components often don't explain all of the observed time series since there is often a random component characterizing data measured over time. Not considering the latter component can have considerable impacts on the inference procedures (as seen earlier) and it is therefore important to adequately analyse them (see next section).

### The random component (Noise)

From this section onwards we will refer to *time series as being solely the random noise component*. Keeping this in mind, a *time series* is a particular kind of *stochastic process* which, generally speaking, is a collection of random variables indexed by a set of numbers. Not surprisingly, the index of reference for a time series is given by *time* and, consequently, a time series is a collection of random variables indexed (or "measured") over time such as, for example, the daily price of a financial asset or the monthly average temperature in a given location. In terms of notation, a time series is often represented as

 \[\left(X_1, X_2, ..., X_n \right) \;\;\; \text{ or } \;\;\; \left(X_t\right)_{t = 1,...,n}.\]
 
The time index $t$ is contained within either the set of reals, $\mathbb{R}$, or
integers, $\mathbb{N}$. When $t \in \mathbb{R}$, the time series becomes a
*continuous-time* stochastic process such a Brownian motion, a model used to
represent the random movement of particles within a suspended liquid or gas. However, within this book, we will limit ourselves to the cases where $t \in \mathbb{N}$, better known as *discrete-time* processes. *Discrete-time* processes are measured sequentially at fixed
and equally spaced intervals in time. This implies that we will uphold two general assumptions for the time series considered in this book:

1. $t$ is not random, e.g. the time at which each observation is measured is known, and
2. the time between two consecutive observations is constant. 

This book will also focus on certain representations of time series based on parametric probabilistic models. For example, one of the fundamental probability models used in time series analysis is called the *white noise* model and is defined as

\[X_t \mathop \sim \limits^{iid} N(0, \sigma^2).\]

This statement simply means that $(X_t)$ is normally distributed and independent over time. Ideally, this is the type of process that we would want to observe once we have performed a statistical modelling procedure. However, despite it appearing to be an excessively simple model to be considered for time series, it is actually a crucial component to construct a wide range of more complex time series models (see Chapter 2). Indeed, unlike the white noise process, time series are typically *not* independent over time. For example, if we suppose that the temperature in State College is unusually low on a given dat, then it is reasonable to assume that temperature the day after will also be low.


## Basic Time Series Models {#basicmodels}

In this section, we introduce some simple time series models. Before doing so
it is useful to define $\Omega_t$ as all the information avaiable up to time
$t-1$, i.e.

\[\Omega_t = \left(X_{t-1}, X_{t-2}, ..., X_0 \right).\]

As we will see this compact notation is quite useful.

### White noise processes {#wn}

The building block for most time series models is the Gaussian white noise
process, which can be defined as

\[{W_t}\mathop \sim \limits^{iid} N\left( {0,\sigma _w^2} \right).\]

This definition implies that:

1. $\mathbb{E}[W_t | \Omega_t] = 0$ for all $t$,
2. $\text{cov}\left(W_t, W_{t-h} \right) = \boldsymbol{1}_{h = 0} \; \sigma^2$ for
all $t, h$.

Therefore, in this process there is an absence of temporal (or serial) 
dependence and is homoskedastic (i.e. it has a constant variance). 
White noise can be generalized into two sorts of processes: *weak* and *strong*. The process $(W_t)$ is
a weak white noise if

1. $\mathbb{E}[W_t] = 0$ for all $t$,
2. $\text{var}\left(W_t\right) = \sigma^2$ for all $t$,
3. $\text{cov} \left(W_t, W_{t-h}\right) = 0$, for all $t$, and for all $h \neq 0$.

Note that this definition does not imply that $W_t$ and $W_{t-h}$ are
independent (for $h \neq 0$) but simply uncorrelated.
However, the notion of independence is used to define a *strong* white noise as

1. $\mathbb{E}[W_t] = 0$ and $\text{var}(W_t) = \sigma^2 < \infty$, for all $t$,
2. $F(W_t) = F(W_{t-h})$, for all $t,h$ (where $F(W_t)$ denotes the distribution of $W_t$),
3. $W_t$ and $W_{t-h}$ are independent for all $t$ and for all $h \neq 0$.

It is clear from these definitions that if a process is a strong white noise
it is also a weak white noise. However, the converse is not true as shown in
the following example:

```{example, label="weaknotstrong"}

Let $Y_t \mathop \sim F_{t+2}$, where $F_{t+2}$ denotes
a Student distribution with $t+2$ degrees of freedom. Assuming the 
sequence $(Y_1, \ldots, Y_n)$ to be independent, we 
let $X_t = \sqrt{\frac{t}{t+2}} Y_t$. Then, the process $(X_t)$ is obviously
not a strong white noise as the distribution of $X_t$ changes with $t$. However,
this process is a weak white noise since we have:

- $\mathbb{E}[X_t] = \sqrt{\frac{t}{t+2}} \mathbb{E}[Y_t] = 0$ for all $t$.
- $\text{var}(X_t) = \frac{t}{t+2} \text{var}(Y_t) = \frac{t}{t+2} \frac{t+2}{t} = 1$ for all $t$.
- $\text{cov}(X_t, X_{t+h}) = 0$ (by independence), for all $t$, and for all $h \neq 0$.

```

The code below presents an example of how to simulate a Gaussian white noise process.

```{r example_WN, fig.height = 4, fig.width = 7, cache = TRUE}
n = 1000                               # process length
sigma2 = 1                             # process variance
Xt = gen_gts(n, WN(sigma2 = sigma2))
plot(Xt)
```

### Random Walk Processes {#rw}

The term *random walk* was first introduced by Karl Pearson in the early
nineteen-hundreds. Regarding white noise, there exist a large range of random walk
processes. For example, one of the simplest forms of a random walk process can be
explained as follows: suppose that you are walking on campus and your
next step can either be to your left, your right, forward or backward
(each with equal probability). Two realizations of such processes are
represented below:

```{r RW2d, fig.height = 5, fig.width = 5.5, cache = TRUE, fig.align='center'}
set.seed(5)
RW2dimension(steps = 10^2)
RW2dimension(steps = 10^4)
```

Such processes inspired Karl Pearson's famous quote that

>
> "*the most likely place to find a drunken walker is somewhere near his starting point.*"
> 

Empirical evidence of this phenomenon is not too hard to find on a Friday night. In this text, we only consider one very specific form of
random walk, namely the Gaussian random walk which can be defined as:

$$X_t = X_{t-1} + W_t,$$

<!--chapter:end:01-intro.Rmd-->


# Stochastic Properties of Time Series


<!--chapter:end:02-stationarity.Rmd-->

