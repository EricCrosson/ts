```{r, echo = FALSE}
library(simts)
library(rdatamarket)
```

# ARIMA Models

As we saw in the introduction to this book (and in various sections throughout it), a time series can be made of two components: a deterministic (non-stationary) component and a stochastic component. The latter component has been the main focus of this book where different classes of time series models have been studied assuming that this stochastic component respects certain properties (i.e. stationarity). For the former component (i.e. the deterministic component) we assume that we are able to explain non-stationary behaviours such as trends and seasonality via regression-type methods which include time-related covariates. 

However, there are non-stationary behaviours that can be addressed without the need for estimation procedures to make a time series stationary. In fact, estimating parameters to remove deterministic components in the data adds uncertainty when modelling the time-dependent stochastic component since the residuals from the previous fitting are only an approximation of the stochastic component and may be a biased representation of the time series if the model used to fit the deterministic component is misspecified. Considering the possible drawbacks when using regression techniques to explain the deterministic components, we have already seen a technique that can be used to remove non-stationary components without the need for regression and this was based on the use of the backshift operator $B$. The latter consists in a $d$-order differencing defined as:

$$\delta^d X_t = (1 - B)^d X_t.$$
We have already seen some examples where a first-order difference of a non-stationary time series can make the time series stationary. Indeed, a time series with a linear drift and/or a random walk can be made stationary by taking a first difference. 

```{example}
For example, consider the model

\[X_t = \Delta + X_{t-1} + W_t,\]
where $\Delta$ is a drift constant and $W_t \overset{iid}{\sim} WN(0, \sigma^2)$. A first difference of this process delivers:

\[\delta X_t = X_t - X_{t-1} = \Delta + W_t,\]

that is a stationary process with $E[\delta X_t] = \Delta$ and $Cov(\delta X_{t+h}, \delta X_t) = \sigma^2$ for $h = 0$ and zero otherwise. 
```

A first-order difference can therefore remove linear trends in a time series but, if the non-stationary component of a time series has other behaviours, higher order differences can allow to make the series stationary. 

```{example}
For example, take the following process:

$$X_t = \beta_0 + \beta_1 t + \beta_2 t^2 + Y_t,$$
where $(Y_t)$ is a stationary time series. If we took the first difference of this time series we would obtain

\begin{align*}
			\delta X_t &= X_t - X_{t-1}\\
			& = (\beta_0 + \beta_1 t + \beta_2 t^2 + Y_t) - (\beta_0 + \beta_1 (t-1) + \beta_2 (t-1)^2 + Y_{t-1}) \\
			& = \beta_1 + \beta_2 (2t - 1) + \delta Y_t .
\end{align*}

This time series is not stationary either since its expectation depends on time. However, let us take the second difference:

\begin{align*}
			\delta^2 X_t &= \delta X_t - \delta X_{t-1}\\
			& = (\beta_1 + \beta_2 (2t - 1) + \delta Y_t) - (\beta_1 + \beta_2 (2(t-1) - 1) + \delta Y_{t-1}) \\
			& =  \beta_2 2 + \delta^2 Y_t,
\end{align*}

which is now a stationary process with $E[\delta^2 X_t] = 2 \beta_2$ and covariance function of $\delta^2 Y_t$ which is a stationary process by definition.
```

Therefore, if the time-dependent expectation of a time series can be explained by a $d^{th}$-order polynomial (i.e. $\sum_{j=0}^d \beta_j t^j$), the $d^{th}$-order difference of this time series will be stationary. There are many other non-stationary time series that can be made stationary in this manner (e.g. stochastic trend models).

Based on the properties of differencing, we can define the class of ARIMA($p$,$d$,$q$) models as follows.

```{definition, name = "ARIMA(p,d,q) Models"}
A process $(X_t)$ follows an ARIMA(p,d,q) model if the process $(\delta^d X_t)$ follows an ARMA(p,q) model.
```

Based on this, the drift plus random walk described earlier would correspond to an ARIMA(0,1,0) since the first difference of the process delivers a white noise model with non-zero constant mean. To better illustrate the properties of these processes, let us consider the following ARIMA(2,1,1) model where $(X_t)$ is such that:

$$\delta X_t - 0.9 \delta X_{t-1} + 0.6 \delta X_{t-2} = 0.5 W_{t-1} + W_t.$$
Below is a simulated realization of the time series of length $T = 200$.

```{r}
set.seed(123)
Xt = gen_gts(n = 200, model = ARIMA(ar = c(0.9, -0.6), i = 1, ma = 0.3, sigma2 = 0.5))
plot(Xt)
```

From the plot it is quite clear that the time series may not be stationary. For this reason, let us take the first difference of the time series and check if this operation allows us to visually satisfy the stationarity assumptions.

```{r}
d_Xt = gts(diff(Xt))
plot(d_Xt)
```

The first difference of the time series now appears to be stationary and, for this reason, let us analyse the ACF and PACF plots of this new time series.

```{r}
corr_analysis(d_Xt)
```

Both the ACF and PACF plots appear to have a decaying pattern with no clear cut-off points. Therefore, since these plots don't perfectly fit either an AR($p$) model or MA($q$) model, we may consider an ARMA($p$,$q$) model for which the descriptive analysis provided by these plots is not necessarily helpful to understand the possible orders of the AR($p$) and MA($q$) components. For this reason, let us make use of the model selection criteria considering all models within an ARMA(3,3) for the process ($\delta X_t$).

```{r, eval=FALSE}
select(ARIMA(3, 1, 3), Xt)
```

```{r, echo=FALSE}
suppressWarnings(simts::select(ARIMA(3, 1, 3), Xt))
```

From the selection process we can see that all three criteria select the ARMA(2,1) model which is indeed the true model that generated the observed time series. As in the previous sections, let us now consider also an example from some real data. The considered time series represents monthly sales of shampoo from 1901-1903 (available using the `rdatamarket` package) whose plot is shown below.

```{r}
Xt = gts(as.numeric(dmseries("https://datamarket.com/data/set/22r0/sales-of-shampoo-over-a-three-year-period#!ds=22r0&display=line")), start = 1901, freq = 12, name_ts = "Sales (Units)", data_name = "Monthly Shampoo Sales", name_time = "")
plot(Xt)
```

The plot of the time series shows a clear updward trend in the time series which could eventually be fitted by a (linear) regression. However, as discussed earlier, the use of estimation may not deliver "accurate" residuals that well represent the stochastic time series we want to model. Therefore, let us check if a first-order difference is capable of making the time series stationary.

```{r}
d_Xt = gts(diff(Xt))

par(mfrow = c(2,1))
plot(d_Xt)
corr_analysis(d_Xt)
```

We can see how the first-differncing of the time series has allowed to make it apparently stationary so an ARIMA($p$,$d$,$q$) model with $d=1$ could be a good class of models to explain and predict the time series. The ACF and PACF plots would suggest a low order model since both the ACF and PACF become less significant starting from the first lags. For this reason, let us consider all candidate models included in an ARIMA(2,1,2) and evaluate them based on the model selection criteria.

```{r, eval=FALSE}
select(ARIMA(2, 1, 2), Xt)
```

```{r, echo=FALSE}
suppressWarnings(simts::select(ARIMA(2, 1, 2), Xt))
```

In this case, the most appropriate model would appear to be an ARIMA(0,1,2), meaning that an MA(2) model fitted to the first-difference of the original time series is the "best" to model the observed sales of shampoo.

# SARIMA Models `r emo::ji("warning")`

As seen in the previous section, ARMA models can be adapted in order to tackle different forms of non-stationary time series through the use of differencing, thereby delivering the class of ARIMA models. However, aside from non-stationarity, within different applied examples seen this far you may have noticed that many ACF (and PACF) plots denoted some forms of seasonality. Indeed, despite removing possible deterministic seasonal effects via regression techniques, it is often the case that some stochastic seasonal components appear in the (residual) time series. For this reason, the class of AR(I)MA models can be extended in order to take this seasonality into account thereby delivering the class of SARIMA models.

To introduce the more general class of SARIMA models, let us first consider the standard AR(1) model

$$X_t = \phi X_{t-1} + W_t,$$
which explains the behaviour in time via the previous observation. Based on this model, for all time points the dependence on the past is explained by the previous observation at time $t-1$. However, in the case of seasonal effects, this dependence on the past may be explained (also) by other lags $t-s$, where $s > 1$. For example, the following model

$$X_t = \phi X_{t-12} + W_t,$$
is a first-order seasonal AR model with $s = 12$ which is equivalent to an AR(12) model with $\phi_i = 0$ for $i = 1,...,11,$ and $\phi_{12} \neq 0$. However, using an AR(12) model would be theoretically incorrect for this case (since there is no direct dependence on intermediate observations) and would be statistically inefficient to estimate and, consequently, the definition of a seasonal component is more appropriate. In fact, the definition of $s = 12$ for the above example is typically the case of time series for economic or ecological phenomena where there can be a dependence between the same months.

Having justified the need for the extension to SARIMA models, let us more formally define this class of models starting from the example provided above.

```{definition, name = "Seasonal Autoregressive Model of Order 1"}
A sesaonal autoregressive model of order 1 is defined as follows:

$$X_t = \Phi X_{t-s} + W_{t} \Leftrightarrow (1 - \Phi B^{s}) X_t = W_t$$

The model would have the following properties:

1. $$\mathbb{E}[{X_t}] = 0$$
2. $$\gamma(0) = \text{Var}\left[X_t\right] = \frac{\sigma^2}{1-\Phi^2}$$
3. $$\rho(h) = \begin{cases}
1 &\text{ if } h = 0\\
\Phi^{\left|h\right|} &\text{ if } h = \pm \, s \cdot k, \, k = 1, 2, \cdots\\
0 &\text{ otherwise. }
\end{cases}$$

As can be seen, these properties differ from those of a standard AR(1) model only with respect to the dependence lag $s$ (where intermediate observations between $t$ and $t-s$ have no impact on $X_t$).
```

One aspect to underline at this point is the different notation used to explain seasonal dependence. Indeed, for standard AR($p$) models the notation for autoregressive parameters in this book is $\phi_i$ while for the seasonal dependence we have $\Phi_i$. This is because a model can contain both standard AR($p$) components (i.e. direct dependence on observations between $t$ and $t-p$) as well as seasonal components where direct dependence can occur with observations at lags $s > p$ (with no direct dependence on observations between $t-p$ and $t-s$).

Keeping in mind the need for this additional notation, we can now define the first-order seasonal moving average process.

```{definition, name = "Seasonal Moving Average of Order 1"}
A seasonal moving average model of order 1 is defined as follows:

$$X_t = W_t + \Theta W_{t-s} \Leftrightarrow X_t = (1 - \Theta B^{s}) W_t$$

$$\gamma(h) = \begin{cases}
\left({1+\Theta^2}\right)\sigma^2 &\text{ if } h = 0 \\
\Theta \sigma^2 &\text{ if } h = \pm \, s \cdot k, \, k = 1, 2, \cdots \\
0 &\text{ otherwise .} \\
\end{cases}$$

```

Therefore, a seasonal moving average is also defined in the same manner as a standard moving average model where the direct dependence is not strictly on the immediately adjacent past observations. Following the same logic, we can therefore define the seasonal equivalent of the autoregressive and moving average operators. 

```{definition, name = "Seasonal Autoregressive Operator"}
Similarly, to the regular autoregressive operator, the seasonal autoregressive operator is defined as:

$$\Phi_P(B^S) = 1 - \Phi_1 B^S - \Phi_2B^{2S} - \cdots - \Phi_PB^{PS}$$
```

```{definition, name = "Seasonal Moving Average Operator"}
The seasonal moving average operator is defined as:

$$\Theta_Q(B^S) = 1 + \Theta_1 B^S + \Theta_2B^{2S} + \cdots + \Theta_QB^{QS}$$
```

In these cases, $P$ and $Q$ play the same role as the $p$ and $q$ orders for ARMA($p$,$q$) models thereby determining how far into the past the direct seasonal dependence is present. Given this notation, we can define a pure seasonal ARMA model.

```{definition, name = "Pure Seasonal ARMA Model"}
A pure Seasonal ARMA model is defined as:
  
\[\Phi_P(B^S) X_t = \Theta_Q(B^S) W_t\]
```

The above model therefore considers all direct seasonal dependencies without any components of standard ARMA models which can nevertheless be included to deliver a general SARMA model. Indeed, the more general SARMA model is defined as follows.

```{definition, name = "Seasonal ARMA Model"}
Seasonal Autoregressive Moving Average models are often referred to with the notation $ARMA(p, q)\times(P, Q)_{S}$ which indicates the following model:

$$\Phi_p \left({B^S}\right) \phi\left(B\right) X_t = \Theta_Q \left({ B^S }\right) \theta \left({ B }\right) W_t$$
```

As you can notice, the standard autoregressive and moving average operators have now been added to the previous definition of pure seasonal ARMA models. Hence, as mentioned above, these models allow for different forms of direct dependence between observations and innovation (white) noise. Let us consider an example to understand the structure and properties of these models.

```{example, name = "Mixed Seasonal Model"}
Consider the following time series model that contains both a seasonal term and
a standard ARMA component:

$$X_t = \Phi X_{t-12}  + \theta W_{t-1} + W_t, \,\,\, \left| \Phi \right| < 1, \,\, \left| \theta \right| < 1,$$

where $W_t \sim WN(0, \sigma^2)$. Based on the previously defined notation, we can refer to this model as an $ARMA(0,1)\times(1, 0)_{12}$ model. The properties of this model can be derived as follows:

$$\text{Var}\left( {{X_t}} \right) = {\Phi ^2} \text{Var} \left( {{X_{t - 12}}} \right) + {\sigma ^2} + {\theta ^2}{\sigma ^2} $$
  
Based on the parameter definitions, we know that this process is stationary (causal and invertible) and therefore $\text{Var} \left( {{X_t}} \right) = \text{Var} \left( {{X_{t - 12}}} \right) = \gamma \left( 0 \right)$. Using this information we can derive the following:
  
\begin{align}
\gamma \left( 0 \right) &= \frac{{{\sigma ^2}\left( {1 + {\theta ^2}} \right)}}{{1 - {\Phi ^2}}} \\[0.5cm]
  \gamma \left( 1 \right) &= \text{cov}\left( {{X_t},{X_{t - 1}}} \right) = \text{cov}\left( {\Phi {X_{t - 12}} + {W_t} + \theta {W_{t - 1}},{X_{t - 1}}} \right) \notag \\[0.2cm]
   &= \Phi \text{cov}\left( {{X_{t - 12}},{X_{t - 1}}} \right) + \underbrace {\text{cov}\left( {{W_t},{X_{t - 1}}} \right)}_{ = 0} + \theta \text{cov}\left( {{W_{t - 1}},{X_{t - 1}}} \right) \notag  \\
   &= \Phi \gamma \left( {11} \right) + \theta {\sigma ^2}
\end{align}

Then, for $h > 1$ we have

\begin{align}
  \gamma \left( h \right) &= \text{cov}\left( {{X_t},{X_{t - h}}} \right) = \text{cov}\left( {\Phi {X_{t - 12}} + {W_t} + \theta {W_{t - 1}},{X_{t - h}}} \right) \notag \\[0.2cm]
  &= \Phi \text{cov}\left( {{X_{t - 12}},{X_{t - h}}} \right) \notag \\[0.2cm]
   &= \Phi \gamma \left( {h - 12} \right) \\
\end{align}

Based on the above properties, we can show that the autocovariance will not be equal to zero for lags $h$ that lie within the seasonal dependence structure. More specifically, let us use the last general definition of the autocovariance and, based on the symmetry of the autocovariance (i.e. $\gamma (h) = \gamma(-h)$), derive the explicit form for the autocovariance at lag $h = 1$:
\begin{equation}
 \gamma \left( 1 \right) = \Phi \gamma \left( {11} \right) + \theta {\sigma ^2} = {\Phi ^2}\gamma \left( 1 \right) + \theta {\sigma ^2}
\end{equation}

Based on the above equality we have:

$$\gamma \left( 1 \right) = \frac{{\theta {\sigma ^2}}}{{1 - {\Phi ^2}}}.$$
  
When the lag does not lie within the seasonal dependence structure the autocovariance will be zero. Consider, for example, the autocovariance at lag $h=2$:
\begin{align}
  \gamma \left( 2 \right) &= \text{cov} \left( {{X_t},{X_{t - 2}}} \right) = \operatorname{cov} \left( {\Phi {X_{t - 12}} + {W_t} + \theta {W_{t - 1}},{X_{t - 2}}} \right) \notag \\[0.2cm]
   &= \Phi \text{cov} \left( {{X_{t - 12}},{X_{t - 2}}} \right) = \Phi \gamma \left( {10} \right) = {\Phi ^2}\gamma \left( 2 \right).
\end{align}

Again, based on the above equality it can easily be seen that

$$\gamma \left( 2 \right) \left(1 -  {\Phi ^2}\right) = 0 \,\, \Rightarrow \,\, \gamma \left( 2 \right) = 0$$.

In this example, the same would hold for:

\begin{equation}
\gamma \left( 3 \right) = \gamma \left( 4 \right) = \cdots = \gamma \left( 10 \right) = 0,
\end{equation}

for all $h > 1$. Following these results, the general autocovariance of this model can be summarized as follows:
\begin{align*}
\gamma \left( {12h} \right) &= {\Phi ^h}\gamma \left( 0 \right) &h = 0, 1, 2, \ldots \\
\gamma \left( {12h + 1} \right) &= \gamma \left( {12h - 1} \right) = {\Phi ^h}\gamma \left( 1 \right) &h = 0, 1, 2, \ldots \\
\gamma \left( {h} \right) &= 0  &\text{otherwise}
\end{align*}

The autocorrelation function is easily derived from the above autocovariance structure and is given by:

\begin{align*}
  \rho \left( {12h} \right) &= {\Phi ^h} & h = 0, 1, 2, \ldots  \\
  \rho \left( {12h - 1} \right) &= \rho \left( {12h + 1} \right) = \frac{\theta }{{1 + {\theta ^2}}}{\Phi ^h} & h = 0, 1, 2, \ldots \\
  \rho \left( h \right) &= 0 & \text{otherwise}  \\
\end{align*}
```

Given these derivations, let us verify what the estimated ACF of the above model looks like and, for this reason, we simulate a time series of length $T = 10^4$ from an $ARMA(0,1)\times(1, 0)_{12}$ model with $\theta = -0.8$ and $\Phi = 0.95$.

```{r mixed_sarima, cache = TRUE}
model = SARIMA(ar=0, i=0,ma=-0.8, sar=0.95, si = 0 , sma = 0, s = 12)
Xt = gen_gts(10e4, model)
plot(auto_corr(Xt, lag.max = 40))
```

From the plot we can see how the autocorrelation is highly significant at each lag $12 \cdot h$ ($h=0,1,2,...$) as well as all adjacent lags $12 \cdot h \pm 1$. This corresponds to what is expected from the autocorrelation structure of the model whose autocovariance structure we derived above.

Having provided a brief overview of how to derive and study the autocovariance (and autocorrelation) structure of these models, let us now verify how we can classify a time series model as a Seasonal ARMA. To better explain this, consider the following example.

```{example, name = "Classifying a Seasonal ARMA"}
Returning to our previous example, we can see that the time series follows an $ARMA(0,1)\times(1,0)_{12}$ process.

\begin{align*}
  {X_t} &= \Phi {X_{t - 12}} + {W_t} + \theta {W_{t - 1}} \hfill \\
  \underbrace {\left( {{X_t} - \Phi {B^{12}}} \right)}_{{\Phi _1}\left( {{B^{12}}} \right)}\underbrace 1_{\phi \left( B \right)}{X_t} &= \underbrace 1_{{\theta _Q}\left( B \right)}\underbrace {\left( {1 - \theta B} \right)}_{\theta \left( B \right)}{W_t} \hfill \\
\end{align*}
```

```{definition, name = "Seasonal ARIMA Model Form"}
The form of a Seasonal Autoregressive Integrated Moving Average models is denoted as $ARIMA(p, d, q)\times(P, D, Q)_S$:

$$\Phi_p \left({B^S}\right) \phi\left(B\right) \nabla^D_S \nabla^d X_t = \delta + \Theta_Q \left({ B^S }\right) \theta \left({ B }\right) W_t$$

where $\nabla^d = \left({1-B}\right)^d$ and $\nabla^D_S = \left({1-B^S}\right)^D$.
```

```{example, name = "Classifying a SARIMA"}
Consider the following time series:
\begin{align}
  {X_t} &= {X_{t - 1}} + {X_{t - 12}} - {X_{t - 13}} + {W_t} + \phi {W_{t - 1}} + \theta {W_{t - 12}} + \theta \phi {W_{t - 13}} \hfill \\
  {X_t} - {X_{t - 1}} - {X_{t - 12}} + {X_{t - 13}} &= {W_t} + \phi {W_{t - 1}} + \theta {W_{t - 12}} + \theta \phi {W_{t - 13}} \hfill \\
  \left( {1 - B - {B^{12}} + {B^{13}}} \right){X_t} &= \left( {1 + \phi B + \theta {B^{12}} + \phi \theta {B^{13}}} \right){W_t} \hfill \\
  \underbrace {\left( {1 - {B^{12}}} \right)}_{{\nabla _{12}}}\underbrace {\left( {1 - B} \right)}_\nabla {X_t} &= \underbrace {\left( {1 + \theta {B^{12}}} \right)}_{{\theta _Q}\left( {{B^S}} \right)}\underbrace {\left( {1 + \phi B} \right)}_{\theta \left( B \right)}{W_t} \hfill \\
\end{align}

The end result indicates that the SARIMA is given by: $ARIMA(0,1,1)\times(0,1,1)_{12}$.
```

In practice, identifying the parametrization of a SARIMA model is problematic.
There is no easy way to find $p, d, q, P, D, Q, S$.


```{r, fig.asp = 0.6, fig.width = 7, fig.align='center', fig.cap="TO DO", cache = TRUE}
Xt = gts(as.numeric(dmseries("https://datamarket.com/data/set/22pw/monthly-lake-erie-levels-1921-1970#!ds=22pw&display=line")),
    start = 1921, freq = 12, name_ts = "Water Levels",
    data_name = "Monthly Lake Erie Levels", name_time = "")
plot(Xt)
```

```{r, fig.asp = 0.45, fig.width = 8, fig.align='center', fig.cap="Empirical ACF (left) and PACF (right) of the Lake Erie time series data.", cache = TRUE}
corr_analysis(Xt)
```


```{r, fig.asp = 0.6, fig.width = 7, fig.align='center', fig.cap="TO DO", cache = TRUE}
Xt = gts(as.numeric(dmseries("https://datamarket.com/data/set/235j/number-of-daily-births-in-quebec-jan-01-1977-to-dec-31-1990#!ds=235j&display=line")),
    start = 1977, freq = 365, name_ts = "Number of Births",
    data_name = "Number of Births in Quebec", name_time = "")
plot(Xt)
```

```{r, fig.asp = 0.45, fig.width = 8, fig.align='center', fig.cap="Empirical ACF (left) and PACF (right) of the Births time series data.", cache = TRUE}
corr_analysis(Xt, lag.max = 50)
```

```{r, cache = TRUE}
mod = estimate(SARIMA(ar = 2, ma = 1, sar = 2, sma = 1, s = 7, si = 1), Xt, method = "mle")
check(mod)
```

```{r, cache = TRUE}
predict(mod, n.ahead = 30)
```