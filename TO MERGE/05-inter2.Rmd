```{r, echo = FALSE}
library(simts)
library(rdatamarket)
```

# ARIMA Models `r emo::ji("warning")`

As we saw in the introduction to this book (and in various sections throughout it), a time series can be made of two components: a deterministic (non-stationary) component and a stochastic component. The latter component has been the main focus of this book where different classes of time series models have been studied assuming that this stochastic component respects certain properties (i.e. stationarity). For the former component (i.e. the deterministic component) we assume that we are able to explain non-stationary behaviours such as trends and seasonality via regression-type methods which include time-related covariates. 

However, there are non-stationary behaviours that can be addressed without the need for estimation procedures to make a time series stationary. In fact, estimating parameters to remove deterministic components in the data adds uncertainty when modelling the time-dependent stochastic component since the residuals from the previous fitting are only an approximation of the stochastic component and may be a biased representation of the time series if the model used to fit the deterministic component is misspecified. Considering the possible drawbacks when using regression techniques to explain the deterministic components, we have already seen a technique that can be used to remove non-stationary components without the need for regression and this was based on the use of the backshift operator $B$. The latter consists in a $d$-order differencing defined as:

$$\delta^d X_t = (1 - B)^d X_t.$$
We have already seen some examples where a first-order difference of a non-stationary time series can make the time series stationary. Indeed, a time series with a linear drift and/or a random walk can be made stationary by taking a first difference. 

```{example}
For example, consider the model

\[X_t = \Delta + X_{t-1} + W_t,\]
where $\Delta$ is a drift constant and $W_t \overset{iid}{\sim} WN(0, \sigma^2)$. A first difference of this process delivers:

\[\delta X_t = X_t - X_{t-1} = \Delta + W_t,\]

that is a stationary process with $E[\delta X_t] = \Delta$ and $Cov(\delta X_{t+h}, \delta X_t) = \sigma^2$ for $h = 0$ and zero otherwise. 
```

A first-order difference can therefore remove linear trends in a time series but, if the non-stationary component of a time series has other behaviours, higher order differences can allow to make the series stationary. 

```{example}
For example, take the following process:

$$X_t = \beta_0 + \beta_1 t + \beta_2 t^2 + Y_t,$$
where $(Y_t)$ is a stationary time series. If we took the first difference of this time series we would obtain

\begin{align*}
			\delta X_t &= X_t - X_{t-1}\\
			& = (\beta_0 + \beta_1 t + \beta_2 t^2 + Y_t) - (\beta_0 + \beta_1 (t-1) + \beta_2 (t-1)^2 + Y_{t-1}) \\
			& = \beta_1 + \beta_2 (2t - 1) + \delta Y_t .
\end{align*}

This time series is not stationary either since its expectation depends on time. However, let us take the second difference:

\begin{align*}
			\delta^2 X_t &= \delta X_t - \delta X_{t-1}\\
			& = (\beta_1 + \beta_2 (2t - 1) + \delta Y_t) - (\beta_1 + \beta_2 (2(t-1) - 1) + \delta Y_{t-1}) \\
			& =  \beta_2 2 + \delta^2 Y_t,
\end{align*}

which is now a stationary process with $E[\delta^2 X_t] = 2 \beta_2$ and covariance function of $\delta^2 Y_t$ which is a stationary process by definition.
```

Therefore, if the time-dependent expectation of a time series can be explained by a $d^{th}$-order polynomial (i.e. $\sum_{j=0}^d \beta_j t^j$), the $d^{th}$-order difference of this time series will be stationary. There are many other non-stationary time series that can be made stationary in this manner (e.g. stochastic trend models).

Based on the properties of differencing, we can define the class of ARIMA($p$,$d$,$q$) models as follows.

```{definition, name = "ARIMA(p,d,q) Models"}
A process $(X_t)$ follows an ARIMA(p,d,q) model if the process $(\delta^d X_t)$ follows an ARMA(p,q) model.
```

Based on this, the drift plus random walk described earlier would correspond to an ARIMA(0,1,0) since the first difference of the process delivers a white noise model with non-zero constant mean. To better illustrate the properties of these processes, let us consider the following ARIMA(2,1,1) model where $(X_t)$ is such that:

$$\delta X_t - 0.9 \delta X_{t-1} + 0.6 \delta X_{t-2} = 0.5 W_{t-1} + W_t.$$
Below is a simulated realization of the time series of length $T = 200$.

```{r}
set.seed(123)
Xt = gen_gts(n = 200, model = ARIMA(ar = c(0.9, -0.6), i = 1, ma = 0.3, sigma2 = 0.5))
plot(Xt)
```

From the plot it is quite clear that the time series may not be stationary. For this reason, let us take the first difference of the time series and check if this operation allows us to visually satisfy the stationarity assumptions.

```{r}
d_Xt = gts(diff(Xt))
plot(d_Xt)
```

The first difference of the time series now appears to be stationary and, for this reason, let us analyse the ACF and PACF plots of this new time series.

```{r}
corr_analysis(d_Xt)
```

Both the ACF and PACF plots appear to have a decaying pattern with no clear cut-off points. Therefore, since these plots don't perfectly fit either an AR($p$) model or MA($q$) model, we may consider an ARMA($p$,$q$) model for which the descriptive analysis provided by these plots is not necessarily helpful to understand the possible orders of the AR($p$) and MA($q$) components. For this reason, let us make use of the model selection criteria considering all models within an ARMA(3,3) for the process ($\delta X_t$).

```{r, eval=FALSE}
select(ARIMA(3, 1, 3), Xt)
```

```{r, echo=FALSE}
suppressWarnings(simts::select(ARIMA(3, 1, 3), Xt))
```

From the selection process we can see that all three criteria select the ARMA(2,1) model which is indeed the true model that generated the observed time series. As in the previous sections, let us now consider also an example from some real data. The considered time series represents monthly sales of shampoo from 1901-1903 (available using the `rdatamarket` package) whose plot is shown below.

```{r}
Xt = gts(as.numeric(dmseries("https://datamarket.com/data/set/22r0/sales-of-shampoo-over-a-three-year-period#!ds=22r0&display=line")), start = 1901, freq = 12, name_ts = "Sales (Units)", data_name = "Monthly Shampoo Sales", name_time = "")
plot(Xt)
```

The plot of the time series shows a clear updward trend in the time series which could eventually be fitted by a (linear) regression. However, as discussed earlier, the use of estimation may not deliver "accurate" residuals that well represent the stochastic time series we want to model. Therefore, let us check if a first-order difference is capable of making the time series stationary.

```{r}
d_Xt = gts(diff(Xt))

par(mfrow = c(2,1))
plot(d_Xt)
corr_analysis(d_Xt)
```

We can see how the first-differncing of the time series has allowed to make it apparently stationary so an ARIMA($p$,$d$,$q$) model with $d=1$ could be a good class of models to explain and predict the time series. The ACF and PACF plots would suggest a low order model since both the ACF and PACF become less significant starting from the first lags. For this reason, let us consider all candidate models included in an ARIMA(2,1,2) and evaluate them based on the model selection criteria.

```{r, eval=FALSE}
select(ARIMA(2, 1, 2), Xt)
```

```{r, echo=FALSE}
suppressWarnings(simts::select(ARIMA(2, 1, 2), Xt))
```

In this case, the most appropriate model would appear to be an ARIMA(0,1,2), meaning that an MA(2) model fitted to the first-difference of the original time series is the "best" to model the observed sales of shampoo.

# SARIMA Models `r emo::ji("warning")`

As seen in the previous section, ARMA models can be adapted in order to tackle different forms of non-stationary time series through the use of differencing, thereby delivering the class of ARIMA models. However, aside from non-stationarity, within different applied examples seen this far you may have noticed that many ACF (and PACF) plots denoted some forms of seasonality. Indeed, despite removing possible deterministic seasonal effects via regression techniques, it is often the case that some stochastic seasonal components appear in the (residual) time series. For this reason, the class of AR(I)MA models can be extended in order to take this seasonality into account thereby delivering the class of SARIMA models.

To introduce the more general class of SARIMA models, let us first consider the standard AR(1) model

$$X_t = \phi X_{t-1} + W_t,$$
which explains the behaviour in time via the previous observation. Based on this model, for all time points the dependence on the past is explained by the previous observation at time $t-1$. However, in the case of seasonal effects, this dependence on the past may be explained (also) by other lags $t-s$, where $s > 1$. For example, the following model

$$X_t = \phi X_{t-12} + W_t,$$
is a first-order seasonal AR model with $s = 12$ which is equivalent to an AR(12) model with $\phi_i = 0$ for $i = 1,...,11,$ and $\phi_{12} \neq 0$. However, using an AR(12) model would be theoretically incorrect for this case (since there is no direct dependence on intermediate observations) and would be statistically inefficient to estimate and, consequently, the definition of a seasonal component is more appropriate. In fact, the definition of $s = 12$ for the above example is typically the case of time series for economic or ecological phenomena where there can be a dependence between the same months.

Having justified the need for the extension to SARIMA models, let us more formally define this class of models starting from the example provided above.

```{definition, name = "Seasonal Autoregressive Model of Order 1"}
A sesaonal autoregressive model of order 1 is defined as follows:

$$X_t = \Phi X_{t-s} + W_{t} \Leftrightarrow (1 - \Phi B^{s}) X_t = W_t$$

The model would have the following properties:

1. $$\mathbb{E}[{X_t}] = 0$$
2. $$\gamma(0) = \text{Var}\left[X_t\right] = \frac{\sigma^2}{1-\Phi^2}$$
3. $$\rho(h) = \begin{cases}
1 &\text{ if } h = 0\\
\Phi^{\left|h\right|} &\text{ if } h = \pm \, s \cdot k, \, k = 1, 2, \cdots\\
0 &\text{ otherwise. }
\end{cases}$$

As can be seen, these properties differ from those of a standard AR(1) model only with respect to the dependence lag $s$ (where intermediate observations between $t$ and $t-s$ have no impact on $X_t$).
```

One aspect to underline at this point is the different notation used to explain seasonal dependence. Indeed, for standard AR($p$) models the notation for autoregressive parameters in this book is $\phi_i$ while for the seasonal dependence we have $\Phi_i$. This is because a model can contain both standard AR($p$) components (i.e. direct dependence on observations between $t$ and $t-p$) as well as seasonal components where direct dependence can occur with observations at lags $s > p$ (with no direct dependence on observations between $t-p$ and $t-s$).

Keeping in mind the need for this additional notation, we can now define the first-order seasonal moving average process.

```{definition, name = "Seasonal Moving Average of Order 1"}
A seasonal moving average model of order 1 is defined as follows:

$$X_t = W_t + \Theta W_{t-s} \Leftrightarrow X_t = (1 - \Theta B^{s}) W_t$$

$$\gamma(h) = \begin{cases}
\left({1+\Theta^2}\right)\sigma^2 &\text{ if } h = 0 \\
\Theta \sigma^2 &\text{ if } h = \pm \, s \cdot k, \, k = 1, 2, \cdots \\
0 &\text{ otherwise .} \\
\end{cases}$$

```

Therefore, a seasonal moving average is also defined in the same manner as a standard moving average model where the direct dependence is not strictly on the immediately adjacent past observations. Following the same logic, we can therefore define the seasonal equivalent of the autoregressive and moving average operators. 

```{definition, name = "Seasonal Autoregressive Operator"}
Similarly, to the regular autoregressive operator, the seasonal autoregressive operator is defined as:

$$\Phi_P(B^S) = 1 - \Phi_1 B^S - \Phi_2B^{2S} - \cdots - \Phi_PB^{PS}$$
```

```{definition, name = "Seasonal Moving Average Operator"}
The seasonal moving average operator is defined as:

$$\Theta_Q(B^S) = 1 + \Theta_1 B^S + \Theta_2B^{2S} + \cdots + \Theta_QB^{QS}$$
```

In these cases, $P$ and $Q$ play the same role as the $p$ and $q$ orders for ARMA($p$,$q$) models thereby determining how far into the past the direct seasonal dependence is present. Given this notation, we can define a pure seasonal ARMA model.

```{definition, name = "Pure Seasonal ARMA Model"}
A pure Seasonal ARMA model is defined as:
  
\[\Phi_P(B^S) X_t = \Theta_Q(B^S) W_t\]
```

The above model therefore considers all direct seasonal dependencies without any components of standard ARMA models which can nevertheless be included to deliver a general SARMA model. Indeed, the more general SARMA model is defined as follows.

```{definition, name = "Seasonal ARMA Model"}
Seasonal Autoregressive Moving Average models are often referred to with the notation $ARMA(p, q)\times(P, Q)_{S}$ which indicates the following model:

$$\Phi_p \left({B^S}\right) \phi\left(B\right) X_t = \Theta_Q \left({ B^S }\right) \theta \left({ B }\right) W_t$$
```

As you can notice, the standard autoregressive and moving average operators have now been added to the previous definition of pure seasonal ARMA models. Hence, as mentioned above, these models allow for different forms of direct dependence between observations and innovation (white) noise. Let us consider an example to understand the structure and properties of these models.

```{example, name = "Mixed Seasonal Model"}
Consider the following time series model that contains both a seasonal term and
a standard ARMA component:

$$X_t = \Phi X_{t-12}  + \theta W_{t-1} + W_t, \,\,\, \left| \Phi \right| < 1, \,\, \left| \theta \right| < 1,$$

where $W_t \sim WN(0, \sigma^2)$. Based on the previously defined notation, we can refer to this model as an $ARMA(0,1)\times(1, 0)_{12}$ model. The properties of this model can be derived as follows:

$$\text{Var}\left( {{X_t}} \right) = {\Phi ^2} \text{Var} \left( {{X_{t - 12}}} \right) + {\sigma ^2} + {\theta ^2}{\sigma ^2} $$
  
Based on the parameter definitions, we know that this process is stationary (causal and invertible) and therefore $\text{Var} \left( {{X_t}} \right) = \text{Var} \left( {{X_{t - 12}}} \right) = \gamma \left( 0 \right)$. Using this information we can derive the following:
  
\begin{align}
\gamma \left( 0 \right) &= \frac{{{\sigma ^2}\left( {1 + {\theta ^2}} \right)}}{{1 - {\Phi ^2}}} \\[0.5cm]
  \gamma \left( 1 \right) &= \text{cov}\left( {{X_t},{X_{t - 1}}} \right) = \text{cov}\left( {\Phi {X_{t - 12}} + {W_t} + \theta {W_{t - 1}},{X_{t - 1}}} \right) \notag \\[0.2cm]
   &= \Phi \text{cov}\left( {{X_{t - 12}},{X_{t - 1}}} \right) + \underbrace {\text{cov}\left( {{W_t},{X_{t - 1}}} \right)}_{ = 0} + \theta \text{cov}\left( {{W_{t - 1}},{X_{t - 1}}} \right) \notag  \\
   &= \Phi \gamma \left( {11} \right) + \theta {\sigma ^2}
\end{align}

Then, for $h > 1$ we have

\begin{align}
  \gamma \left( h \right) &= \text{cov}\left( {{X_t},{X_{t - h}}} \right) = \text{cov}\left( {\Phi {X_{t - 12}} + {W_t} + \theta {W_{t - 1}},{X_{t - h}}} \right) \notag \\[0.2cm]
  &= \Phi \text{cov}\left( {{X_{t - 12}},{X_{t - h}}} \right) \notag \\[0.2cm]
   &= \Phi \gamma \left( {h - 12} \right) \\
\end{align}

Based on the above properties, we can show that the autocovariance will not be equal to zero for lags $h$ that lie within the seasonal dependence structure. More specifically, let us use the last general definition of the autocovariance and, based on the symmetry of the autocovariance (i.e. $\gamma (h) = \gamma(-h)$), derive the explicit form for the autocovariance at lag $h = 1$:
\begin{equation}
 \gamma \left( 1 \right) = \Phi \gamma \left( {11} \right) + \theta {\sigma ^2} = {\Phi ^2}\gamma \left( 1 \right) + \theta {\sigma ^2}
\end{equation}

Based on the above equality we have:

$$\gamma \left( 1 \right) = \frac{{\theta {\sigma ^2}}}{{1 - {\Phi ^2}}}.$$
  
When the lag does not lie within the seasonal dependence structure the autocovariance will be zero. Consider, for example, the autocovariance at lag $h=2$:
\begin{align}
  \gamma \left( 2 \right) &= \text{cov} \left( {{X_t},{X_{t - 2}}} \right) = \operatorname{cov} \left( {\Phi {X_{t - 12}} + {W_t} + \theta {W_{t - 1}},{X_{t - 2}}} \right) \notag \\[0.2cm]
   &= \Phi \text{cov} \left( {{X_{t - 12}},{X_{t - 2}}} \right) = \Phi \gamma \left( {10} \right) = {\Phi ^2}\gamma \left( 2 \right).
\end{align}

Again, based on the above equality it can easily be seen that

$$\gamma \left( 2 \right) \left(1 -  {\Phi ^2}\right) = 0 \,\, \Rightarrow \,\, \gamma \left( 2 \right) = 0$$.

In this example, the same would hold for:

\begin{equation}
\gamma \left( 3 \right) = \gamma \left( 4 \right) = \cdots = \gamma \left( 10 \right) = 0.
\end{equation}

Following these results, the general autocovariance of this model can be summarized as follows:
\begin{align*}
\gamma \left( {12h} \right) &= {\Phi ^h}\gamma \left( 0 \right) &h = 0, 1, 2, \ldots \\[0.2cm]
\gamma \left( {12h + 1} \right) &= \gamma \left( {12h - 1} \right) = {\Phi ^h}\gamma \left( 1 \right) &h = 0, 1, 2, \ldots \\[0.2cm]
\gamma \left( {h} \right) &= 0  &\text{otherwise}
\end{align*}

The autocorrelation function is easily derived from the above autocovariance structure and is given by:

\begin{align*}
  \rho \left( {12h} \right) &= {\Phi ^h} & h = 0, 1, 2, \ldots  \\
  \rho \left( {12h - 1} \right) &= \rho \left( {12h + 1} \right) = \frac{\theta }{{1 + {\theta ^2}}}{\Phi ^h} & h = 0, 1, 2, \ldots \\
  \rho \left( h \right) &= 0 & \text{otherwise}  \\
\end{align*}
```

Given these derivations, let us verify what the estimated ACF of the above model looks like and, for this reason, we simulate a time series of length $T = 10^4$ from an $ARMA(0,1)\times(1, 0)_{12}$ model with $\theta = -0.8$ and $\Phi = 0.95$.

```{r mixed_sarima, cache = TRUE}
model = SARIMA(ar=0, i=0,ma=-0.8, sar=0.95, si = 0 , sma = 0, s = 12)
Xt = gen_gts(10e4, model)
plot(auto_corr(Xt, lag.max = 40))
```

From the plot we can see how the autocorrelation is highly significant at each lag $12 h$ as well as all adjacent lags $12 h \pm 1$. This corresponds to what is expected from the autocorrelation structure of the model whose autocovariance structure we derived above.

Having provided a brief overview of how to derive and study the autocovariance (and autocorrelation) structure of these models, let us now verify how we can classify a time series model as a Seasonal ARMA. To better explain this, let us again consider the model whose autocovariance we have just studied.

```{example, name = "Classifying a Seasonal ARMA"}
Returning to our previous example, we can see that the time series follows an $ARMA(0,1)\times(1,0)_{12}$ process by rearranging the terms as follows.

\begin{align*}
  {X_t} &= \Phi {X_{t - 12}} + {W_t} + \theta {W_{t - 1}} \hfill \\[0.2cm]
{X_t} - \Phi {X_{t - 12}} &= {W_t} + \theta {W_{t - 1}} \hfill \\[0.2cm]
  \underbrace {\left( {1 - \Phi {B^{12}}} \right)}_{{\Phi _1}\left( {{B^{12}}} \right)}\underbrace 1_{\phi \left( B \right)}{X_t} &= \underbrace 1_{{\Theta _Q}\left( B \right)}\underbrace {\left( {1 - \theta B} \right)}_{\theta \left( B \right)}{W_t} \hfill \\
\end{align*}
```

For this model we can see how the autoregressive operator ($\phi(B)$) and the seasonal moving average operator ($\Theta_Q(B)$) are included but, for this model, are simply equal to one. With the information provided, we can now finally define the most general class of models presented in this chapter.

```{definition, name = "Seasonal ARIMA Model"}
The Seasonal Autoregressive Integrated Moving Average model is denoted as $ARIMA(p, d, q)\times(P, D, Q)_S$ and defined as:

$$\Phi_P \left({B^S}\right) \phi\left(B\right) \, \delta^D_S \, \delta^d X_t = \Theta_Q \left({ B^S }\right) \theta \left({ B }\right) W_t$$

where $\delta^d = \left({1-B}\right)^d$ and $\delta^D_S = \left({1-B^S}\right)^D$.
```

You can notice that the definition of this model is quite complex but delivers an extremely flexible class of models that allows to describe (and predict) a wide range of time series with different forms of non-stationarity and seasonality. Indeed, aside from the previous forms of non-stationarity addressed by ARIMA models ($\delta^d$), these models also allow to address the same forms of non-stationarity within the seasons ($\delta^D_S$) in addition to the different dependence structures made available by SARMA models. However, the increased complexity of these models comes at a certain cost. For example, classifying a time series model as a SARIMA model becomes a slightly more lengthy procedure.

```{example, name = "Classifying a SARIMA"}
Consider the following time series:
  
$${X_t} = {X_{t - 1}} + {X_{t - 12}} - {X_{t - 13}} + {W_t} + \theta {W_{t - 1}} + \Theta {W_{t - 12}} + \Theta \theta {W_{t - 13}}.$$
  
We can rearrange the terms of this model in order to classify it as a SARIMA model as follows:

\begin{align}
  {X_t} - {X_{t - 1}} - {X_{t - 12}} + {X_{t - 13}} &= {W_t} + \theta {W_{t - 1}} + \Theta {W_{t - 12}} + \Theta \theta {W_{t - 13}} \hfill \\[0.2cm]
  \left( {1 - B - {B^{12}} + {B^{13}}} \right){X_t} &= \left( {1 + \theta B + \Theta {B^{12}} + \theta \Theta {B^{13}}} \right){W_t}
\end{align}

At this point the trick is to try and detect if the polynomials of the backshift operator $B$ can be factorized into simple differencing polynomials and/or seasonal and standard ARMA polynomials. In this case, it is possible to show this as follows:

$$\underbrace {\left( {1 - {B^{12}}} \right)}_{{\delta_{12}^1}}\underbrace {\left( {1 - B} \right)}_{\delta^1} {X_t} = \underbrace {\left( {1 + \Theta {B^{12}}} \right)}_{{\Theta _Q}\left( {{B^S}} \right)}\underbrace {\left( {1 + \theta B} \right)}_{\theta \left( B \right)}{W_t}, $$

Indeed, the polynomials for the observations $X_t$ do not depend on any parameters and are therefore simply differencing polynomials while those for the innovation noise both depend on parameters and therefore constitute the seasonal and standard moving average polynomials. Based on this final form, we can conclude that the initial time series model can be classified as a SARIMA model which can be denoted as $ARIMA(0,1,1)\times(0,1,1)_{12}$.
```

Another additional problem due to the complexity of these models is identifying the order of the different terms $p, d, q, P, D, Q, s$. In fact, as we saw in the previous sections, this is already not straightforward for standard ARMA models and becomes even more challenging for SARIMA models. The first approach is to understand if the time series is stationary and, if not, try and visually assess whether a standard and/or seasonal differencing allows to make the time series stationary (thereby defining $d$ and/or $D$). Once this is done, one can use the ACF and PACF plots to check the behaviour of the autocovariance and eventually detect forms of stochastic seasonality with peaks at regular lagged intervals (thereby defining $s$). Finally, the definition of the terms $p, q, P, Q$ is usually done via prior knowledge of the phenomenon underlying the time series data or via model selection criteria for which one determines a (large) model within which to select among all possible candidates included within this model.

The procedure to modelling data always has a subjective input and two (or more) different models can be equally effective in describing and/or predicting a given time series. Therefore, as for all applied examples in this book, we will now provide insight to one among many possible ways to fit a model to a given time series. For this reason, let us consider the monthly water levels of Lake Erie from 1921 to 1970.

```{r, fig.asp = 0.6, fig.width = 7, fig.align='center', fig.cap="Monthly Water Levels of Lake Erie between 1921 and 1970", cache = TRUE}
Xt = gts(as.numeric(dmseries("https://datamarket.com/data/set/22pw/monthly-lake-erie-levels-1921-1970#!ds=22pw&display=line")),
    start = 1921, freq = 12, name_ts = "Water Levels",
    data_name = "Monthly Lake Erie Levels", name_time = "")
plot(Xt)
```

We can see how the time series appears to fluctuate hinting that it may not be stationary. Moreover, knowing that the time series represents a natural phenomenon observed over months, it could be reasonable to think that the weather (and consequent water cycle) has an impact on the water levels thereby entailing some form of seasonality. Indeed, when looking at ACF and PACF plots of this data, it can be seen how there is a sinusoidal wave with peaks at lags 12 and 24 (and possibly further) indicating that there appears to be high correlation between the same months.

```{r, fig.asp = 0.45, fig.width = 8, fig.align='center', fig.cap="Empirical ACF (left) and PACF (right) of the Lake Erie time series data.", cache = TRUE}
corr_analysis(Xt)
```

Considering the possible non-stationarity in the data, would differencing the time series make it stationary? And if so, which kind of differencing? Let us consider the first-order difference as well as a seasonal difference with $s = 12$ (since that what appears in the ACF plot).

```{r, fig.asp = 0.45, fig.width = 8, fig.align='center', fig.cap="First difference (top) and difference at lag 12 (bottom) of the Lake Erie time series data.", cache = TRUE}
d_Xt = gts(diff(Xt))
d_s = gts(diff(Xt, lag = 12))
par(mfrow = c(2,1))
plot(d_Xt)
plot(d_s)
```

We can see how both differencing approaches appear to make the resulting time series (reasonably) stationary. If we take a look at the ACF and PACF plots of the seasonal differencing, we can see how the ACF does not show clear seasonality anymore while some residual seasonality can be observed in the PACF plot.

```{r, fig.asp = 0.45, fig.width = 8, fig.align='center', fig.cap="Empirical ACF (left) and PACF (right) of the seasonal difference ($s = 12$) of the Lake Erie time series data.", cache = TRUE}
corr_analysis(d_s)
```

Since both differencings appear to do a good job in making the time series stationary, let us consider a SARIMA model with $d=1$, $D=1$ and $s = 12$. Moreover, considering the structure of the ACF we could envisage using an AR($p$) component in additional to some seaonsal components to address the residual seasonality in the PACF. Hence, let us estimate a SARIMA model defined as $ARIMA(2,1,0)\times(1,1,1)_{12}$.

```{r, cache = TRUE}
mod = estimate(SARIMA(ar = 2, i = 1, sar = 1, sma = 1, s = 12, si = 1), Xt, method = "mle")
check(mod)
```

Based on the diagnostic plots, we can see that this model does a reasonably good job and could therefore be considered as a candidate model for explaining/predicting this time series. Supposing we were hydrologists who intend to plan the management of water resources over the next two years: we may therefore be interested in understanding/predicting the future behaviour of the lake's water levels over the next 24 months. As for ARMA models, we can also predict using SARIMA models and, for this particular data and model, we obtain the following forecast.

```{r, cache = TRUE}
predict(mod, n.ahead = 24)
```

As you can see, the forecasted values (and confidence intervals) appear to reasonably follow the pattern of the observed time series confirming that the fitted model could be a possible candidate to consider for the considered time series data.










```{r, fig.asp = 0.6, fig.width = 7, fig.align='center', fig.cap="TO DO", cache = TRUE}
Xt = gts(as.numeric(dmseries("https://datamarket.com/data/set/235j/number-of-daily-births-in-quebec-jan-01-1977-to-dec-31-1990#!ds=235j&display=line")),
    start = 1977, freq = 365, name_ts = "Number of Births",
    data_name = "Number of Births in Quebec", name_time = "")
plot(Xt)
```

```{r, fig.asp = 0.45, fig.width = 8, fig.align='center', fig.cap="Empirical ACF (left) and PACF (right) of the Births time series data.", cache = TRUE}
corr_analysis(Xt, lag.max = 50)
```

```{r, cache = TRUE}
mod = estimate(SARIMA(ar = 2, ma = 1, sar = 2, sma = 1, s = 7, si = 1), Xt, method = "mle")
check(mod)
```

```{r, cache = TRUE}
predict(mod, n.ahead = 30)
```