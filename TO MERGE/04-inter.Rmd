# Under development

## Moving Average Models `r emo::ji("warning")`

The class of AR($p$) models is a very general class that allows to take into account different forms of linear dependence between past and future observations. However, there are some forms of linear dependence that can appear as "shocks" in the innovation noise of a time series. In this sense, we have already seen a model that describes a certain form of this dependence which is the MA(1) defined as

$$X_t = \theta W_{t-1} + W_t$$
where $(W_t)$ is a white noise process. As can be seen, the observed time series $(X_t)$ is a linear combination of the innovation process. In this section we generalise this process to the class of MA($q$) processes that are defined as follows.

```{definition, label="maq", name="Moving Average of Order Q"}
A Moving Average of Order $q$ or MA($q$) model is defined as follows:

$${X_t} = \theta_1 W_{t-1} + ... + \theta_q W_{t-q} + W_t,$$
  
where $\theta_q \neq 0$.
```

If we make use of the backshift operator defined earlier in this chapter, we can rewrite this model as:

\[\begin{aligned}
  X_t &= \theta_1 B W_t + ... + \theta_q  B^q W_t + W_t \\ 
  &= (\theta_1  B + ... + \theta_q  B^q + 1) W_t .\\
\end{aligned} \]

Based on this we can deliver the following definition.

```{definition, label="maqo", name="Moving Average Operator"}
The moving average operator is defined as
$$\theta(B) \equiv 1 + \theta_1 B + ... + \theta_q B^q.$$
```

This allows us to write an MA($q$) process as

$$X_t = \theta (B) W_t .$$
Following this definition, it is possible to see that an MA($q$) process is always stationary. Indeed, an MA($q$) respects the defintion of a linear process (see Definition \@ref(def:lp)) where $\psi_0 = 1$, $\psi_j = \theta_j$ for $j = 1, ..., q$, and $\psi_j = 0$ for $j > q$ and, based on this, we can show its stationarity.

```{example, name="Stationarity of an MA(q) Process"}
Based on the above definitions, we can rewrite an MA(q) process as $X_t = \sum\limits_{i = 0}^q {{\theta _i}{W_{t - i}}}$, where $\theta_0 = 1$ and assume the condition that $\sum\limits_{j = 0}^q {\theta _j^2} < \infty$ (if $\theta_j < \infty$ for all $j$ and $q < \infty$, this condition is verified). Using this expression we start verifying (weak) stationarity by checking the expectation of an MA(q) process which is given by

$$E\left[ X_t \right] = E\left[ \sum\limits_{i = 0}^q {{\theta _i}{W_{t - i}}} \right] = \sum\limits_{i = 0}^q {{\theta _i}}  E\left[ {W_{t - i}} \right] = 0,$$
  
which confirms that the expectation is constant. We now have to verify the conditions on the autocovariance function which is derived as follows:

\begin{align}
Cov \left( {{X_{t + h}},{X_t}} \right) &= Cov \left( {\sum\limits_{j = 0}^q {{\theta _j}{W_{t + h - j}}} ,\sum\limits_{i = 0}^q {{\theta _i}{W_{t - i}}} } \right) \\
&= \sum\limits_{j = 0}^q {\sum\limits_{i = 0}^q {{\theta _j}{\theta _i}Cov \left( {{W_{t + h - j}},{W_{t - i}}} \right)} }  \\
&= \sum\limits_{j = 0}^q {\sum\limits_{i = 0}^q {{\theta _j}{\theta _i}\underbrace {Cov \left( {{W_{t + h - j}},{W_{t - i}}} \right)}_{ = 0 \, \text{for} \, i = j - h}} }  + {1_{\left\{ {\left| h \right| \leqslant q} \right\}}}\sum\limits_{j = 0}^{q - \left| h \right|} {{\theta _{j + \left| h \right|}}{\theta _j} \underbrace{Cov \left( {{W_t},{W_t}} \right)}_{= \sigma^2}} \\
&= {1_{\left\{ {\left| h \right| \leqslant q} \right\}}}{\sigma ^2}\sum\limits_{j = 0}^{q - \left| h \right|} {{\theta _{j + \left| h \right|}}{\theta _j}}
\end{align}

As a result, we have:

\[{1_{\left\{ {\left| h \right| \leqslant q} \right\}}}{\sigma ^2}\sum\limits_{j = 0}^{q - \left| h \right|} {{\theta _{j + \left| h \right|}}{\theta _j}}  \leqslant {\sigma ^2}\sum\limits_{j = 0}^q {\theta _j^2}  < \infty \]

Given these results, we can see that the variance is given by ${\sigma ^2}\sum\limits_{j = 0}^q {\theta _j^2}$ which is finite and does not depend on time and the covariance only depends on the lag $h$ (not on the time $t$). Therefore, an MA(q) process is weakly stationary.
```

Although an MA($q$) process is always weakly stationary, it is important to well define these models since they can be characterized by certain parametrizations that don't allow them to be uniquely identify them. Indeed, the latter issues can be referred to as identifiability issues in which different MA($q$) models (of the same order $q$) can produce identical autocovariance functions. Due to this issue, it would be impossible





```{example, name="Non-uniqueness of MA models"}
One particular issue of MA models is the fact that they are not unique. 
In essence, one is not able to correctly tell if the process is of one model
or another. Consider the following two models:

$$
\begin{aligned}
\mathcal{M}_1:&{}& X_t &= W_{t-1} + \frac{1}{\theta}W_t,&{}& W_t\sim \mathcal{N} (0, \sigma^2\theta^2) \\
\mathcal{M}_2:&{}& Y_t &= V_{t-1} + \theta V_t,&{}& V_t\sim \mathcal{N} (0,\sigma^2)
\end{aligned}
$$

By observation, one can note that the models share the same expectation:

\[E\left[ {{X_t}} \right] = E\left[ {{Y_t}} \right] = 0\]

However, for the autocovariance, the process requires a bit more effort.

\begin{align}
Cov \left( {{X_t},{X_{t + h}}} \right) &= Cov \left( {{W_t} + \frac{1}{\theta }{W_{t - 1}},{W_{t + h}} + \frac{1}{\theta }{W_{t + h - 1}}} \right) = {1_{\left\{ {h = 0} \right\}}}{\sigma ^2}{\theta ^2} + {\sigma ^2} + {1_{\left\{ {\left| h \right| = 1} \right\}}}\frac{{{\sigma ^2}{\theta ^2}}}{\theta } = {\sigma ^2}\left( {{1_{\left\{ {h = 0} \right\}}}{\theta ^2} + 1 + {1_{\left\{ {\left| h \right| = 1} \right\}}}\theta } \right) \\
Cov \left( {{Y_t},{Y_{t + h}}} \right) &= Cov \left( {{V_t} + \theta {V_{t - 1}},{V_{t + h}} + \theta {V_{t + h - 1}}} \right) = {1_{\left\{ {h = 0} \right\}}}{\sigma ^2}{\theta ^2} + {\sigma ^2} + {1_{\left\{ {\left| h \right| = 1} \right\}}}{\sigma ^2}\theta  = {\sigma ^2}\left( {{1_{\left\{ {h = 0} \right\}}}{\theta ^2} + 1 + {1_{\left\{ {\left| h \right| = 1} \right\}}}\theta } \right)
\end{align}

Therefore, $Cov \left( {{X_t},{X_{t + h}}} \right) = Cov \left( {{Y_t},{Y_{t + h}}} \right)$! Moreover, since $W_t$ and $V_t$ are Gaussian
the models are viewed as being similar and, thus, cannot be distinguished.
```

The implication of the last example is rather profound. In particular, consider

\[\vec X = \left[ {\begin{array}{*{20}{c}}
  {{X_1}} \\ 
   \vdots  \\ 
  {{X_N}} 
\end{array}} \right],\vec Y = \left[ {\begin{array}{*{20}{c}}
  {{Y_1}} \\ 
   \vdots  \\ 
  {{Y_N}} 
\end{array}} \right]\]

Thus, the covariance matrix is given by:

\[Cov \left( {\vec X} \right) = {\sigma ^2}\left[ {\begin{array}{*{20}{c}}
  {\left( {1 + {\theta ^2}} \right)}&\theta &0& \cdots &0 \\ 
  \theta &{\left( {1 + {\theta ^2}} \right)}&\theta &{}& \vdots  \\ 
  0&\theta &{\left( {1 + {\theta ^2}} \right)}&{}&{} \\ 
   \vdots &{}&{}& \ddots &{} \\ 
  0& \cdots &{}&{}&{\left( {1 + {\theta ^2}} \right)} 
\end{array}} \right] = Cov \left( {\vec Y} \right) = \Omega \]

Now, consider the $\vec \beta$ to be the parameter vector for estimates and
the approach to estimate is via the MLE:

\[L\left( {\vec \beta |\vec X} \right) = {\left( {2\pi } \right)^{ - \frac{N}{2}}}{\left| \Omega  \right|^{ - \frac{1}{2}}}\exp \left( { - \frac{1}{2}{{\vec X}^T}{\Omega ^{ - 1}}\vec X} \right)\]

If for both models the following parameters ${{\vec \beta }_1} = \left[ {\begin{array}{*{20}{c}}
  \theta  \\ 
  {{\sigma ^2}} 
\end{array}} \right],{{\vec \beta }_2} = \left[ {\begin{array}{*{20}{c}}
  {\frac{1}{\theta }} \\ 
  {{\sigma ^2}\theta } 
\end{array}} \right]$ are set, then

\[L\left( {{{\vec \beta }_1}|\vec X} \right) = L\left( {{{\vec \beta }_2}|\vec X} \right)\]

There is a huge problem being able to identify what the values of the parameters
are. To ensure that this problem does not arise in practice, there is 
the requirement for invertibility, or being able to transform an MA(q) into
an AR($\infty$).