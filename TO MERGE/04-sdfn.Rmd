# Under development

## Autoregressive Moving Average Models `r emo::ji("warning")`

```{r, echo = FALSE}
library(simts)
suppressPackageStartupMessages(library(rdatamarket))
```

The classes of time series models discussed this far (i.e. AR($p$) and MA($q$) models) are both classes that address different forms of dependence between observations over time. The class of AR($p$) models describes the direct (and indirect) linear dependence between observations over time while MA($q$) models describe the dependence in the innovation processes rather than the observations themselves. Separately, these classes of models therefore allow to take into account two extremely common dependence settings within time series. In this section however we present a class of models that combines the features of these two classes and are called ARMA($p$,$q$) models. These are defined as follows.

```{definition, label="defARMA", name="ARMA models"}
A process $(X_{t})$ is an ARMA($p$,$q$) process if $(X_t)$ (or $(X_{t}-\mathbb{E}[X_{t}])$) satisfies the linear difference equation
		\[X_{t} = \phi_{1}X_{t-1} + \cdots + \phi_{p}X_{t-p} + W_{t}+\theta_{1}W_{t-1}+ \cdots+\theta_{q}W_{t-q},\]
where $W_{t}\sim \mathcal{N}(0,\sigma_{w}^2)$. 
An ARMA($p$,$q$) can be written in concise form as:
				$\phi \left( B \right) 
		\[X_t = \theta \left( B \right)W_t, \]
where $\phi(B)$ and $\theta(B)$ are the AR and MA polynomials (operators):
	\[\phi(B)=1-\phi_{1}B-\cdots-\phi_{p}B^p \]
	\[\theta(B)=1+\theta_{1}B+\cdots+\theta_{q}B^q. \]
```
<br>

Based on this definition, we can see how ARMA($p$,$q$) models allow to take into account the two forms of dependence of the previous classes of models and generalize them. Indeed, if we fix $q = 0$, an ARMA($p$,$q$) model simply becomes an AR($p$) model while, on the contrary, if we fix $p = 0$ these models become MA($q$) models. Hence, the class of ARMA($p$,$q$) models is an extremely flexible class which inherits the properties (and constraints) of the two classes of models discussed this far.

```{example, name="Causality and Invertibility of ARMA(p,q) models"}
One of the main properties that ARMA models inherit from the classes of AR(p) and MA(q) models are the conditions required for these models to be stationary and for their parameters to be identifiable. Indeed, we saw that AR(p) models need to respect the condition of *causality* (since they are always invertible) while MA(q) models need to respect the condition of *invertibility* (since they are always causal).
```

Given these conditions, we can now define the conditions for the ARMA($p$,$q$) models we are going to consider in this section.

```{definition, name="Causal and Invertible ARMA(p,q) models"}
An ARMA model is causal and invertible if:
  
- the AR($p$) part is causal, and
- the MA($q$) part is invertible
```

If the above conditions are respected, then we are sure to be working with ARMA models that are stationary and whose parameters can be uniquely identified. This is a direct consequence of the separate properties and conditions for AR($p$) models, on one side, and MA($q$) models, on the other side. However, there are additional "constraints" that need to be considered given the wider class of dependence that ARMA models can take into account. Indeed, with the copresence of AR($p$) and MA($q$) models can lead to the definition of ARMA($p$,$q$) models that can be re-expressed as another ARMA($p*$,$q*$) model where $p \leq p*$ and/or $q \leq q*$. Remember that the identifiability issue for MA($q$) models exists only for models with the same order $q$ so this is a different setting. This issue for ARMA models is called "parameter redundancy".

```{example, name="Parameter Redundancy of ARMA models", label = "redunARMA"}
Let us consider the ARMA(1,1) process:
		\begin{align*}
			X_t - 0.9 X_{t-1} &= W_t - 0.9 W_{t-1}.
		\end{align*}
Using the polynomial operators defined earlier, this model can be :
		\begin{equation*}
			X_t - 0.9 X_{t-1} = W_t - 0.9 W_t \Longleftrightarrow (1 - 0.9B) X_t = (1 - 0.9 B) W_t.
		\end{equation*}
Based on the expression above, we can see that both sides of the ARMA equation share a common term which is $(1 - 0.9B)$ which therefore can be simplified thereby defining the mode 
\[X_t = W_t.\] 

Hence, the specified ARMA(1,1) model is actually a white noise mode.  
```

In general, if a model has autoregressive and moving average operators that share a common root, then the model has **redundant parameters** and can consistute an "over-parametrization" of the problem. Let us further explore this problem through another example.

```{example, name="Parameter Redundancy of ARMA models", label = "redunARMA2"}
Consider the following model:
		\begin{equation*}
			X_t = 0.3 X_{t-1} + 0.1 X_{t-2} + W_t + W_{t-1} + 0.16 W_{t-2},
 		\end{equation*}
which is an ARMA(2,2). By rearranging the terms, we can do the following calculations
		\begin{equation*}
			\begin{aligned} 
				 X_t - 0.3 X_{t-1} - 0.1 X_{t-2} &= W_t + W_{t-1} + 0.16 W_{t-2}\\
				 (1 - 0.3 B - 0.1 B^2 ) X_t  &= (1 + B + 0.16B^2) W_t\\
				 (1 + 0.2B)(1 - 0.5 B) X_t  &= (1 + 0.2B)(1 + 0.8 B) W_t\\
				(1 - 0.5 B) X_t  &= (1 + 0.8 B) W_t\\
				 X_t  &= 0.5 X_{t-1} + W_t - 0.8 W_{t-1}.
			\end{aligned}
		\end{equation*}
Therefore, the initial ARMA(2,2) model is in fact an **ARMA(1,1)** model. Note that the latter model is causal (as $|\phi|<1$) and invertible (as $|\theta| < 1$) and therefore respects the conditions we highlighted at the beginning of this section.
```



### ACF/PACF

- Derivation the ACF/PACF of ARMA($p$,$q$) is generally difficult. \textbf{Example}: Given an ARMA(1,1), we have $\rho(h) = \phi^{h-1} \rho(1)$ where
			
\begin{equation*}
				\rho(1) = \frac{\left(1+\theta \phi \right)\left(\theta + \phi\right)}{1+2\phi\theta+\theta^2}.
\end{equation*}
			
- It is difficult to identify ARMA process from their ACF/PACF. Indeed, we have:

Table: (\#acfpacf) Caption here

-------------------------------------------------------------
                  AR($p$)         MA($q$)       ARMA($p$,$q$)
------------- --------------- --------------- --------------- 
    ACF         Tails off     Cuts off after      Tails off
                                  lag $q$
                                  
   PACF       Cuts off after    Tails off         Tails off
                 lag $p$
-------------------------------------------------------------


As an example let us consider the following two ARMA($q$) models:

$$
\begin{aligned}
X_t &= 0.3 X_{t-1} - 0.8 X_{t-2} +  0.7 W_{t-1} + W_t \\
Y_t &= 1.2 X_{t-1} - 0.25 X_{t-2} - 0.1 W_{t-1} - 0.75 W_{t-2} + W_t, \\
\end{aligned}
$$

where $W_t \overset{iid}{\sim} \mathcal{N}(0, 1)$

```{r, cache = FALSE, fig.cap="TO DO", fig.asp = 1, fig.width = 8, fig.align='center'}
Xt = gen_gts(n = 500, ARMA(ar = c(0.3,-0.8), ma = 0.7, sigma2 = 1))
Yt = gen_gts(n = 500, ARMA(ar = c(1.2, -0.25), 
             ma = c(-0.1, -0.75), sigma2 = 1))
par(mfrow = c(2,1))
plot(Xt, main = expression("Simulated process: "* X[t] *" with "* T *" = 500"))
plot(Yt, main = expression("Simulated process: "* Y[t] *" with "* T *" = 500"))
```

The theoretical ACF/PACF

```{r, fig.asp = 0.45, fig.width = 8, fig.align='center', fig.cap="TO DO"}
par(mfrow = c(1,2))
plot(theo_acf(ar = c(0.3,-0.8), ma = 0.7), main = expression("Theo. ACF - MA(1); "* theta *" = 0.9"))
plot(theo_pacf(ar = c(0.3,-0.8), ma = 0.7), main = expression("Theo. PACF - MA(1); "* theta *" = 0.9"))
```

```{r, fig.asp = 0.45, fig.width = 8, fig.align='center', fig.cap="TO DO"}
par(mfrow = c(1,2))
plot(theo_acf(ar = c(1.2, -0.25), ma = c(-0.1, -0.75)), main = expression("Theo. ACF - MA(1); "* theta *" = 0.9"))
plot(theo_pacf(ar = c(1.2, -0.25), ma = c(-0.1, -0.75)), main = expression("Theo. ACF - MA(1); "* theta *" = -0.9"))
```

Remark on the identification of the order of an ARMA model:

- ACF and PACF of ARMA models are difficult to interpret.
- It is generally easier to consider a list of candidate models and selected the ``best'' model in this list using a model selection criterion or an estimator of the prediction error (e.g. MAPE).
- Check diagnostic plot to assess if the model is reasonable for this time series.


```{r, fig.asp = 0.6, fig.width = 7, fig.align='center', fig.cap="TO DO", cache = TRUE}
Xt = gts(as.numeric(dmseries("https://datamarket.com/data/set/22y1/annual-copper-prices-1800-1997#!ds=22y1&display=line")),
    start = 1921, freq = 12, name_ts = "Copper prices (minus the long-term trend)",
    data_name = "Copper Price", name_time = "")
plot(Xt)
```

```{r, fig.asp = 0.45, fig.width = 8, fig.align='center', fig.cap="Empirical ACF (left) and PACF (right) of the Copper time series data."}
corr_analysis(Xt)
```

```{r, fig.asp = 1.45, fig.width = 8, fig.align='center', fig.cap="Empirical ACF (left) and PACF (right) of the Copper time series data."}
select(ARMA(4,5), Xt)
```

To decide:

```{r, eval = FALSE}
evaluate(list(AR(1), ARMA(3,2), ARMA(3, 5)), Xt, criterion = "MAPE", start = 0.5)
```

```{r, eval = TRUE, cache = TRUE, fig.asp = 0.73, fig.width = 7, fig.align='center', fig.cap="Diagnostic plots for the residuals of a TO DO"}
model_copper = estimate(ARMA(3,2), Xt)
check(model_copper)
```

## ARIMA

## SARIMA

### Real data example

```{r, fig.asp = 0.6, fig.width = 7, fig.align='center', fig.cap="TO DO", cache = TRUE}
Xt = gts(as.numeric(dmseries("https://datamarket.com/data/set/22pw/monthly-lake-erie-levels-1921-1970#!ds=22pw&display=line")),
    start = 1921, freq = 12, name_ts = "Water Levels",
    data_name = "Monthly Lake Erie Levels", name_time = "")
plot(Xt)
```

```{r, fig.asp = 0.45, fig.width = 8, fig.align='center', fig.cap="Empirical ACF (left) and PACF (right) of the Lake Erie time series data."}
corr_analysis(Xt)
```


```{r, fig.asp = 0.6, fig.width = 7, fig.align='center', fig.cap="TO DO", cache = TRUE}
Xt = gts(as.numeric(dmseries("https://datamarket.com/data/set/235j/number-of-daily-births-in-quebec-jan-01-1977-to-dec-31-1990#!ds=235j&display=line")),
    start = 1977, freq = 365, name_ts = "Number of Births",
    data_name = "Number of Births in Quebec", name_time = "")
plot(Xt)
```

```{r, fig.asp = 0.45, fig.width = 8, fig.align='center', fig.cap="Empirical ACF (left) and PACF (right) of the Births time series data."}
corr_analysis(Xt, lag.max = 50)
```

```{r}
mod = estimate(SARIMA(ar = 2, ma = 1, sar = 2, sma = 1, s = 7, si = 1), Xt, method = "mle")
check(mod)
```

```{r}
predict(mod, n.ahead = 30)
```