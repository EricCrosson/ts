[
["under-development.html", "Chapter 5 Under development ", " Chapter 5 Under development "],
["moving-average-models-1.html", "5.1 Moving Average Models ⚠️", " 5.1 Moving Average Models ⚠️ The class of AR(\\(p\\)) models is a very general class that allows to take into account different forms of linear dependence between past and future observations. However, there are some forms of linear dependence that can appear as “shocks” in the innovation noise of a time series. In this sense, we have already seen a model that describes a certain form of this dependence which is the MA(1) defined as \\[X_t = \\theta W_{t-1} + W_t\\] where \\((W_t)\\) is a white noise process. As can be seen, the observed time series \\((X_t)\\) is a linear combination of the innovation process. In this section we generalise this process to the class of MA(\\(q\\)) processes that are defined as follows. Definition 4.1 (Moving Average of Order Q) A Moving Average of Order \\(q\\) or MA(\\(q\\)) model is defined as follows: \\[{X_t} = \\theta_1 W_{t-1} + ... + \\theta_q W_{t-q} + W_t,\\] where \\(\\theta_q \\neq 0\\). If we make use of the backshift operator defined earlier in this chapter, we can rewrite this model as: \\[\\begin{aligned} X_t &amp;= \\theta_1 B W_t + ... + \\theta_q B^q W_t + W_t \\\\ &amp;= (\\theta_1 B + ... + \\theta_q B^q + 1) W_t .\\\\ \\end{aligned} \\] Based on this we can deliver the following definition. Definition 4.2 (Moving Average Operator) The moving average operator is defined as \\[\\theta(B) \\equiv 1 + \\theta_1 B + ... + \\theta_q B^q.\\] This allows us to write an MA(\\(q\\)) process as \\[X_t = \\theta (B) W_t .\\] Following this definition, it is possible to see that an MA(\\(q\\)) process is always stationary. Indeed, an MA(\\(q\\)) respects the defintion of a linear process (see Definition ??) where \\(\\psi_0 = 1\\), \\(\\psi_j = \\theta_j\\) for \\(j = 1, ..., q\\), and \\(\\psi_j = 0\\) for \\(j &gt; q\\) and, based on this, we can show its stationarity. Example 4.1 (Stationarity of an MA(q) Process) Based on the above definitions, we can rewrite an MA(q) process as \\(X_t = \\sum\\limits_{i = 0}^q {{\\theta _i}{W_{t - i}}}\\), where \\(\\theta_0 = 1\\) and assume the condition that \\(\\sum\\limits_{j = 0}^q {\\theta _j^2} &lt; \\infty\\) (if \\(\\theta_j &lt; \\infty\\) for all \\(j\\) and \\(q &lt; \\infty\\), this condition is verified). Using this expression we start verifying (weak) stationarity by checking the expectation of an MA(q) process which is given by \\[E\\left[ X_t \\right] = E\\left[ \\sum\\limits_{i = 0}^q {{\\theta _i}{W_{t - i}}} \\right] = \\sum\\limits_{i = 0}^q {{\\theta _i}} E\\left[ {W_{t - i}} \\right] = 0,\\] which confirms that the expectation is constant. We now have to verify the conditions on the autocovariance function which is derived as follows: \\[\\begin{align} Cov \\left( {{X_{t + h}},{X_t}} \\right) &amp;= Cov \\left( {\\sum\\limits_{j = 0}^q {{\\theta _j}{W_{t + h - j}}} ,\\sum\\limits_{i = 0}^q {{\\theta _i}{W_{t - i}}} } \\right) \\\\ &amp;= \\sum\\limits_{j = 0}^q {\\sum\\limits_{i = 0}^q {{\\theta _j}{\\theta _i}Cov \\left( {{W_{t + h - j}},{W_{t - i}}} \\right)} } \\\\ &amp;= \\sum\\limits_{j = 0}^q {\\sum\\limits_{i = 0}^q {{\\theta _j}{\\theta _i}\\underbrace {Cov \\left( {{W_{t + h - j}},{W_{t - i}}} \\right)}_{ = 0 \\, \\text{for} \\, i = j - h}} } + {1_{\\left\\{ {\\left| h \\right| \\leqslant q} \\right\\}}}\\sum\\limits_{j = 0}^{q - \\left| h \\right|} {{\\theta _{j + \\left| h \\right|}}{\\theta _j} \\underbrace{Cov \\left( {{W_t},{W_t}} \\right)}_{= \\sigma^2}} \\\\ &amp;= {1_{\\left\\{ {\\left| h \\right| \\leqslant q} \\right\\}}}{\\sigma ^2}\\sum\\limits_{j = 0}^{q - \\left| h \\right|} {{\\theta _{j + \\left| h \\right|}}{\\theta _j}} \\end{align}\\] As a result, we have: \\[{1_{\\left\\{ {\\left| h \\right| \\leqslant q} \\right\\}}}{\\sigma ^2}\\sum\\limits_{j = 0}^{q - \\left| h \\right|} {{\\theta _{j + \\left| h \\right|}}{\\theta _j}} \\leqslant {\\sigma ^2}\\sum\\limits_{j = 0}^q {\\theta _j^2} &lt; \\infty \\] Hence, an MA(q) process is weakly stationary since the variance is . As a consequence, MA(\\(q\\)) processes are weakly stationary processes. Although an MA(\\(q\\)) process is always weakly stationary, it is important to well define these models since they can be characterized by However, the MA(q) processes may not be identifiable through their autocovariance functions. By the latter we mean that different parameteres for a same order MA(q) model can deliver the exact same autocovariance function and it would therefore be impossible to retrieve the parameters of the model by only looking at the autocovariance function. Example 4.2 (Non-uniqueness of MA models) One particular issue of MA models is the fact that they are not unique. In essence, one is not able to correctly tell if the process is of one model or another. Consider the following two models: \\[ \\begin{aligned} \\mathcal{M}_1:&amp;{}&amp; X_t &amp;= W_{t-1} + \\frac{1}{\\theta}W_t,&amp;{}&amp; W_t\\sim \\mathcal{N} (0, \\sigma^2\\theta^2) \\\\ \\mathcal{M}_2:&amp;{}&amp; Y_t &amp;= V_{t-1} + \\theta V_t,&amp;{}&amp; V_t\\sim \\mathcal{N} (0,\\sigma^2) \\end{aligned} \\] By observation, one can note that the models share the same expectation: \\[E\\left[ {{X_t}} \\right] = E\\left[ {{Y_t}} \\right] = 0\\] However, for the autocovariance, the process requires a bit more effort. \\[\\begin{align} Cov \\left( {{X_t},{X_{t + h}}} \\right) &amp;= Cov \\left( {{W_t} + \\frac{1}{\\theta }{W_{t - 1}},{W_{t + h}} + \\frac{1}{\\theta }{W_{t + h - 1}}} \\right) = {1_{\\left\\{ {h = 0} \\right\\}}}{\\sigma ^2}{\\theta ^2} + {\\sigma ^2} + {1_{\\left\\{ {\\left| h \\right| = 1} \\right\\}}}\\frac{{{\\sigma ^2}{\\theta ^2}}}{\\theta } = {\\sigma ^2}\\left( {{1_{\\left\\{ {h = 0} \\right\\}}}{\\theta ^2} + 1 + {1_{\\left\\{ {\\left| h \\right| = 1} \\right\\}}}\\theta } \\right) \\\\ Cov \\left( {{Y_t},{Y_{t + h}}} \\right) &amp;= Cov \\left( {{V_t} + \\theta {V_{t - 1}},{V_{t + h}} + \\theta {V_{t + h - 1}}} \\right) = {1_{\\left\\{ {h = 0} \\right\\}}}{\\sigma ^2}{\\theta ^2} + {\\sigma ^2} + {1_{\\left\\{ {\\left| h \\right| = 1} \\right\\}}}{\\sigma ^2}\\theta = {\\sigma ^2}\\left( {{1_{\\left\\{ {h = 0} \\right\\}}}{\\theta ^2} + 1 + {1_{\\left\\{ {\\left| h \\right| = 1} \\right\\}}}\\theta } \\right) \\end{align}\\] Therefore, \\(Cov \\left( {{X_t},{X_{t + h}}} \\right) = Cov \\left( {{Y_t},{Y_{t + h}}} \\right)\\)! Moreover, since \\(W_t\\) and \\(V_t\\) are Gaussian the models are viewed as being similar and, thus, cannot be distinguished. The implication of the last example is rather profound. In particular, consider \\[\\vec X = \\left[ {\\begin{array}{*{20}{c}} {{X_1}} \\\\ \\vdots \\\\ {{X_N}} \\end{array}} \\right],\\vec Y = \\left[ {\\begin{array}{*{20}{c}} {{Y_1}} \\\\ \\vdots \\\\ {{Y_N}} \\end{array}} \\right]\\] Thus, the covariance matrix is given by: \\[Cov \\left( {\\vec X} \\right) = {\\sigma ^2}\\left[ {\\begin{array}{*{20}{c}} {\\left( {1 + {\\theta ^2}} \\right)}&amp;\\theta &amp;0&amp; \\cdots &amp;0 \\\\ \\theta &amp;{\\left( {1 + {\\theta ^2}} \\right)}&amp;\\theta &amp;{}&amp; \\vdots \\\\ 0&amp;\\theta &amp;{\\left( {1 + {\\theta ^2}} \\right)}&amp;{}&amp;{} \\\\ \\vdots &amp;{}&amp;{}&amp; \\ddots &amp;{} \\\\ 0&amp; \\cdots &amp;{}&amp;{}&amp;{\\left( {1 + {\\theta ^2}} \\right)} \\end{array}} \\right] = Cov \\left( {\\vec Y} \\right) = \\Omega \\] Now, consider the \\(\\vec \\beta\\) to be the parameter vector for estimates and the approach to estimate is via the MLE: \\[L\\left( {\\vec \\beta |\\vec X} \\right) = {\\left( {2\\pi } \\right)^{ - \\frac{N}{2}}}{\\left| \\Omega \\right|^{ - \\frac{1}{2}}}\\exp \\left( { - \\frac{1}{2}{{\\vec X}^T}{\\Omega ^{ - 1}}\\vec X} \\right)\\] If for both models the following parameters \\({{\\vec \\beta }_1} = \\left[ {\\begin{array}{*{20}{c}} \\theta \\\\ {{\\sigma ^2}} \\end{array}} \\right],{{\\vec \\beta }_2} = \\left[ {\\begin{array}{*{20}{c}} {\\frac{1}{\\theta }} \\\\ {{\\sigma ^2}\\theta } \\end{array}} \\right]\\) are set, then \\[L\\left( {{{\\vec \\beta }_1}|\\vec X} \\right) = L\\left( {{{\\vec \\beta }_2}|\\vec X} \\right)\\] There is a huge problem being able to identify what the values of the parameters are. To ensure that this problem does not arise in practice, there is the requirement for invertibility, or being able to transform an MA(q) into an AR(\\(\\infty\\)). "]
]
