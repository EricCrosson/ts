[
["under-development.html", "Chapter 5 Under development ", " Chapter 5 Under development "],
["autoregressive-moving-average-models.html", "5.1 Autoregressive Moving Average Models ⚠️", " 5.1 Autoregressive Moving Average Models ⚠️ The classes of time series models discussed this far (i.e. AR(\\(p\\)) and MA(\\(q\\)) models) are both classes that address different forms of dependence between observations over time. The class of AR(\\(p\\)) models describes the direct (and indirect) linear dependence between observations over time while MA(\\(q\\)) models describe the dependence in the innovation processes rather than the observations themselves. Separately, these classes of models therefore allow to take into account two extremely common dependence settings within time series. In this section however we present a class of models that combines the features of these two classes and are called ARMA(\\(p\\),\\(q\\)) models. These are defined as follows. Definition 4.1 (ARMA models) A process \\((X_{t})\\) is an ARMA(\\(p\\),\\(q\\)) process if \\((X_t)\\) (or \\((X_{t}-\\mathbb{E}[X_{t}])\\)) satisfies the linear difference equation \\[X_{t} = \\phi_{1}X_{t-1} + \\cdots + \\phi_{p}X_{t-p} + W_{t}+\\theta_{1}W_{t-1}+ \\cdots+\\theta_{q}W_{t-q},\\] where \\(W_{t}\\sim \\mathcal{N}(0,\\sigma_{w}^2)\\). An ARMA(\\(p\\),\\(q\\)) can be written in concise form as: $( B ) \\[X_t = \\theta \\left( B \\right)W_t, \\] where \\(\\phi(B)\\) and \\(\\theta(B)\\) are the AR and MA polynomials (operators): \\[\\phi(B)=1-\\phi_{1}B-\\cdots-\\phi_{p}B^p \\] \\[\\theta(B)=1+\\theta_{1}B+\\cdots+\\theta_{q}B^q. \\] Based on this definition, we can see how ARMA(\\(p\\),\\(q\\)) models allow to take into account the two forms of dependence of the previous classes of models and generalize them. Indeed, if we fix \\(q = 0\\), an ARMA(\\(p\\),\\(q\\)) model simply becomes an AR(\\(p\\)) model while, on the contrary, if we fix \\(p = 0\\) these models become MA(\\(q\\)) models. Hence, the class of ARMA(\\(p\\),\\(q\\)) models is an extremely flexible class which inherits the properties (and constraints) of the two classes of models discussed this far. Example 4.1 (Causality and Invertibility of ARMA(p,q) models) One of the main properties that ARMA models inherit from the classes of AR(p) and MA(q) models are the conditions required for these models to be stationary and for their parameters to be identifiable. Indeed, we saw that AR(p) models need to respect the condition of causality (since they are always invertible) while MA(q) models need to respect the condition of invertibility (since they are always causal). Given these conditions, we can now define the conditions for the ARMA(\\(p\\),\\(q\\)) models we are going to consider in this section. Definition 4.2 (Causal and Invertible ARMA(p,q) models) An ARMA model is causal and invertible if: the AR(\\(p\\)) part is causal, and the MA(\\(q\\)) part is invertible If the above conditions are respected, then we are sure to be working with ARMA models that are stationary and whose parameters can be uniquely identified. This is a direct consequence of the separate properties and conditions for AR(\\(p\\)) models, on one side, and MA(\\(q\\)) models, on the other side. However, there are additional “constraints” that need to be considered given the wider class of dependence that ARMA models can take into account. Indeed, with the copresence of AR(\\(p\\)) and MA(\\(q\\)) models can lead to the definition of ARMA(\\(p\\),\\(q\\)) models that can be re-expressed as another ARMA(\\(p*\\),\\(q*\\)) model where \\(p \\leq p*\\) and/or \\(q \\leq q*\\). Remember that the identifiability issue for MA(\\(q\\)) models exists only for models with the same order \\(q\\) so this is a different setting. This issue for ARMA models is called “parameter redundancy”. Example 4.2 (Parameter Redundancy of ARMA models) Let us consider the ARMA(1,1) process: \\[\\begin{align*} X_t - 0.9 X_{t-1} &amp;= W_t - 0.9 W_{t-1}. \\end{align*}\\] Using the polynomial operators defined earlier, this model can be : \\[\\begin{equation*} X_t - 0.9 X_{t-1} = W_t - 0.9 W_t \\Longleftrightarrow (1 - 0.9B) X_t = (1 - 0.9 B) W_t. \\end{equation*}\\] Based on the expression above, we can see that both sides of the ARMA equation share a common term which is \\((1 - 0.9B)\\) which therefore can be simplified thereby defining the mode \\[X_t = W_t.\\] Hence, the specified ARMA(1,1) model is actually a white noise mode. In general, if a model has autoregressive and moving average operators that share a common root, then the model has redundant parameters and can consistute an “over-parametrization” of the problem. Let us further explore this problem through another example. Example 4.3 (Parameter Redundancy of ARMA models) Consider the following model: \\[\\begin{equation*} X_t = 0.3 X_{t-1} + 0.1 X_{t-2} + W_t + W_{t-1} + 0.16 W_{t-2}, \\end{equation*}\\] which is an ARMA(2,2). By rearranging the terms, we can do the following calculations \\[\\begin{equation*} \\begin{aligned} X_t - 0.3 X_{t-1} - 0.1 X_{t-2} &amp;= W_t + W_{t-1} + 0.16 W_{t-2}\\\\ (1 - 0.3 B - 0.1 B^2 ) X_t &amp;= (1 + B + 0.16B^2) W_t\\\\ (1 + 0.2B)(1 - 0.5 B) X_t &amp;= (1 + 0.2B)(1 + 0.8 B) W_t\\\\ (1 - 0.5 B) X_t &amp;= (1 + 0.8 B) W_t\\\\ X_t &amp;= 0.5 X_{t-1} + W_t - 0.8 W_{t-1}. \\end{aligned} \\end{equation*}\\] Therefore, the initial ARMA(2,2) model is in fact an ARMA(1,1) model. Note that the latter model is causal (as \\(|\\phi|&lt;1\\)) and invertible (as \\(|\\theta| &lt; 1\\)) and therefore respects the conditions we highlighted at the beginning of this section. As a result of this, it is important to define (and estimate) ARMA models that are not an over-parametrization of a more “simple” ARMA model. 5.1.1 Autocovariance of ARMA Models Since ARMA(\\(p\\),\\(q\\)) models are a class of models that inherits much of the properties of AR(\\(p\\)) and MA(\\(q\\)) models, as for their properties in terms of stationarity and identifiability, they also inherit much of the characteristics of the previously defined models in terms of their autocovariance (autocorrelation) functions. However, while the analytical derivation of the autocovariance functions for AR(\\(p\\)) and MA(\\(q\\)) models was a relatively feasible task, this is less the case for ARMA models. Example 4.4 (Autocorrelation of an ARMA(1,1)) For an ARMA(1,1) we have \\[\\rho(h) = \\phi^{h-1} \\rho(1)\\] where \\[\\begin{equation*} \\rho(1) = \\frac{\\left(1+\\theta \\phi \\right)\\left(\\theta + \\phi\\right)}{1+2\\phi\\theta+\\theta^2}. \\end{equation*}\\] From the above example we can observe how for the lowest order ARMA model the autocorrelation function assumes a much more complex structure compared to those presented in the previous sections for AR(\\(p\\)) and MA(\\(q\\)) models. This is even more the case for higher order ARMA models and, for this reason, aside from brute-force calculation of the ACF for specific ARMA models, different software make these computations based on user-provided parameter values. Aside from the actual derivation of the ACF (and PACF) of ARMA models, another aspect of these models that cannot be directly transferred from the AR(\\(p\\)) and MA(\\(q\\)) models described earlier is the ease of interpretation of their ACF and PACF plots. Indeed, as we saw in the previous sections, the ACF and PACF plots of AR(\\(p\\)) and MA(\\(q\\)) models are quite informative in terms of understanding which of the two classes of models an observed time series could have been generated from and, once this has been decided, which order (\\(p\\) or \\(q\\)) is the most appropriate. However, this is not the case anymore for ARMA(\\(p\\),\\(q\\)) models since they’re a result of the combination of the two previous classes and it’s (almost) impossible to distinguish which features of the ACF and PACF belong to which part of the ARMA(\\(p\\),\\(q\\)) model. The table below summarizes the characteristics of the ACF and PACF plots for the three classes of models discussed this far. (#acfpacf) Features of the ACF and PACF plots for AR(\\(p\\)), MA(\\(q\\)) and ARMA(\\(p\\),\\(q\\)) models. AR(\\(p\\)) MA(\\(q\\)) ARMA(\\(p\\),\\(q\\)) ACF Tails off Cuts off after lag \\(q\\) Tails off PACF Cuts off after lag \\(p\\) Tails off Tails off As highlighted in the table, the first two classes have distinctive features to them while the class of ARMA(\\(p\\),\\(q\\)) models have similar characteristics for both ACF and PACF plots. To better illustrate these concepts, let us consider the following two ARMA(\\(p\\),\\(q\\)) models: \\[ \\begin{aligned} X_t &amp;= 0.3 X_{t-1} - 0.8 X_{t-2} + 0.7 W_{t-1} + W_t \\\\ Y_t &amp;= 1.2 X_{t-1} - 0.25 X_{t-2} - 0.1 W_{t-1} - 0.75 W_{t-2} + W_t, \\\\ \\end{aligned} \\] where \\(W_t \\overset{iid}{\\sim} \\mathcal{N}(0, 1)\\). The first model is an ARMA(2,1) model while the second is an ARMA(2,2) model and, as can be seen, the parameters between these two models are quite different. The figures below show a realization from each of these two models for time series of length \\(T=500\\). Xt = gen_gts(n = 500, ARMA(ar = c(0.3,-0.8), ma = 0.7, sigma2 = 1)) Yt = gen_gts(n = 500, ARMA(ar = c(1.2, -0.25), ma = c(-0.1, -0.75), sigma2 = 1)) par(mfrow = c(2,1)) plot(Xt, main = expression(&quot;Simulated process: &quot;* X[t] *&quot; with &quot;* T *&quot; = 500&quot;)) plot(Yt, main = expression(&quot;Simulated process: &quot;* Y[t] *&quot; with &quot;* T *&quot; = 500&quot;)) Figure 4.1: Plots of simulated time series from an ARMA(2,1) model (top plot) and from an ARMA(2,2) model (bottom plot) As for the previous classes of models, a direct plot of the time series doesn’t necessarily allow to acquire much information on the possible models underlying the data. Let us consequently consider the theoretical ACF and PACF plots of these two models to understand if we can obtain more information. par(mfrow = c(1,2)) plot(theo_acf(ar = c(0.3,-0.8), ma = 0.7), main = expression(&quot;Theo. ACF - MA(1); &quot;* theta *&quot; = 0.7&quot;)) plot(theo_pacf(ar = c(0.3,-0.8), ma = 0.7), main = expression(&quot;Theo. PACF - MA(1); &quot;* theta *&quot; = 0.7&quot;)) Figure 4.2: Theoretical ACF (left plot) and PACF (right plot) for the ARMA(2,1) model defined in the text. par(mfrow = c(1,2)) plot(theo_acf(ar = c(1.2, -0.25), ma = c(-0.1, -0.75)), main = expression(&quot;Theo. ACF - MA(2); &quot;* theta *&quot; = -0.75&quot;)) plot(theo_pacf(ar = c(1.2, -0.25), ma = c(-0.1, -0.75)), main = expression(&quot;Theo. PACF - MA(2); &quot;* theta *&quot; = -0.75&quot;)) Figure 4.3: Theoretical ACF (left plot) and PACF (right plot) for the ARMA(2,2) model defined in the text. As can be seen from the ACF and PACF plots of these two models, it would be complicated to determine what type of model and what order would be more appropriate. For this reason, for ARMA(\\(p\\),\\(q\\)) models the ACF and PACF plots can only help to understand if the autocovariance has a reasonable behaviour (e.g. tails off as the lags increase) or, for example, if there appears to be some deterministic trends and/or seasonalities are left in the data. Therefore, a more reasonable (and simple) approach to understand if an ARMA(\\(p\\),\\(q\\)) is appropriate is via a model selection criterion or an estimator of the out-of-sample predicition error (e.g. MAPE). Once this is done, the selected model(s) can fit to the observed time series and a residual analysis can be performed in order to understand if the chosen model(s) appear to do a reasonable job. Let us illustrate the above procedure through another applied example. The time series shown in the figure below represents the annual copper prices from 1800 to 1997 (with the long-term trend removed) which can be downloaded in R via the rdatamarket library. Xt = gts(as.numeric(dmseries(&quot;https://datamarket.com/data/set/22y1/annual-copper-prices-1800-1997#!ds=22y1&amp;display=line&quot;)), start = 1921, freq = 12, name_ts = &quot;Copper prices (minus the long-term trend)&quot;, data_name = &quot;Copper Price&quot;, name_time = &quot;&quot;) plot(Xt) Figure 4.4: TO DO It would appear that the process could be considered as being stationary and, given this, let us analyse the estimated ACF and PACF plots for the considered time series. corr_analysis(Xt) Figure 4.5: Empirical ACF (left) and PACF (right) of the annual copper prices. The ACF plot could eventually suggest an AR(\\(p\\)) model, however the PACF plot doesn’t appear to deliver an exact cut-off and both appear to tail-off with no obvious behaviour that would allow to assign them to an AR(\\(p\\)) model or MA(\\(q\\)) model. Let us therefore consider all possible models included in an ARMA(4,5) model (which therefore include all possible model in an AR(4) model and in an MA(5) model). The figure below shows the behaviour (and minima) of the three selection criteria discussed earlier in this chapter where each plot fixes the value of \\(q\\) (for the MA(\\(q\\)) part of the model) and explores the value of these criteria for different orders \\(p\\) (for the AR(\\(p\\)) part of the model). select(ARMA(4,5), Xt) Figure 4.6: Empirical ACF (left) and PACF (right) of the Copper time series data. From the selection procedure it would appear that the BIC criterion selects a simple AR(1) model for the annual copper time series while the AIC selects an ARMA(3,5) model. This reflects the properties of these two criteria since the BIC usually selects lower order models (e.g. it can under-fit the data) while the AIC usually does the opposite (e.g. it can over-fit the data). As expected, the HQ criterion lies somewhere inbetween the two previous criteria and selects an ARMA(3,2) model. In order to decide which of these models to use in order to describe or predict the time series, one could estimate the MAPE for these three selected models. ## MAPE SD ## AR(1) 0.3375733 0.06285691 ## ARMA(3,2) 0.3577872 0.04619443 ## ARMA(3,5) 0.4471867 0.05205741 ## ## MAPE suggests: AR(1) evaluate(list(AR(1), ARMA(3,2), ARMA(3, 5)), Xt, criterion = &quot;MAPE&quot;, start = 0.5) Based on the MAPE, we would probably choose the AR(1) model but, considering the standard deviations of the empirical MAPE, the ARMA(3,2) appears to be close enough candidate model. To obtain further information to choose a final model, one could check the behaviour of the residuals from these two models. model_copper_ar1 = estimate(AR(1), Xt) check(model_copper_ar1) Figure 4.7: Diagnostic plots for the residuals of a fitted AR(1) to the annual copper prices. model_copper_arma32 = estimate(ARMA(3,2), Xt) check(model_copper_arma32) Figure 4.8: Diagnostic plots for the residuals of a fitted AR(1) to the annual copper prices. From the two diagnostic plots we can see that the residuals from the AR(1) appear to preserve a certain degree of dependence (aside from large deviations from the normality assumption which, nevertheless, is not necessarily required). On the other hand, the residuals from the ARMA(3,2) have an overall better behaviour and, if one were to choose a unique model for this data, the ARMA(3,2) model would appear to be a good candidate. "],
["arima.html", "5.2 ARIMA", " 5.2 ARIMA "],
["sarima.html", "5.3 SARIMA", " 5.3 SARIMA 5.3.1 Real data example Xt = gts(as.numeric(dmseries(&quot;https://datamarket.com/data/set/22pw/monthly-lake-erie-levels-1921-1970#!ds=22pw&amp;display=line&quot;)), start = 1921, freq = 12, name_ts = &quot;Water Levels&quot;, data_name = &quot;Monthly Lake Erie Levels&quot;, name_time = &quot;&quot;) plot(Xt) Figure 4.9: TO DO corr_analysis(Xt) Figure 4.10: Empirical ACF (left) and PACF (right) of the Lake Erie time series data. Xt = gts(as.numeric(dmseries(&quot;https://datamarket.com/data/set/235j/number-of-daily-births-in-quebec-jan-01-1977-to-dec-31-1990#!ds=235j&amp;display=line&quot;)), start = 1977, freq = 365, name_ts = &quot;Number of Births&quot;, data_name = &quot;Number of Births in Quebec&quot;, name_time = &quot;&quot;) plot(Xt) Figure 4.11: TO DO corr_analysis(Xt, lag.max = 50) Figure 4.12: Empirical ACF (left) and PACF (right) of the Births time series data. mod = estimate(SARIMA(ar = 2, ma = 1, sar = 2, sma = 1, s = 7, si = 1), Xt, method = &quot;mle&quot;) check(mod) predict(mod, n.ahead = 30) "]
]
