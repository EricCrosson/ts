<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Applied Time Series Analysis with R</title>
  <meta name="description" content="Applied Time Series Analysis with R">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Applied Time Series Analysis with R" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="SMAC-Group/ts" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Applied Time Series Analysis with R" />
  
  
  

<meta name="author" content="Stéphane Guerrier, Roberto Molinari, Haotian Xu and Yuming Zhang">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  <link rel="shortcut icon" href="favicon.ico" type="image/x-icon">
<link rel="prev" href="fundtimeseries.html">
<link rel="next" href="sdzlj.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; position: absolute; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; }
pre.numberSource a.sourceLine:empty
  { position: absolute; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: absolute; left: -5em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Applied Time Series Analysis with R</a></li>

<li class="divider"></li>
<li class="part"><span><b>I Foundation</b></span></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#conventions"><i class="fa fa-check"></i><b>1.1</b> Conventions</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#bibliographic-note"><i class="fa fa-check"></i><b>1.2</b> Bibliographic Note</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i><b>1.3</b> Acknowledgements</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#license"><i class="fa fa-check"></i><b>1.4</b> License</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="basic-elements-of-time-series.html"><a href="basic-elements-of-time-series.html"><i class="fa fa-check"></i><b>2</b> Basic Elements of Time Series</a><ul>
<li class="chapter" data-level="2.1" data-path="basic-elements-of-time-series.html"><a href="basic-elements-of-time-series.html#the-wold-decomposition"><i class="fa fa-check"></i><b>2.1</b> The Wold Decomposition</a><ul>
<li class="chapter" data-level="2.1.1" data-path="basic-elements-of-time-series.html"><a href="basic-elements-of-time-series.html#the-deterministic-component-signal"><i class="fa fa-check"></i><b>2.1.1</b> The Deterministic Component (Signal)</a></li>
<li class="chapter" data-level="2.1.2" data-path="basic-elements-of-time-series.html"><a href="basic-elements-of-time-series.html#the-random-component-noise"><i class="fa fa-check"></i><b>2.1.2</b> The Random Component (Noise)</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="basic-elements-of-time-series.html"><a href="basic-elements-of-time-series.html#eda"><i class="fa fa-check"></i><b>2.2</b> Exploratory Data Analysis for Time Series</a></li>
<li class="chapter" data-level="2.3" data-path="basic-elements-of-time-series.html"><a href="basic-elements-of-time-series.html#dependence-in-time-series"><i class="fa fa-check"></i><b>2.3</b> Dependence in Time Series</a></li>
<li class="chapter" data-level="2.4" data-path="basic-elements-of-time-series.html"><a href="basic-elements-of-time-series.html#basicmodels"><i class="fa fa-check"></i><b>2.4</b> Basic Time Series Models</a><ul>
<li class="chapter" data-level="2.4.1" data-path="basic-elements-of-time-series.html"><a href="basic-elements-of-time-series.html#wn"><i class="fa fa-check"></i><b>2.4.1</b> White Noise</a></li>
<li class="chapter" data-level="2.4.2" data-path="basic-elements-of-time-series.html"><a href="basic-elements-of-time-series.html#rw"><i class="fa fa-check"></i><b>2.4.2</b> Random Walk</a></li>
<li class="chapter" data-level="2.4.3" data-path="basic-elements-of-time-series.html"><a href="basic-elements-of-time-series.html#ar1"><i class="fa fa-check"></i><b>2.4.3</b> First-Order Autoregressive Model</a></li>
<li class="chapter" data-level="2.4.4" data-path="basic-elements-of-time-series.html"><a href="basic-elements-of-time-series.html#ma1"><i class="fa fa-check"></i><b>2.4.4</b> Moving Average Process of Order 1</a></li>
<li class="chapter" data-level="2.4.5" data-path="basic-elements-of-time-series.html"><a href="basic-elements-of-time-series.html#drift"><i class="fa fa-check"></i><b>2.4.5</b> Linear Drift</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="basic-elements-of-time-series.html"><a href="basic-elements-of-time-series.html#lts"><i class="fa fa-check"></i><b>2.5</b> Composite Stochastic Processes</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="fundtimeseries.html"><a href="fundtimeseries.html"><i class="fa fa-check"></i><b>3</b> Fundamental Properties of Time Series</a><ul>
<li class="chapter" data-level="3.1" data-path="fundtimeseries.html"><a href="fundtimeseries.html#the-autocorrelation-and-autocovariance-functions"><i class="fa fa-check"></i><b>3.1</b> The Autocorrelation and Autocovariance Functions</a><ul>
<li class="chapter" data-level="3.1.1" data-path="fundtimeseries.html"><a href="fundtimeseries.html#a-fundamental-representation"><i class="fa fa-check"></i><b>3.1.1</b> A Fundamental Representation</a></li>
<li class="chapter" data-level="3.1.2" data-path="fundtimeseries.html"><a href="fundtimeseries.html#admissible-autocorrelation-functions"><i class="fa fa-check"></i><b>3.1.2</b> Admissible Autocorrelation Functions 😱</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="fundtimeseries.html"><a href="fundtimeseries.html#stationary"><i class="fa fa-check"></i><b>3.2</b> Stationarity</a><ul>
<li class="chapter" data-level="3.2.1" data-path="fundtimeseries.html"><a href="fundtimeseries.html#assessing-weak-stationarity-of-time-series-models"><i class="fa fa-check"></i><b>3.2.1</b> Assessing Weak Stationarity of Time Series Models</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="fundtimeseries.html"><a href="fundtimeseries.html#estimation-of-moments-stationary-processes"><i class="fa fa-check"></i><b>3.3</b> Estimation of Moments (Stationary Processes)</a><ul>
<li class="chapter" data-level="3.3.1" data-path="fundtimeseries.html"><a href="fundtimeseries.html#estimation-of-the-mean-function"><i class="fa fa-check"></i><b>3.3.1</b> Estimation of the Mean Function</a></li>
<li class="chapter" data-level="3.3.2" data-path="fundtimeseries.html"><a href="fundtimeseries.html#sample-autocovariance-and-autocorrelation-functions"><i class="fa fa-check"></i><b>3.3.2</b> Sample Autocovariance and Autocorrelation Functions</a></li>
<li class="chapter" data-level="3.3.3" data-path="fundtimeseries.html"><a href="fundtimeseries.html#robustness-issues"><i class="fa fa-check"></i><b>3.3.3</b> Robustness Issues</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="the-family-of-autoregressive-moving-average-models.html"><a href="the-family-of-autoregressive-moving-average-models.html"><i class="fa fa-check"></i><b>4</b> The Family of Autoregressive Moving Average Models</a><ul>
<li class="chapter" data-level="4.1" data-path="the-family-of-autoregressive-moving-average-models.html"><a href="the-family-of-autoregressive-moving-average-models.html#linear-processes"><i class="fa fa-check"></i><b>4.1</b> Linear Processes</a></li>
<li class="chapter" data-level="4.2" data-path="the-family-of-autoregressive-moving-average-models.html"><a href="the-family-of-autoregressive-moving-average-models.html#ardefinition"><i class="fa fa-check"></i><b>4.2</b> Autoregressive Models - AR(p)</a><ul>
<li class="chapter" data-level="4.2.1" data-path="the-family-of-autoregressive-moving-average-models.html"><a href="the-family-of-autoregressive-moving-average-models.html#properties-of-arp-models"><i class="fa fa-check"></i><b>4.2.1</b> Properties of AR(p) models</a></li>
<li class="chapter" data-level="4.2.2" data-path="the-family-of-autoregressive-moving-average-models.html"><a href="the-family-of-autoregressive-moving-average-models.html#estimation-of-arp-models"><i class="fa fa-check"></i><b>4.2.2</b> Estimation of AR(p) models</a></li>
<li class="chapter" data-level="4.2.3" data-path="the-family-of-autoregressive-moving-average-models.html"><a href="the-family-of-autoregressive-moving-average-models.html#forecasting-arp-models"><i class="fa fa-check"></i><b>4.2.3</b> Forecasting AR(p) Models</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="the-family-of-autoregressive-moving-average-models.html"><a href="the-family-of-autoregressive-moving-average-models.html#diagnostic-tools-for-time-series"><i class="fa fa-check"></i><b>4.3</b> Diagnostic Tools for Time Series</a><ul>
<li class="chapter" data-level="4.3.1" data-path="the-family-of-autoregressive-moving-average-models.html"><a href="the-family-of-autoregressive-moving-average-models.html#the-partial-autocorrelation-function-pacf"><i class="fa fa-check"></i><b>4.3.1</b> The Partial AutoCorrelation Function (PACF)</a></li>
<li class="chapter" data-level="4.3.2" data-path="the-family-of-autoregressive-moving-average-models.html"><a href="the-family-of-autoregressive-moving-average-models.html#portmanteau-tests"><i class="fa fa-check"></i><b>4.3.2</b> Portmanteau Tests</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="the-family-of-autoregressive-moving-average-models.html"><a href="the-family-of-autoregressive-moving-average-models.html#inference-for-arp-models"><i class="fa fa-check"></i><b>4.4</b> Inference for AR(p) Models</a></li>
<li class="chapter" data-level="4.5" data-path="the-family-of-autoregressive-moving-average-models.html"><a href="the-family-of-autoregressive-moving-average-models.html#model-selection"><i class="fa fa-check"></i><b>4.5</b> Model Selection</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="sdzlj.html"><a href="sdzlj.html"><i class="fa fa-check"></i><b>5</b> sdzlj</a><ul>
<li class="chapter" data-level="5.1" data-path="sdzlj.html"><a href="sdzlj.html#moving-average-models"><i class="fa fa-check"></i><b>5.1</b> Moving Average Models ⚠️</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="proofs.html"><a href="proofs.html"><i class="fa fa-check"></i><b>A</b> Proofs 😱</a><ul>
<li class="chapter" data-level="A.1" data-path="proofs.html"><a href="proofs.html#appendixa"><i class="fa fa-check"></i><b>A.1</b> Proof of Theorem 3.1</a></li>
<li class="chapter" data-level="A.2" data-path="proofs.html"><a href="proofs.html#appendixc"><i class="fa fa-check"></i><b>A.2</b> Proof of Theorem 4.1</a></li>
<li class="chapter" data-level="A.3" data-path="proofs.html"><a href="proofs.html#appendixoptim"><i class="fa fa-check"></i><b>A.3</b> Proof of Theorem 4.2</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="appendixb.html"><a href="appendixb.html"><i class="fa fa-check"></i><b>B</b> Robust Regression Methods</a><ul>
<li class="chapter" data-level="B.1" data-path="appendixb.html"><a href="appendixb.html#the-classical-least-squares-estimator"><i class="fa fa-check"></i><b>B.1</b> The Classical Least-Squares Estimator</a></li>
<li class="chapter" data-level="B.2" data-path="appendixb.html"><a href="appendixb.html#robust-estimators-for-linear-regression-models"><i class="fa fa-check"></i><b>B.2</b> Robust Estimators for Linear Regression Models</a></li>
<li class="chapter" data-level="B.3" data-path="appendixb.html"><a href="appendixb.html#applications-of-robust-estimation"><i class="fa fa-check"></i><b>B.3</b> Applications of Robust Estimation</a></li>
</ul></li>
<li class="chapter" data-level="C" data-path="proofs.html"><a href="proofs.html#appendixc"><i class="fa fa-check"></i><b>C</b> Proofs</a></li>
<li class="divider"></li>
<li><a href="https://github.com/SMAC-Group/ts" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Applied Time Series Analysis with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="the-family-of-autoregressive-moving-average-models" class="section level1">
<h1><span class="header-section-number">Chapter 4</span> The Family of Autoregressive Moving Average Models</h1>
<blockquote>
<p>“<em>Essentially, all models are wrong, but some are useful</em>”, George Box</p>
</blockquote>
<p>In this chapter we introduce a class of time series models that is considerably flexible and among the most commonly used to describe stationary time series. This class is represented by the Seasonal AutoRegressive Integrated Moving Average (SARIMA) models which, among others, combine and include the autoregressive and moving average models seen in the previous chapter. To introduce this class of models, we start by describing a sub-class called AutoRegressive Moving Average (ARMA) models which represent the backbone on which the SARIMA class is built. The importance of ARMA models resides in their flexibility as well as their capacity of describing (or closely approximating) almost all the features of a stationary time series. The autoregressive parts of these models describe how consecutive observations in time influence each other while the moving average parts capture some possible unobserved shocks thereby allowing to model different phenomena which can be observed in various fields going from biology to finance.</p>
<p>With this premise, the first part of this chapter introduces and explains the class of ARMA models in the following manner. First of all we will discuss the class of linear processes, which ARMA models belong to, and we will then proceed to a detailed description of autoregressive models in which we review their definition, explain their properties, introduce the main estimation methods for their parameters and highlight the diagnostic tools which can help understand if the estimated models appear to be appropriate or sufficient to well describe the observed time series. Once this is done, we will then use most of the results given for the autoregressive models to further describe and discuss moving average models, for which we underline the property of invertibility, and finally the ARMA models. Indeed, the properties and estimation methods for the latter class are directly inherited from the discussions on the autoregressive and moving average models.</p>
<p>The second part of this chapter introduces the general class of SARIMA models, passing through the class of ARIMA models. These models allow to apply the ARMA modeling framework also to time series that have particular non-stationary components to them such as, for example, linear and/or seasonal trends. Extending ARMA modeling to these cases allows SARIMA models to be an extremely flexible class of models that can be used to describe a wide range of phenomena.</p>
<div id="linear-processes" class="section level2">
<h2><span class="header-section-number">4.1</span> Linear Processes</h2>
<p>In order to discuss the classes of models mentioned above, we first present the class of linear processes which underlie many of the most common time series models.</p>

<div class="definition">
<p><span id="def:lp" class="definition"><strong>Definition 4.1  (Linear Process)  </strong></span>A time series, <span class="math inline">\((X_t)\)</span>, is defined to be a linear process if it can be expressed
as a linear combination of white noise as follows:</p>
<p><span class="math display">\[{X_t} = \mu + \sum\limits_{j =  - \infty }^\infty  {{\psi _j}{W_{t - j}}} \]</span></p>
where <span class="math inline">\(W_t \sim \mathcal{N}(0, \sigma^2)\)</span> and <span class="math inline">\(\sum\limits_{j = - \infty }^\infty {\left| {{\psi _j}} \right|} &lt; \infty\)</span>.
</div>

<p><br></p>
<p>Note, the latter assumption is required to ensure that the series has a limit. Furthermore, the set of coefficients <span class="math display">\[{( {\psi _j}) _{j =  - \infty , \cdots ,\infty }}\]</span> can be viewed as a linear filter. These coefficients do not have to be all equal nor symmetric as later examples will show. Generally, the properties of a linear process related to mean and variance are given by:</p>
<p><span class="math display">\[\begin{aligned}
\mu_{X} &amp;= \mu \\
\gamma_{X}(h) &amp;= \sigma _W^2\sum\limits_{j =  - \infty }^\infty  {{\psi _j}{\psi _{h + j}}}
\end{aligned}\]</span></p>
<p>The latter is derived from</p>
<p><span class="math display">\[\begin{aligned}
  \gamma \left( h \right) &amp;= Cov\left( {{x_t},{x_{t + h}}} \right) \\
   &amp;= Cov\left( {\mu  + \sum\limits_{j =  - \infty }^\infty  {{\psi _j}{w_{t - j}}} ,\mu  + \sum\limits_{j =  - \infty }^\infty  {{\psi _j}{w_{t + h - j}}} } \right) \\
   &amp;= Cov\left( {\sum\limits_{j =  - \infty }^\infty  {{\psi _j}{w_{t - j}}} ,\sum\limits_{j =  - \infty }^\infty  {{\psi _j}{w_{t + h - j}}} } \right) \\
   &amp;= \sum\limits_{j =  - \infty }^\infty  {{\psi _j}{\psi _{j + h}}Cov\left( {{w_{t - j}},{w_{t - j}}} \right)}  \\
   &amp;= \sigma _w^2\sum\limits_{j =  - \infty }^\infty  {{\psi _j}{\psi _{j + h}}}  \\ 
\end{aligned} \]</span></p>
<p>Within the above derivation, the key is to realize that
<span class="math inline">\(Cov\left( {{w_{t - j}},{w_{t + h - j}}} \right) = 0\)</span> if <span class="math inline">\(t - j \ne t + h - j\)</span>.</p>
<p>Lastly, another convenient way to formalize the definition of a linear process is through the use of the <strong>backshift operator</strong> (or lag operator) which is itself defined as follows:</p>
<p><span class="math display">\[B\,X_t = X_{t-1}.\]</span></p>
<p>The properties of the backshift operator allow us to create composite functions of the type</p>
<p><span class="math display">\[B^2 \, X_t = B (B \, X_t) = B \, X_{t-1} = X_{t-2}\]</span>
which allows to generalize as follows</p>
<p><span class="math display">\[B^k \, X_t = X_{t-k}.\]</span>
Moreover, we can apply the inverse operator to it (i.e. <span class="math inline">\(B^{-1} \, B = 1\)</span>) thereby allowing us to have, for example:</p>
<p><span class="math display">\[X_t = B^{-1} \, B X_t = B^{-1} X_{t-1}\]</span></p>

<div class="example">
<span id="exm:backdiff" class="example"><strong>Example 4.1  (d-order Differences)  </strong></span>We can re-express <span class="math inline">\(X_t - X_{t-1}\)</span> as
<span class="math display">\[\delta X_t = (1 - B) X_t\]</span>
or a second order difference as
<span class="math display">\[\delta^2 X_t = (1 - B)^2 X_t\]</span>
thereby generalizing to a d-order difference as follows:
<span class="math display">\[\delta^d X_t = (1 - B)^d X_t.\]</span>
</div>

<p><br></p>
<p>Having defined the backshift operator, we can now provide an alternative definition of a linear process as follows:</p>
<p><span class="math display">\[{X_t} = \mu + \psi \left( B \right){W_t}\]</span></p>
<p>where <span class="math inline">\(\psi ( B )\)</span> is a polynomial function in <span class="math inline">\(B\)</span> whose coefficients are given by the linear filters <span class="math inline">\((\psi_j)\)</span> (we’ll describe these polynomials further on).</p>

<div class="example">
<p><span id="exm:lpwn" class="example"><strong>Example 4.2  (Linear Process of White Noise)  </strong></span>
The white noise process <span class="math inline">\((X_t)\)</span>, defined in <a href="basic-elements-of-time-series.html#wn">2.4.1</a>,
can be expressed as a linear process as follows:</p>
<p><span class="math display">\[\psi _j = \begin{cases}
      1 , &amp;\mbox{ if } j = 0\\
      0 , &amp;\mbox{ if } |j| \ge 1
\end{cases}.\]</span></p>
<p>and <span class="math inline">\(\mu = 0\)</span>.</p>
Therefore, <span class="math inline">\(X_t = W_t\)</span>, where <span class="math inline">\(W_t \sim WN(0, \sigma^2_W)\)</span>
</div>

<p><br></p>

<div class="example">
<p><span id="exm:lpma1" class="example"><strong>Example 4.3  (Linear Process of Moving Average Order 1)  </strong></span>
Similarly, consider <span class="math inline">\((X_t)\)</span> to be a MA(1) process,
given by <a href="basic-elements-of-time-series.html#ma1">2.4.4</a>. The process can be expressed linearly through the following filters:</p>
<p><span class="math display">\[\psi _j = \begin{cases}
      1, &amp;\mbox{ if } j = 0\\
      \theta , &amp;\mbox{ if } j = 1 \\
      0, &amp;\mbox{ if } j \ge 2
\end{cases}.\]</span></p>
<p>and <span class="math inline">\(\mu = 0\)</span>.</p>
Thus, we have: <span class="math inline">\(X_t = W_t + \theta W_{t-1}\)</span>
</div>

<p><br></p>

<div class="example">
<p><span id="exm:lpsma" class="example"><strong>Example 4.4  (Linear Process and Symmetric Moving Average)  </strong></span>
Consider a symmetric moving average given by:</p>
<p><span class="math display">\[{X_t} = \frac{1}{{2q + 1}}\sum\limits_{j =  - q}^q {{W_{t + j}}} \]</span></p>
<p>Thus, <span class="math inline">\((X_t)\)</span> is defined for <span class="math inline">\(q + 1 \le t \le n-q\)</span>. The above process
would be a linear process since:</p>
<p><span class="math display">\[\psi _j = \begin{cases}
      \frac{1}{{2q + 1}} , &amp;\mbox{ if } -q \le j \le q\\
      0 , &amp;\mbox{ if } |j| &gt; q
\end{cases}.\]</span></p>
<p>and <span class="math inline">\(\mu = 0\)</span>.</p>
<p>In practice, if <span class="math inline">\(q = 1\)</span>, we would have:</p>
<span class="math display">\[{X_t} = \frac{1}{3}\left( {{W_{t - 1}} + {W_t} + {W_{t + 1}}} \right)\]</span>
</div>

<p><br></p>

<div class="example">
<p><span id="exm:lpar1" class="example"><strong>Example 4.5  (Autoregressive Process of Order 1)  </strong></span>If <span class="math inline">\(\left\{X_t\right\}\)</span> follows an AR(1) model defined in <a href="basic-elements-of-time-series.html#ar1">2.4.3</a>, the linear filters are a function of the time lag:</p>
<p><span class="math display">\[\psi _j = \begin{cases}
      \phi^j , &amp;\mbox{ if } j \ge 0\\
      0 , &amp;\mbox{ if } j &lt; 0
\end{cases}.\]</span></p>
and <span class="math inline">\(\mu = 0\)</span>. We would require the condition that <span class="math inline">\(\left| \phi \right| &lt; 1\)</span> in order to respect the condition on the filters (i.e. <span class="math inline">\(\sum\limits_{j = - \infty }^\infty {\left| {{\psi _j}} \right|} &lt; \infty\)</span>).
</div>

</div>
<div id="ardefinition" class="section level2">
<h2><span class="header-section-number">4.2</span> Autoregressive Models - AR(p)</h2>
<p>The class of autoregressive models is based on the idea that previous values in the time series are needed to explain current values in the series. For this class of models, we assume that the <span class="math inline">\(p\)</span> previous observations are needed for this purpose and we therefore denote this class as AR(<span class="math inline">\(p\)</span>). In the previous chapter, the model we introduced was an AR(1) in which only the immediately previous observation is needed to explain the following one and therefore represents a particular model which is part of the more general class of AR(<span class="math inline">\(p\)</span>) models.</p>

<div class="definition">
<span id="def:arp" class="definition"><strong>Definition 4.2  (Autoregressive Models of Order p)  </strong></span>The AR(p) models can be formally represented as follows
<span class="math display">\[(X_t) = {\phi_1}{X_{t - 1}} + ... + {\phi_p}{X_{t - p}} + {W_t},\]</span>
where <span class="math inline">\(\phi_i \neq 0\)</span> (for <span class="math inline">\(i = 1, ..., p\)</span>) and <span class="math inline">\(W_t\)</span> is a (Gaussian) white noise process with variance <span class="math inline">\(\sigma^2\)</span>.
</div>

<p>As earlier in this book, we will assume that the expectation of the process <span class="math inline">\(({X_t})\)</span>, as well as that of the following ones in this chapter, is zero. The reason for this simplification is that if <span class="math inline">\(\mathbb{E} [ X_t ] = \mu\)</span>, we can define an AR process <em>around</em> <span class="math inline">\(\mu\)</span> as follows:</p>
<p><span class="math display">\[X_t - \mu = \sum_{i = 1}^p \phi_i \left(X_{t-i} - \mu \right) + W_t,\]</span></p>
<p>which is equivalent to</p>
<p><span class="math display">\[X_t  = \mu^{\star} +  \sum_{i = 1}^p \phi_i X_{t-i}  + W_t,\]</span></p>
<p>where <span class="math inline">\(\mu^{\star} = \mu (1 - \sum_{i = 1}^p \phi_i)\)</span>. Therefore, to simplify the notation we will generally consider only zero mean processes, since adding means (as well as other deterministic trends) is easy.</p>
<p>A useful way of representing AR(<span class="math inline">\(p\)</span>) processes is through the backshift operator introduced in the previous section and is as follows</p>
<p><span class="math display">\[\begin{aligned}
  {X_t} &amp;= {\phi_1}{X_{t - 1}} + ... + {\phi_p}{X_{t - p}} + {W_t} \\
   &amp;= {\phi_1}B{X_t} + ... + {\phi_p}B^p{X_t} + {W_t} \\
   &amp;= ({\phi_1}B + ... + {\phi_p}B^p){X_t} + {W_t} \\ 
\end{aligned},\]</span></p>
<p>which finally yields</p>
<p><span class="math display">\[(1 - {\phi _1}B - ... - {\phi_p}B^p){X_t} = {W_t},\]</span></p>
<p>which, in abbreviated form, can be expressed as</p>
<p><span class="math display">\[\phi(B){X_t} = W_t.\]</span></p>
<p>We will see that <span class="math inline">\(\phi(B)\)</span> is important to establish the stationarity of these processes and is called the <em>autoregressive</em> operator. Moreover, this quantity is closely related to another important property of AR(p) processes called <em>causality</em>. Before formally defining this new property we consider the following example which provides an intuitive illustration of its importance.</p>
<p><strong>Example:</strong> Consider a classical AR(1) model with <span class="math inline">\(|\phi| &gt; 1\)</span>. Such a model could be expressed as</p>
<p><span class="math display">\[X_t = \phi^{-1} X_{t+1} - \phi^{-1} W_t = \phi^{-k} X_{t+k} - \sum_{i = 1}^{k-1} \phi^{-i} W_{t+i}.\]</span></p>
<p>Since <span class="math inline">\(|\phi| &gt; 1\)</span>, we obtain</p>
<p><span class="math display">\[X_t = - \sum_{j = 1}^{\infty} \phi^{-j} W_{t+j},\]</span></p>
<p>which is a linear process and therefore is stationary. Unfortunately, such a model is useless because we need the future to predict the future. These processes are called non-causal.</p>
<div id="properties-of-arp-models" class="section level3">
<h3><span class="header-section-number">4.2.1</span> Properties of AR(p) models</h3>
<p>In this section we will describe the main property of the AR(p) model which has already been mentioned in the previous paragraphs and therefore let us now introduce the property of causality in a more formal manner.</p>
<p><strong>Definition:</strong> An AR(p) model is <em>causal</em> if the time series <span class="math inline">\((X_t)_{-\infty}^{\infty}\)</span> can be written as a one-sided linear process:
<span class="math display" id="eq:causal">\[\begin{equation}
    X_t = \sum_{j = 0}^{\infty} \psi_j W_{t-j} = \frac{1}{\phi(B)} W_t = \psi(B) W_t,
\tag{4.1}
\end{equation}\]</span>
where <span class="math inline">\(\phi(B) = \sum_{j = 0}^{\infty} \phi_j B^j\)</span>, and <span class="math inline">\(\sum_{j=0}^{\infty}|\phi_j| &lt; \infty\)</span> and setting <span class="math inline">\(\phi_0 = 1\)</span>.</p>
<p>As discussed earlier this condition implies that only the past values of the time series can explain the future values of it and not viceversa. Moreover, given the expression of the linear filters given by
<span class="math display">\[\frac{1}{\phi(B)}\]</span>
it is obvious that a solution exists only when <span class="math inline">\(\phi(B) = \sum_{j = 0}^{\infty} \phi_j B^j \neq 0\)</span> (thereby implying causality). A condition for this to be respected is for the roots of <span class="math inline">\(\phi(B) = 0\)</span> to lie outside the unit circle.</p>
<!-- However, it might be difficult and not obvious to show the causality of an AR(p) process by using the above definitions directly, thus the following properties are useful in practice.  -->
<!-- **Causality** -->
<!-- If an AR(p) model is causal, then the coefficients of the one-sided linear process given in \@ref(eq:causal) can be obtained by solving -->
<!-- \begin{equation*} -->
<!--     \psi(z) = \frac{1}{\sum_{j=0}^{\infty} \phi_j z^j} = \frac{1}{\phi(z)}, \mbox{ } |z| \leq 1. -->
<!-- \end{equation*} -->
<!-- It can be seen how there is no solution to the above equation if $\phi(z) = 0$ and therefore an AR(p) is causal if and only if $\phi(z) \neq 0$. A condition for this to be respected is for the roots of $\phi(z) = 0$ to lie outside the unit circle. -->

<div class="example">
<p><span id="exm:AR2asLP" class="example"><strong>Example 4.6  (Transform an AR(2) into a Linear Process)  </strong></span>Consider an AR(2) process <span class="math display">\[X_t = 1.3 X_{t-1} - 0.4 X_{t-2} + W_t,\]</span> which we would like to transform into a linear process. This can be done using the following approach:</p>
<ul>
<li><p>Step 1:
The autoregressive operator of this model can be expressed as
<span class="math display">\[
\phi(B) = 1-1.3B+0.4B^2 = (1-0.5B)(1-0.8B),
\]</span>
and has roots 2 and 1.25, both <span class="math inline">\(&gt;1\)</span>. Thus, we should be able to convert it into a linear process.</p></li>
<li><p>Step 2: We know that if an AR(p) process has all its roots outside the unit circle, then we can write <span class="math inline">\(X_t = \frac{1}{\phi(B)} W_t\)</span>. By applying the partial fractions trick, we can inverse the autoregressive operator <span class="math inline">\(\phi(B)\)</span> as follows:
<span class="math display">\[ \begin{aligned}
\phi^{-1}(B) &amp;= \frac{1}{(1-0.5B)(1-0.8B)} = \frac{c_1}{(1-0.5B)} + \frac{c_2}{(1-0.8B)} \\
&amp;= \frac{c_2(1-0.5B) + c_1(1-0.8B)}{(1-0.5B)(1-0.8B)} = \frac{(c_1 + c_2)-(0.8c_1+0.5c_2)B}{(1-0.5B)(1-0.8B)}.
\end{aligned} \]</span></p></li>
</ul>
<p>To solve for <span class="math inline">\(c_1\)</span> and <span class="math inline">\(c_2\)</span>:
<span class="math display">\[ \begin{cases}
      c_1 + c_2 &amp;=1\\
      0.8c_1+0.5c_2 &amp;=0
\end{cases} \to 
\begin{cases}
      c_1 &amp;= -5/3\\
      c_2 &amp;= 8/3.
\end{cases} \]</span></p>
<p>So we obtain
<span class="math display">\[
\phi^{-1}(B) = \frac{-5}{3(1-0.5B)} + \frac{8}{3(1-0.8B)}.
\]</span></p>
<ul>
<li>Step 3: Using the Geometric series, i.e. <span class="math inline">\(a\sum_{j=0}^{\infty} r^j = \frac{a}{1-r}\)</span> if <span class="math inline">\(|r| &lt;1\)</span>, we have
<span class="math display">\[ \begin{cases}
    \frac{-5}{3(1-0.5B)} = -\frac{5}{3} \sum_{j=0}^\infty 0.5^j B^j, &amp;\mbox{ if } |B| &lt; 2 \\
    \frac{8}{3(1-0.8B)} = \frac{8}{3} \sum_{j=0}^\infty 0.8^j B^j, &amp;\mbox{ if } |B| &lt; 1.25.
\end{cases} \]</span></li>
</ul>
<p>So we can express <span class="math inline">\(\phi^{-1}(B)\)</span> as
<span class="math display">\[
\phi^{-1}(B) = \sum_{j=0}^\infty \Big[ -\frac{5}{3} (0.5)^j  + \frac{8}{3} (0.8)^j \Big] B^j, \;\;\; \text{if  } |B|&lt;1.25.
\]</span></p>
<ul>
<li>Step 4: Finally, we obtain
<span class="math display">\[ \begin{aligned}
X_t &amp;= \phi(B)^{-1} W_t = \sum_{j=0}^\infty \Big[ -\frac{5}{3} (0.5)^j  + \frac{8}{3} (0.8)^j \Big] B^j W_t \\
&amp;= \sum_{j=0}^\infty \Big[ -\frac{5}{3} (0.5)^j  + \frac{8}{3} (0.8)^j \Big] W_{t-j},
\end{aligned} \]</span>
which verifies that the AR(2) is causal, and therefore is stationary.
</div></li>
</ul>

<div class="example">
<p><span id="exm:AR2causalcond" class="example"><strong>Example 4.7  (Causal Conditions for an AR(2) Process)  </strong></span>We already know that an AR(1) is causal with the simple condition <span class="math inline">\(|\phi_1|&lt;1\)</span>. It seems natural to believe that an AR(2) should be causal (and therefore stationary) with the condition that <span class="math inline">\(|\phi_i| &lt;1, \; i=1,2\)</span>. However, this is actually not the case as we illustrate below.</p>
<p>We can express an AR(2) process as
<span class="math display">\[
X_t = \phi_1 X_{t-1} + \phi_2 X_{t-2} + W_t = \phi_1 BX_t + \phi_2 B^2 X_t + W_t,
\]</span>
thereby delivering the following autoregressive operator:
<span class="math display">\[
\phi(B) = 1-\phi_1 B - \phi_2 B^2 = \Big( 1-\frac{B}{\lambda_1} \Big) \Big( 1-\frac{B}{\lambda_2} \Big)
\]</span>
where <span class="math inline">\(\lambda_1\)</span> and <span class="math inline">\(\lambda_2\)</span> are the roots of <span class="math inline">\(\phi(B)\)</span> such that
<span class="math display">\[ \begin{aligned}
\phi_1 &amp;=  \frac{1}{\lambda_1} + \frac{1}{\lambda_2}, \\
\phi_2 &amp;= - \frac{1}{\lambda_1} \frac{1}{\lambda_2}.
\end{aligned} \]</span></p>
<p>That is,
<span class="math display">\[\begin{aligned}
\lambda_1 &amp;= \frac{\phi_1 + \sqrt{\phi_1^2 + 4\phi_2}}{-2\phi_2}, \\
\lambda_2 &amp;= \frac{\phi_1 - \sqrt{\phi_1^2 + 4\phi_2}}{-2\phi_2}.
\end{aligned} \]</span></p>
<p>In order to ensure the causality of the model, we need the roots of <span class="math inline">\(\phi(B)\)</span>, i.e. <span class="math inline">\(\lambda_1\)</span> and <span class="math inline">\(\lambda_2\)</span>, to lie outside the unit circle.</p>
<p><span class="math display">\[ \begin{cases}
|\lambda_1| &amp;&gt; 1 \\
|\lambda_2| &amp;&gt; 1,
\end{cases} \]</span>
if and only if
<span class="math display">\[ \begin{cases}
\phi_1 + \phi_2 &amp;&lt; 1 \\
\phi_2 - \phi_1 &amp;&lt; 1 \\
|\phi_2| &amp;&lt;1.
\end{cases} \]</span></p>
<p>We can show the <em>if</em> part of the statement as follows:
<span class="math display">\[ \begin{aligned}
&amp; \phi_1 + \phi_2 = \frac{1}{\lambda_1} + \frac{1}{\lambda_2} - \frac{1}{\lambda_1 \lambda_2} = \frac{1}{\lambda_1} \Big(1-\frac{1}{\lambda_2} \Big) + \frac{1}{\lambda_2} &lt; 1 - \frac{1}{\lambda_2} + \frac{1}{\lambda_2} = 1 \;\; \text{since } 1-\frac{1}{\lambda_2} &gt; 0, \\
&amp; \phi_2 - \phi_1 = -\frac{1}{\lambda_1 \lambda_2} - \frac{1}{\lambda_1} - \frac{1}{\lambda_2} = -\frac{1}{\lambda_1} \Big( \frac{1}{\lambda_2} +1 \Big) - \frac{1}{\lambda_2} &lt; \frac{1}{\lambda_2}+1-\frac{1}{\lambda_2} = 1 \;\; \text{since } \frac{1}{\lambda_2}+1 &gt; 0, \\
&amp; |\phi_2| = \frac{1}{|\lambda_1| |\lambda_2|} &lt; 1.
\end{aligned} \]</span></p>
<p>We can also show the <em>only if</em> part of the statement as follows:</p>
<p>Since <span class="math inline">\(\lambda_1 = \frac{\phi_1 + \sqrt{\phi_1^2 + 4\phi_2}}{-2\phi_2}\)</span> and <span class="math inline">\(\phi_2 - 1 &lt; \phi_1 &lt; 1- \phi_2\)</span>, we have
<span class="math display">\[
\lambda_1^2 = \frac{(\phi_1 + \sqrt{\phi_1^2 + 4\phi_2})^2}{4\phi_2^2} &lt; \frac{\Big( (1-\phi_2)+ \sqrt{(1-\phi_2)^2 + 4\phi_2} \Big)^2}{4\phi_2^2} = \frac{4}{4\phi_2^2} \leq 1. 
\]</span></p>
<p>Since <span class="math inline">\(\lambda_2 = \frac{\phi_1 - \sqrt{\phi_1^2 + 4\phi_2}}{-2\phi_2}\)</span> and <span class="math inline">\(\phi_2 - 1 &lt; \phi_1 &lt; 1- \phi_2\)</span>, we have
<span class="math display">\[
\lambda_2^2 = \frac{(\phi_1 - \sqrt{\phi_1^2 + 4\phi_2})^2}{4\phi_2^2} &lt; \frac{\Big( (\phi_2-1)+ \sqrt{(\phi_2-1)^2 + 4\phi_2} \Big)^2}{4\phi_2^2} = \frac{4\phi_2^2}{4\phi_2^2} = 1. 
\]</span></p>
Finally, the causal region of an AR(2) is demonstrated as
</div>

<div class="figure" style="text-align: center"><span id="fig:correxample2"></span>
<img src="images/causal_AR2.png" alt="Causal Region for Parameters of an AR(2) Process" width="827" />
<p class="caption">
Figure 4.1: Causal Region for Parameters of an AR(2) Process
</p>
</div>
</div>
<div id="estimation-of-arp-models" class="section level3">
<h3><span class="header-section-number">4.2.2</span> Estimation of AR(p) models</h3>
<p>Given the above defined properties of AR(p) models, we will now discuss how these models can be estimated, more specifically how the <span class="math inline">\(p+1\)</span> parameters can be obtained from an observed time series. Indeed, a reliable estimation of these models is necessary in order to intepret and describe different natural phenomena and/or forecast possible future values of the time series.</p>
<p>A first approach builds upon the earlier definition of AR(<span class="math inline">\(p\)</span>) models being a linear process. Recall that
<span class="math display">\[\begin{equation}
    X_t = \sum_{j = 1}^{p} \phi_j X_{t-j}
\end{equation}\]</span>
which delivers the following autocovariance function
<span class="math display">\[\begin{equation}
    \gamma(h) = \text{cov}(X_{t+h}, X_t) = \text{cov}\left(\sum_{j = 1}^{p} \phi_j X_{t+h-j}, X_t\right) = \sum_{j = 1}^{p} \phi_j \gamma(h-j), \mbox{ } h \geq 1.
\end{equation}\]</span>
Rearranging the above expressions we obtain the following general equations
<span class="math display">\[\begin{equation}
    \gamma(h) - \sum_{j = 1}^{p} \phi_j \gamma(h-j) = 0, \mbox{ } h \geq 1
\end{equation}\]</span>
and, recalling that <span class="math inline">\(\gamma(h) = \gamma(-h)\)</span>,
<span class="math display">\[\begin{equation}
    \gamma(0) - \sum_{j = 1}^{p} \phi_j \gamma(j) = \sigma_w^2.
\end{equation}\]</span>
We can now define the Yule-Walker equations.</p>
<p><strong>Definition:</strong> The Yule-Walker equations are given by
<span class="math display">\[\begin{equation}
    \gamma(h) = \phi_1 \gamma(h-1) + ... + \phi_p \gamma(h-p), \mbox{ } h = 1,...,p
\end{equation}\]</span>
and
<span class="math display">\[\begin{equation}
    \sigma_w^2 = \gamma(0) - \phi_1 \gamma(1) - ... - \phi_p \gamma(p).
\end{equation}\]</span>
which in matrix notation can be defined as follows
<span class="math display">\[\begin{equation}
    \Gamma_p \mathbf{\phi} = \mathbf{\gamma}_p \,\, \text{and} \,\, \sigma_w^2 = \gamma(0) - \mathbf{\phi}&#39;\mathbf{\gamma}_p
\end{equation}\]</span>
where <span class="math inline">\(\Gamma_p\)</span> is the <span class="math inline">\(p\times p\)</span> matrix containing the autocovariances <span class="math inline">\(\gamma(k-j)\)</span>, where <span class="math inline">\(j,k = 1, ...,p\)</span>, while <span class="math inline">\(\mathbf{\phi} = (\phi_1,...,\phi_p)&#39;\)</span> and <span class="math inline">\(\mathbf{\gamma}_p = (\gamma(1),...,\gamma(p))&#39;\)</span> are <span class="math inline">\(p\times 1\)</span> vectors.</p>
<p>Considering the Yule-Walker equations, it is possible to use a method of moments approach and simply replace the theoretical quantities given in the previous definition with their empirical (estimated) counterparts that we saw in the previous chapter. This gives us the following Yule-Walker estimators
<span class="math display">\[\begin{equation}
    \hat{\mathbf{\phi}} = \hat{\Gamma}_p^{-1}\hat{\mathbf{\gamma}}_p \,\, \text{and} \,\, \hat{\sigma}_w^2 = \hat{\gamma}(0) - \hat{\mathbf{\gamma}}_p&#39;\hat{\Gamma}_p^{-1}\hat{\mathbf{\gamma}}_p .
\end{equation}\]</span></p>
<p>These estimators have the following asymptotic properties.</p>
<p><strong>Consistency and Asymptotic Normality of Yule-Walker estimators:</strong>
The Yule-Walker estimators for a causal AR(<span class="math inline">\(p\)</span>) model have the following asymptotic properties:</p>
<span class="math display">\[\begin{equation*}
\sqrt{T}(\hat{\mathbf{\phi}}- \mathbf{\phi}) \xrightarrow{\mathcal{D}} \mathcal{N}(\mathbf{0},\sigma_w^2\Gamma_p^{-1}) \,\, \text{and} \,\, \hat{\sigma}_w^2 \xrightarrow{\mathcal{P}} \sigma_w^2 .
\end{equation*}\]</span>
<p>Therefore the Yule-Walker estimators have an asymptotically normal distribution and the estimator of the innovation variance is consistent. Moreover, these estimators are also optimal for AR(<span class="math inline">\(p\)</span>) models, meaning that they are also efficient. However, there is also another method which allows to achieve this efficiency (also for general ARMA models that will be tackled further on) and this is the Maximum Likelihood Estimation (MLE) method. Considering an AR(1) model as an example, and assuming without loss of generality that its expectation is zero, we have the following representation of the AR(1) model
<span class="math display">\[\begin{equation*}
X_t = \phi X_{t-1} + W_t,
\end{equation*}\]</span>
where <span class="math inline">\(|\phi|&lt;1\)</span> and <span class="math inline">\(W_t \overset{iid}{\sim} \mathcal{N}(0,\sigma_w^2)\)</span>. Supposing we have observations <span class="math inline">\((x_t)_{t=1,...,T}\)</span> issued from this model, then the likelihood function for this setting is given by
<span class="math display">\[\begin{equation*}
L(\phi,\sigma_w^2) = f(\phi,\sigma_w^2|x_1,...,x_T)
\end{equation*}\]</span>
which, for an AR(1) model, can be rewritten as follows
<span class="math display">\[\begin{equation*}
L(\phi,\sigma_w^2) = f(x_1)f(x_2|x_1)\cdot \cdot \cdot f(x_T|x_{T-1}).
\end{equation*}\]</span>
If we define <span class="math inline">\(\Omega_t^p\)</span> as the information contained in the previous <span class="math inline">\(p\)</span> observations (before time <span class="math inline">\(t\)</span>), the above expression can be generalized for an AR(p) model as follows
<span class="math display">\[\begin{equation*}
L(\phi,\sigma_w^2) = f(x_1,...,x_p)f(x_{p+1}|\Omega_{p+1}^p)\cdot \cdot \cdot f(x_T|\Omega_{T}^p)
\end{equation*}\]</span>
where <span class="math inline">\(f(x_1,...,x_p)\)</span> is the joint probability distribution of the first <span class="math inline">\(p\)</span> observations. Going back to the AR(1) setting, based on our assumption on <span class="math inline">\((W_t)\)</span> we know that <span class="math inline">\(x_t|x_{t-1} \sim \mathcal{N}(\phi x_{t-1},\sigma_w^2)\)</span> and therefore we have that
<span class="math display">\[\begin{equation*}
f(x_t|x_{t-1}) = f_w(x_t - \phi x_{t-1})
\end{equation*}\]</span>
where <span class="math inline">\(f_w(\cdot)\)</span> is the distribution of <span class="math inline">\(W_t\)</span>. This rearranges the likelihood function as follows
<span class="math display">\[\begin{equation*}
L(\phi,\sigma_w^2) = f(x_1)\prod_{t=2}^T f_w(x_t - \phi x_{t-1}),
\end{equation*}\]</span>
where <span class="math inline">\(f(x_1)\)</span> can be found through the causal representation
<span class="math display">\[\begin{equation*}
x_1 = \sum_{j=0}^{\infty} \phi^j w_{1-j},
\end{equation*}\]</span>
which implies that <span class="math inline">\(x_1\)</span> follows a normal distribution with zero expectation and a variance given by <span class="math inline">\(\frac{\sigma_w^2}{(1-\phi^2)}\)</span>. Based on this, the likelihood function of an AR(1) finally becomes
<span class="math display">\[\begin{equation*}
L(\phi,\sigma_w^2) = (2\pi \sigma_w^2)^{-\frac{T}{2}} (1 - \phi^2)^{\frac{1}{2}} \exp \left(-\frac{S(\phi)}{2 \sigma_w^2}\right),
\end{equation*}\]</span>
with <span class="math inline">\(S(\phi) = (1-\phi^2) x_1^2 + \sum_{t=2}^T (x_t -\phi x_{t-1})^2\)</span>. Once the derivative of the logarithm of the likelihood is taken, the minimization of the negative of this function is usually done numerically. However, if we condition on the initial values, the AR(p) models are linear and, for example, we can then define the conditional likelihood of an AR(1) as
<span class="math display">\[\begin{equation*}
L(\phi,\sigma_w^2|x_1) = (2\pi \sigma_w^2)^{-\frac{T-1}{2}} \exp \left(-\frac{S_c(\phi)}{2 \sigma_w^2}\right),
\end{equation*}\]</span>
where
<span class="math display">\[\begin{equation*}
S_c(\phi) = \sum_{t=2}^T (x_t -\phi x_{t-1})^2 .
\end{equation*}\]</span>
The latter is called the conditional sum of squares and <span class="math inline">\(\phi\)</span> can be estimated as a straightforward linear regression problem. Once an estimate <span class="math inline">\(\hat{\phi}\)</span> is obtained, this can be used to obtain the conditional maximum likelihood estimate of <span class="math inline">\(\sigma_w^2\)</span>
<span class="math display">\[\begin{equation*}
\hat{\sigma}_w^2 = \frac{S_c(\hat{\phi})}{(T-1)} .
\end{equation*}\]</span></p>
<p>The estimation methods presented so far are standard for these kind of models. Nevertheless, if the data suffers from some form of contamination, these methods can become highly biased. For this reason, some robust estimators are available to limit this problematic if there are indeed outliers in the observed time series. One of these methods relies on the estimator proposed in Kunsch (1984) who underlines that the MLE score function of an AR(p) is given by
<span class="math display">\[\begin{equation*}
 \kappa(\mathbf{\theta}|x_j,...x_{j+p}) = \frac{\partial}{\partial \mathbf{\theta}} (x_{j+p} - \sum_{k=1}^p \phi_k x_{j+p-k})^2,
\end{equation*}\]</span>
where <span class="math inline">\(\theta\)</span> is the parameter vector containing, in the case of an AR(1)
model, the two parameters <span class="math inline">\(\phi\)</span> and <span class="math inline">\(\sigma_w^2\)</span>
(i.e. <span class="math inline">\(\theta = [\phi \,\, \sigma_w^2]\)</span>). This delivers the estimating equation
<span class="math display">\[\begin{equation*}
\sum_{j=1}^{n-p} \kappa (\hat{\mathbf{\theta}}|x_j,...x_{j+p}) = 0 .
\end{equation*}\]</span>
The score function <span class="math inline">\(\kappa(\cdot)\)</span> is clearly not bounded, in the sense that if
we arbitrarily move a value of <span class="math inline">\((x_t)\)</span> to infinity then the score function also
goes to infinity thereby delivering a biased estimation procedure. To avoid that
outlying observations bias the estimation excessively, a bounded score function
can be used to deliver an M-estimator given by
<span class="math display">\[\begin{equation*}
\sum_{j=1}^{n-p} \psi (\hat{\mathbf{\theta}}|x_j,...x_{j+p}) = 0,
\end{equation*}\]</span>
where <span class="math inline">\(\psi(\cdot)\)</span> is a function of bounded variation. When conditioning on the
first <span class="math inline">\(p\)</span> observations, this problem can be brought back to a linear regression
problem which can be applied in a robust manner using the robust regression
tools available in <code>R</code> such as <code>rlm</code> or <code>lmrob</code>. However, another
available tool in <code>R</code> which does not require a strict specification of the distribution function (also for general ARMA models) is the <code>method = &quot;rgmwm&quot;</code> option within the <code>estimate()</code> function (in the <code>simts</code> package). This function makes use of a quantity called the wavelet variance (denoted as <span class="math inline">\(\boldsymbol{\nu}\)</span>) which is estimated robustly and then used to retrieve the parameters <span class="math inline">\(\theta\)</span> of the time series model. The robust estimate is obtained by solving the following minimization problem
<span class="math display">\[\begin{equation*}
\hat{\boldsymbol{\theta}} = \underset{\boldsymbol{\theta} \in \boldsymbol{\Theta}}{\text{argmin}} (\hat{\boldsymbol{\nu}} - \boldsymbol{\nu}(\boldsymbol{\theta}))^T\boldsymbol{\Omega}(\hat{\boldsymbol{\nu}} - \boldsymbol{\nu}({\boldsymbol{\theta}})),
\end{equation*}\]</span>
where <span class="math inline">\(\hat{\boldsymbol{\nu}}\)</span> is the robustly estimated wavelet variance, <span class="math inline">\(\boldsymbol{\nu}({\boldsymbol{\theta}})\)</span> is
the theoretical wavelet variance (implied by the model we want to estimate) and <span class="math inline">\(\boldsymbol{\Omega}\)</span> is a positive definite weighting matrix. Below we show some simulation studies where we present the results of the above estimation procedures in absence and in presence of contamination in the data. As a reminder, so far we have mainly discussed three estimators for the parameters of AR(<span class="math inline">\(p\)</span>) models (i.e. Yule-Walker, maximum likelihod, and RGMWM estimators).</p>
<p>Using the <code>simts</code> package, the first three estimators can be computed as follows:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1-1" data-line-number="1">mod =<span class="st"> </span><span class="kw">estimate</span>(<span class="kw">AR</span>(p), Xt, <span class="dt">method =</span> select_method, <span class="dt">demean =</span> <span class="ot">TRUE</span>)</a></code></pre></div>
<p>In the above sample code <code>Xt</code> denotes the time series (a vector of length <span class="math inline">\(T\)</span>), <code>p</code> is the order of the AR(<span class="math inline">\(p\)</span>) and <code>demean = TRUE</code> indicates that the mean of the process should be estimated (if this is not the case, then use <code>demean = FALSE</code>). The <code>select_method</code> input can be (among others) <code>&quot;mle&quot;</code> for the maximum likelihood and <code>&quot;yule-walker&quot;</code> for the Yule-Walker estimator or <code>&quot;rgmwm&quot;</code> for the RGMWM. For example, if you would like to estimate a zero mean AR(3) with the MLE you can use the code:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2-1" data-line-number="1">mod =<span class="st"> </span><span class="kw">estimate</span>(<span class="kw">AR</span>(<span class="dv">3</span>), Xt, <span class="dt">method =</span> <span class="st">&quot;mle&quot;</span>, <span class="dt">demean =</span> <span class="ot">FALSE</span>)</a></code></pre></div>
<p>On the other hand, if one wishes to estimate the parameters of this model through the RGMWM one can use the following syntax:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb3-1" data-line-number="1">mod =<span class="st"> </span><span class="kw">estimate</span>(<span class="kw">AR</span>(<span class="dv">3</span>), Xt, <span class="dt">method =</span> <span class="st">&quot;rgmwm&quot;</span>, <span class="dt">demean =</span> <span class="ot">FALSE</span>)</a></code></pre></div>
<p>Removing the mean is not strictly necessary for the <code>rgmwm</code> (or <code>gmwm</code>) function since it won’t estimate it and can consistently estimate the parameters of the time series model anyway. We now have the necessary <code>R</code> functions to deliver the above mentioned estimators and we can now proceed to the simulation study. In particular, we simulate three different processes <span class="math inline">\(X_t, Y_t, Z_t\)</span> by using the first as an uncontaminated process defined as <span class="math display">\[X_t = 0.5 X_{t-1} - 0.25 X_{t-2} + W_t,\]</span> with <span class="math inline">\(W_t \overset{iid}{\sim} N(0, 1)\)</span>. This first process <span class="math inline">\((X_t)\)</span> is uncontaminated while the other two processes are contaminated versions of the first that can often be observed in practice. The first type of contamination can be seen in <span class="math inline">\((Y_t)\)</span> and is delivered by replacing a portion of the original process with a process defined as <span class="math display">\[U_t = 0.90 U_{t-1} - 0.40 U_{t-2} + V_t,\]</span> where <span class="math inline">\(V_t \overset{iid}{\sim} N(0, 9)\)</span>. The second form of contamination can be seen in <span class="math inline">\((Z_t)\)</span> and consists in the so-called point-wise contamination where randomly selected points from <span class="math inline">\(X_t\)</span> are replaced with <span class="math inline">\(N_t \overset{iid}{\sim} N(0, 9)\)</span>.</p>
<p>The code below performs the simulation study where it can be seen how the contaminated processes <span class="math inline">\((Y_t)\)</span> and <span class="math inline">\((Z_t)\)</span> are generated. Once this is done, for each simultation the code estimates the parameters of the AR(2) model using the three different estimation methods.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb4-1" data-line-number="1"><span class="co"># Load simts</span></a>
<a class="sourceLine" id="cb4-2" data-line-number="2"><span class="kw">library</span>(simts)</a>
<a class="sourceLine" id="cb4-3" data-line-number="3"></a>
<a class="sourceLine" id="cb4-4" data-line-number="4"><span class="co"># Number of bootstrap iterations</span></a>
<a class="sourceLine" id="cb4-5" data-line-number="5">B =<span class="st"> </span><span class="dv">250</span></a>
<a class="sourceLine" id="cb4-6" data-line-number="6"></a>
<a class="sourceLine" id="cb4-7" data-line-number="7"><span class="co"># Sample size</span></a>
<a class="sourceLine" id="cb4-8" data-line-number="8">n =<span class="st"> </span><span class="dv">500</span></a>
<a class="sourceLine" id="cb4-9" data-line-number="9"></a>
<a class="sourceLine" id="cb4-10" data-line-number="10"> <span class="co"># Proportion of contamination</span></a>
<a class="sourceLine" id="cb4-11" data-line-number="11">eps =<span class="st"> </span><span class="fl">0.05</span>       </a>
<a class="sourceLine" id="cb4-12" data-line-number="12"></a>
<a class="sourceLine" id="cb4-13" data-line-number="13"><span class="co"># Number of contaminated observations</span></a>
<a class="sourceLine" id="cb4-14" data-line-number="14">cont =<span class="st"> </span><span class="kw">round</span>(eps<span class="op">*</span>n)   </a>
<a class="sourceLine" id="cb4-15" data-line-number="15"></a>
<a class="sourceLine" id="cb4-16" data-line-number="16"><span class="co"># Simulation storage</span></a>
<a class="sourceLine" id="cb4-17" data-line-number="17">res.Xt.MLE =<span class="st"> </span>res.Xt.YW =<span class="st"> </span>res.Xt.RGMWM =<span class="st"> </span><span class="kw">matrix</span>(<span class="ot">NA</span>, B, <span class="dv">3</span>)</a>
<a class="sourceLine" id="cb4-18" data-line-number="18">res.Yt.MLE =<span class="st"> </span>res.Yt.YW =<span class="st"> </span>res.Yt.RGMWM =<span class="st"> </span><span class="kw">matrix</span>(<span class="ot">NA</span>, B, <span class="dv">3</span>)</a>
<a class="sourceLine" id="cb4-19" data-line-number="19">res.Zt.MLE =<span class="st"> </span>res.Zt.YW =<span class="st"> </span>res.Zt.RGMWM =<span class="st"> </span><span class="kw">matrix</span>(<span class="ot">NA</span>, B, <span class="dv">3</span>)</a>
<a class="sourceLine" id="cb4-20" data-line-number="20">  </a>
<a class="sourceLine" id="cb4-21" data-line-number="21"><span class="co"># Begin bootstrap</span></a>
<a class="sourceLine" id="cb4-22" data-line-number="22"><span class="cf">for</span> (i <span class="cf">in</span> <span class="kw">seq_len</span>(B)){</a>
<a class="sourceLine" id="cb4-23" data-line-number="23">  <span class="co"># Set seed for reproducibility</span></a>
<a class="sourceLine" id="cb4-24" data-line-number="24">  <span class="kw">set.seed</span>(<span class="dv">1982</span> <span class="op">+</span><span class="st"> </span>i)</a>
<a class="sourceLine" id="cb4-25" data-line-number="25">  </a>
<a class="sourceLine" id="cb4-26" data-line-number="26">  <span class="co"># Generate processes</span></a>
<a class="sourceLine" id="cb4-27" data-line-number="27">  Xt =<span class="st"> </span><span class="kw">gen_gts</span>(n, <span class="kw">AR</span>(<span class="dt">phi =</span> <span class="kw">c</span>(<span class="fl">0.5</span>, <span class="fl">0.25</span>), <span class="dt">sigma2 =</span> <span class="dv">1</span>))</a>
<a class="sourceLine" id="cb4-28" data-line-number="28">  Yt =<span class="st"> </span>Zt =<span class="st"> </span>Xt</a>
<a class="sourceLine" id="cb4-29" data-line-number="29">  </a>
<a class="sourceLine" id="cb4-30" data-line-number="30">  <span class="co"># Generate Ut contamination process that replaces a portion of original signal</span></a>
<a class="sourceLine" id="cb4-31" data-line-number="31">  index_start =<span class="st"> </span><span class="kw">sample</span>(<span class="dv">1</span><span class="op">:</span>(n<span class="op">-</span>cont<span class="dv">-1</span>), <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb4-32" data-line-number="32">  index_end =<span class="st"> </span>index_start <span class="op">+</span><span class="st"> </span>cont <span class="op">-</span><span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb4-33" data-line-number="33">  Yt[index_start<span class="op">:</span>index_end] =<span class="st"> </span><span class="kw">gen_gts</span>(cont, <span class="kw">AR</span>(<span class="dt">phi =</span> <span class="kw">c</span>(<span class="fl">0.9</span>,<span class="op">-</span><span class="fl">0.4</span>), <span class="dt">sigma2 =</span> <span class="dv">9</span>))</a>
<a class="sourceLine" id="cb4-34" data-line-number="34">  </a>
<a class="sourceLine" id="cb4-35" data-line-number="35">  <span class="co"># Generate Nt contamination that inject noise at random</span></a>
<a class="sourceLine" id="cb4-36" data-line-number="36">  Zt[<span class="kw">sample</span>(n, cont, <span class="dt">replace =</span> <span class="ot">FALSE</span>)] =<span class="st"> </span><span class="kw">gen_gts</span>(cont, <span class="kw">WN</span>(<span class="dt">sigma2 =</span> <span class="dv">9</span>))</a>
<a class="sourceLine" id="cb4-37" data-line-number="37">  </a>
<a class="sourceLine" id="cb4-38" data-line-number="38">  <span class="co"># Fit Yule-Walker estimators on the three time series</span></a>
<a class="sourceLine" id="cb4-39" data-line-number="39">  mod.Xt.YW =<span class="st"> </span><span class="kw">estimate</span>(<span class="kw">AR</span>(<span class="dv">2</span>), Xt, <span class="dt">method =</span> <span class="st">&quot;yule-walker&quot;</span>, </a>
<a class="sourceLine" id="cb4-40" data-line-number="40">                 <span class="dt">demean =</span> <span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb4-41" data-line-number="41">  mod.Yt.YW =<span class="st"> </span><span class="kw">estimate</span>(<span class="kw">AR</span>(<span class="dv">2</span>), Yt, <span class="dt">method =</span> <span class="st">&quot;yule-walker&quot;</span>, </a>
<a class="sourceLine" id="cb4-42" data-line-number="42">                 <span class="dt">demean =</span> <span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb4-43" data-line-number="43">  mod.Zt.YW =<span class="st"> </span><span class="kw">estimate</span>(<span class="kw">AR</span>(<span class="dv">2</span>), Zt, <span class="dt">method =</span> <span class="st">&quot;yule-walker&quot;</span>, </a>
<a class="sourceLine" id="cb4-44" data-line-number="44">                 <span class="dt">demean =</span> <span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb4-45" data-line-number="45">  </a>
<a class="sourceLine" id="cb4-46" data-line-number="46">  <span class="co"># Store results</span></a>
<a class="sourceLine" id="cb4-47" data-line-number="47">  res.Xt.YW[i, ] =<span class="st"> </span><span class="kw">c</span>(mod.Xt.YW<span class="op">$</span>mod<span class="op">$</span>coef, mod.Xt.YW<span class="op">$</span>mod<span class="op">$</span>sigma2)</a>
<a class="sourceLine" id="cb4-48" data-line-number="48">  res.Yt.YW[i, ] =<span class="st"> </span><span class="kw">c</span>(mod.Yt.YW<span class="op">$</span>mod<span class="op">$</span>coef, mod.Yt.YW<span class="op">$</span>mod<span class="op">$</span>sigma2)</a>
<a class="sourceLine" id="cb4-49" data-line-number="49">  res.Zt.YW[i, ] =<span class="st"> </span><span class="kw">c</span>(mod.Zt.YW<span class="op">$</span>mod<span class="op">$</span>coef, mod.Zt.YW<span class="op">$</span>mod<span class="op">$</span>sigma2)</a>
<a class="sourceLine" id="cb4-50" data-line-number="50">  </a>
<a class="sourceLine" id="cb4-51" data-line-number="51"></a>
<a class="sourceLine" id="cb4-52" data-line-number="52">  <span class="co"># Fit MLE on the three time series</span></a>
<a class="sourceLine" id="cb4-53" data-line-number="53">  mod.Xt.MLE =<span class="st"> </span><span class="kw">estimate</span>(<span class="kw">AR</span>(<span class="dv">2</span>), Xt, <span class="dt">method =</span> <span class="st">&quot;mle&quot;</span>, </a>
<a class="sourceLine" id="cb4-54" data-line-number="54">                 <span class="dt">demean =</span> <span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb4-55" data-line-number="55">  mod.Yt.MLE =<span class="st"> </span><span class="kw">estimate</span>(<span class="kw">AR</span>(<span class="dv">2</span>), Yt, <span class="dt">method =</span> <span class="st">&quot;mle&quot;</span>, </a>
<a class="sourceLine" id="cb4-56" data-line-number="56">                 <span class="dt">demean =</span> <span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb4-57" data-line-number="57">  mod.Zt.MLE =<span class="st"> </span><span class="kw">estimate</span>(<span class="kw">AR</span>(<span class="dv">2</span>), Zt, <span class="dt">method =</span> <span class="st">&quot;mle&quot;</span>, </a>
<a class="sourceLine" id="cb4-58" data-line-number="58">                 <span class="dt">demean =</span> <span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb4-59" data-line-number="59">  </a>
<a class="sourceLine" id="cb4-60" data-line-number="60">  <span class="co"># Store results</span></a>
<a class="sourceLine" id="cb4-61" data-line-number="61">  res.Xt.MLE[i, ] =<span class="st"> </span><span class="kw">c</span>(mod.Xt.MLE<span class="op">$</span>mod<span class="op">$</span>coef, mod.Xt.MLE<span class="op">$</span>mod<span class="op">$</span>sigma2)</a>
<a class="sourceLine" id="cb4-62" data-line-number="62">  res.Yt.MLE[i, ] =<span class="st"> </span><span class="kw">c</span>(mod.Yt.MLE<span class="op">$</span>mod<span class="op">$</span>coef, mod.Yt.MLE<span class="op">$</span>mod<span class="op">$</span>sigma2)</a>
<a class="sourceLine" id="cb4-63" data-line-number="63">  res.Zt.MLE[i, ] =<span class="st"> </span><span class="kw">c</span>(mod.Zt.MLE<span class="op">$</span>mod<span class="op">$</span>coef, mod.Zt.MLE<span class="op">$</span>mod<span class="op">$</span>sigma2)</a>
<a class="sourceLine" id="cb4-64" data-line-number="64">  </a>
<a class="sourceLine" id="cb4-65" data-line-number="65">  <span class="co"># Fit RGMWM on the three time series</span></a>
<a class="sourceLine" id="cb4-66" data-line-number="66">  res.Xt.RGMWM[i, ] =<span class="st"> </span><span class="kw">estimate</span>(<span class="kw">AR</span>(<span class="dv">2</span>), Xt, <span class="dt">method =</span> <span class="st">&quot;rgmwm&quot;</span>, </a>
<a class="sourceLine" id="cb4-67" data-line-number="67">                 <span class="dt">demean =</span> <span class="ot">FALSE</span>)<span class="op">$</span>mod<span class="op">$</span>estimate</a>
<a class="sourceLine" id="cb4-68" data-line-number="68">  res.Yt.RGMWM[i, ] =<span class="st"> </span><span class="kw">estimate</span>(<span class="kw">AR</span>(<span class="dv">2</span>), Yt, <span class="dt">method =</span> <span class="st">&quot;rgmwm&quot;</span>, </a>
<a class="sourceLine" id="cb4-69" data-line-number="69">                 <span class="dt">demean =</span> <span class="ot">FALSE</span>)<span class="op">$</span>mod<span class="op">$</span>estimate</a>
<a class="sourceLine" id="cb4-70" data-line-number="70">  res.Zt.RGMWM[i, ] =<span class="st"> </span><span class="kw">estimate</span>(<span class="kw">AR</span>(<span class="dv">2</span>), Zt, <span class="dt">method =</span> <span class="st">&quot;rgmwm&quot;</span>, </a>
<a class="sourceLine" id="cb4-71" data-line-number="71">                 <span class="dt">demean =</span> <span class="ot">FALSE</span>)<span class="op">$</span>mod<span class="op">$</span>estimate</a>
<a class="sourceLine" id="cb4-72" data-line-number="72">}</a></code></pre></div>
<p>Having performed the estimation, we should now have 250 estimates for each AR(2) parameter and each estimation method. The code below takes the results of the simulation and shows them in the shape of boxplots along with the true values of the parameters. The estimation methods that are denoted as follows:</p>
<ul>
<li><strong>YW</strong>: Yule-Walker estimator</li>
<li><strong>MLE</strong>: Maximum Likelihood Estimator</li>
<li><strong>RGMWM</strong>: the robust version of the GMWM estimator</li>
</ul>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-5"></span>
<img src="ts_files/figure-html/unnamed-chunk-5-1.png" alt="Boxplots of the empirical distribution functions of the Yule-Walker (YW), MLE and GMWM estimators for the parameters of the AR(2) model when using the Xt process (first row of boxplots), Yt process (second row of boxplots) and Zt (third row of boxplots)." width="864" />
<p class="caption">
Figure 4.2: Boxplots of the empirical distribution functions of the Yule-Walker (YW), MLE and GMWM estimators for the parameters of the AR(2) model when using the Xt process (first row of boxplots), Yt process (second row of boxplots) and Zt (third row of boxplots).
</p>
</div>
<p>It can be seen how all methods appear to properly estimate the true parameter values on average when they are applied to the simulated time series from the uncontaminated process <span class="math inline">\((X_t)\)</span>. However, the MLE appears to be slightly more efficient (less variable) compared to the other methods and, in addition, the robust method (RGMWM) appears to be less efficient than the other two estimators. The latter is a known result since robust estimators usually pay a price in terms of efficiency (as an insurance against bias).</p>
<p>On the other hand, when checking the performance of the same methods when applied to the two contaminated processes <span class="math inline">\((Y_t)\)</span> and <span class="math inline">\((Z_t)\)</span> it can be seen that the standard estimators appear to be (highly) biased for most of the estimated parameters (with one exception) while the robust estimator remains close (on average) to the true parameter values that we are aiming to estimate. Therefore, when there’s a suspicion that there could be some (small) contamination in the observed time series, it may be more appropriate to use a robust estimator.</p>
<p>To conclude this section on estimation, we now compare the above studied estimators in different applied settings where we can highlight how to assess which estimator is more appropriate according to the type of setting. For this purpose, let us start with an example we have already checked in the previous chapter when discussing standard and robust estimators of the ACF, more specifically the data on monthly precipitations. As mentioned before when discussing this example, the importance of modelling precipitation data lies in the fact that its usually used to successively model the entire water cycle. Common models for this purpose are either the white noise (WN) model or the AR(1) model. Let us compare the standard and robust ACF again to understand which of these two models seems more appropriate for the data at hand.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb5-1" data-line-number="1"><span class="kw">compare_acf</span>(hydro)</a></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-6"></span>
<img src="ts_files/figure-html/unnamed-chunk-6-1.png" alt="Standard (left) and robust (left) estimates of the ACF function on the monthly precipitation data (hydro)" width="864" />
<p class="caption">
Figure 4.3: Standard (left) and robust (left) estimates of the ACF function on the monthly precipitation data (hydro)
</p>
</div>
<p>As we had underlined in the previous chapter, the standard ACF estimates would suggest that there appears to be no correlation among lags and consequently, the WN model would be the most appropriate. However, the robust ACF estimates depict an entirely different picture where it can be seen that there appears to be a significant autocorrelation over different lags which exponentially decay. Although there appears to be some seasonality in the plot, we will assume that the correct model for this data is an AR(1) since that’s what hydrology theory suggests. Let us therefore estimate the parameters of this model by using a standard estimator (MLE) and a robust estimator (RGMWM). The estimates for the MLE are the following:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb6-1" data-line-number="1">mle_hydro =<span class="st"> </span><span class="kw">estimate</span>(<span class="kw">AR</span>(<span class="dv">1</span>), <span class="kw">as.vector</span>(hydro),  <span class="dt">method =</span> <span class="st">&quot;mle&quot;</span>, <span class="dt">demean =</span> <span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb6-2" data-line-number="2"></a>
<a class="sourceLine" id="cb6-3" data-line-number="3"><span class="co"># MLE Estimates</span></a>
<a class="sourceLine" id="cb6-4" data-line-number="4"><span class="kw">c</span>(mle_hydro<span class="op">$</span>mod<span class="op">$</span>coef[<span class="dv">1</span>], mle_hydro<span class="op">$</span>mod<span class="op">$</span>sigma2)</a></code></pre></div>
<pre><code>##        ar1            
## 0.06497727 0.22205713</code></pre>
<p>From these estimates it would appear that the autocorrelation between lagged variables (i.e. lags of order 1) is extremely low and that (as suggested by the standard ACF plot) a WN model may be more appropriate. Considering the robust ACF however, it is possible that the MLE estimates may not be reliable in this setting. Hence, let us use the RGMWM to estimate the same parameters.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb8-1" data-line-number="1">rgmwm_hydro =<span class="st"> </span><span class="kw">estimate</span>(<span class="kw">AR</span>(<span class="dv">1</span>), hydro, <span class="dt">method =</span> <span class="st">&quot;rgmwm&quot;</span>)<span class="op">$</span>mod<span class="op">$</span>estimate</a>
<a class="sourceLine" id="cb8-2" data-line-number="2"></a>
<a class="sourceLine" id="cb8-3" data-line-number="3"><span class="co"># RGMWM Estimates</span></a>
<a class="sourceLine" id="cb8-4" data-line-number="4"><span class="kw">t</span>(rgmwm_hydro)</a></code></pre></div>
<pre><code>##                  AR    SIGMA2
## Estimates 0.4048702 0.1065875</code></pre>
<p>In this case, we see how the autocorrelation between lagged values is much higher (0.4 compared to 0.06) indicating that there is a stronger dependence in the data than what is suggested by standard estimators. Moreover, the innovation variance is smaller compared to that of the MLE. This is also a known phenomenon when there’s contamination in the data since it leads to less dependence and more variability being detected by non-robust estimators. This estimate of the variance also has a considerable impact on forecast precision (as we’ll see in the next section).</p>
<p>A final applied example that highlights the (potential) difference between estimators according to the type of setting is given by the “Recruitment” data set (in the <code>astsa</code> library). This data refers to the presence of new fish in the population of the Pacific Ocean and is often linked to the currents and temperatures passing through the ocean. As for the previous data set, let us take a look at the data itself and then analyse the standard and robust estimations of the ACF.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb10-1" data-line-number="1"><span class="co"># Format data</span></a>
<a class="sourceLine" id="cb10-2" data-line-number="2">fish =<span class="st"> </span><span class="kw">gts</span>(rec, <span class="dt">start =</span> <span class="dv">1950</span>, <span class="dt">freq =</span> <span class="dv">12</span>, <span class="dt">unit_time =</span> <span class="st">&#39;month&#39;</span>, <span class="dt">name_ts =</span> <span class="st">&#39;Recruitment&#39;</span>)</a>
<a class="sourceLine" id="cb10-3" data-line-number="3"></a>
<a class="sourceLine" id="cb10-4" data-line-number="4"><span class="co"># Plot data</span></a>
<a class="sourceLine" id="cb10-5" data-line-number="5"><span class="kw">plot</span>(fish)</a></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-10"></span>
<img src="ts_files/figure-html/unnamed-chunk-10-1.png" alt="Plot of the time series on fish recruitment monthly data in the Pacific Ocean from 1950 to 1987" width="768" />
<p class="caption">
Figure 4.4: Plot of the time series on fish recruitment monthly data in the Pacific Ocean from 1950 to 1987
</p>
</div>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb11-1" data-line-number="1"><span class="kw">compare_acf</span>(fish)</a></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-11"></span>
<img src="ts_files/figure-html/unnamed-chunk-11-1.png" alt="Standard (left) and robust (left) estimates of the ACF function on the monthly fish recruitment data (rec)" width="864" />
<p class="caption">
Figure 4.5: Standard (left) and robust (left) estimates of the ACF function on the monthly fish recruitment data (rec)
</p>
</div>
<p>We can see that there appears to be a considerable dependence between the lagged variables which decays (in a similar way to the ACF of an AR(<span class="math inline">\(p\)</span>)). Also in this case we see a seasonality in the data but we won’t consider this for the purpose of this example. Given that there doesn’t appear to be any significant contamination in the data, let us consider the Yule-Walker and MLE estimators. The MLE highly depends on the assumed parametric distribution of the time series (i.e. usually Gaussian) and, if this is not respected, the resulting estimations could be unreliable. Hence, a first difference of the time series can often give an idea of the marginal distribution of the time series.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb12-1" data-line-number="1"><span class="co"># Take first differencing of the recruitment data</span></a>
<a class="sourceLine" id="cb12-2" data-line-number="2">diff_fish =<span class="st"> </span><span class="kw">gts</span>(<span class="kw">diff</span>(rec), <span class="dt">start =</span> <span class="dv">1950</span>, <span class="dt">freq =</span> <span class="dv">12</span>, <span class="dt">unit_time =</span> <span class="st">&#39;month&#39;</span>, <span class="dt">name_ts =</span> <span class="st">&#39;Recruitment&#39;</span>)</a>
<a class="sourceLine" id="cb12-3" data-line-number="3"></a>
<a class="sourceLine" id="cb12-4" data-line-number="4"><span class="co"># Plot first differencing of the recruitment data</span></a>
<a class="sourceLine" id="cb12-5" data-line-number="5"><span class="kw">plot</span>(diff_fish)</a></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-13"></span>
<img src="ts_files/figure-html/unnamed-chunk-13-1.png" alt="Plot of the first difference of the time series on fish recruitment monthly data in the Pacific Ocean from 1950 to 1987" width="768" />
<p class="caption">
Figure 4.6: Plot of the first difference of the time series on fish recruitment monthly data in the Pacific Ocean from 1950 to 1987
</p>
</div>
<p>From the plot we can see that observations appear to be collected around a constant value and fewer appear to be further from this value (as would be the case for a normal distribution). However, various of these “more extreme” observations appear to be quite frequent suggesting that the underlying distribution may have a heavier tail compared to the normal distribution. Let us compare the standard and robust ACF estimates of this new time series.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb13-1" data-line-number="1"><span class="kw">compare_acf</span>(diff_fish)</a></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-14"></span>
<img src="ts_files/figure-html/unnamed-chunk-14-1.png" alt="Standard (left) and robust (left) estimates of the ACF function on the first difference of the monthly fish recruitment data (rec)" width="864" />
<p class="caption">
Figure 4.7: Standard (left) and robust (left) estimates of the ACF function on the first difference of the monthly fish recruitment data (rec)
</p>
</div>
<p>In this case we see that the patterns appear to be the same but the values between the standard and robust estimates are slightly (to moderately) different over different lags. This would suggest that there could be some contamination in the data or, in any case, that the normal assumption may not hold exactly. With this in mind, let us estimate an AR(2) model for this data using the Yule-Walker and MLE.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb14-1" data-line-number="1"><span class="co"># MLE of Recruitment data</span></a>
<a class="sourceLine" id="cb14-2" data-line-number="2">yw_fish =<span class="st"> </span><span class="kw">estimate</span>(<span class="kw">AR</span>(<span class="dv">2</span>), rec, <span class="dt">method =</span> <span class="st">&quot;yule-walker&quot;</span>, <span class="dt">demean =</span> <span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb14-3" data-line-number="3"></a>
<a class="sourceLine" id="cb14-4" data-line-number="4"><span class="co"># MLE of Recruitment data</span></a>
<a class="sourceLine" id="cb14-5" data-line-number="5">mle_fish =<span class="st"> </span><span class="kw">estimate</span>(<span class="kw">AR</span>(<span class="dv">2</span>), rec, <span class="dt">method =</span> <span class="st">&quot;mle&quot;</span>, <span class="dt">demean =</span> <span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb14-6" data-line-number="6"></a>
<a class="sourceLine" id="cb14-7" data-line-number="7"><span class="co"># Compare estimates</span></a>
<a class="sourceLine" id="cb14-8" data-line-number="8"><span class="co"># Yule-Walker Estimation</span></a>
<a class="sourceLine" id="cb14-9" data-line-number="9"><span class="kw">c</span>(yw_fish<span class="op">$</span>mod<span class="op">$</span>coef[<span class="dv">1</span>], yw_fish<span class="op">$</span>mod<span class="op">$</span>sigma2)</a></code></pre></div>
<pre><code>##       ar1           
##  1.354069 89.717052</code></pre>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb16-1" data-line-number="1"><span class="co"># MLE Estimation</span></a>
<a class="sourceLine" id="cb16-2" data-line-number="2"><span class="kw">c</span>(mle_fish<span class="op">$</span>mod<span class="op">$</span>coef[<span class="dv">1</span>], mle_fish<span class="op">$</span>mod<span class="op">$</span>sigma2)</a></code></pre></div>
<pre><code>##       ar1           
##  1.351218 89.334361</code></pre>
<p>It can be seen that, in this setting, the two estimators deliver very similar results (at least in terms of the <span class="math inline">\(\phi_1\)</span> and <span class="math inline">\(\phi_2\)</span> coefficients). Indeed, there doesn’t appear to be a strong need for robust estimation and the choice of a standard estimator is justified by the will to obtain efficient estimations. The only slight difference between the two estimations is in the innovation variance parameter <span class="math inline">\(\sigma^2\)</span> and this could be (evenutally) due to the normality assumption that the MLE estimator upholds in this case. If there is therefore a doubt on the fact that the Gaussian assumption does not hold for this data, then it is probably more convenient to use the Yule-Walker estimates.</p>
<p>Until now we have focussed on estimation based on an assumed model. However, how do we choose a model? How can we make inference on the models and their parameters? To perform all these tasks we will need to compute residuals (as, for example, in the linear regression framework). In order to obtain residuals, we need to be able to predict (forecast) values of the time series and, consequently, the next section focuses on forecasting time series.</p>
</div>
<div id="forecasting-arp-models" class="section level3">
<h3><span class="header-section-number">4.2.3</span> Forecasting AR(p) Models</h3>
<p>One of the most interesting aspects of time series analysis is to predict the future unobserved values based on the values that have been observed up to now. However, this is not possible if the underlying (parametric) model is unknown, thus in this section we assume, for purpose of illustration, that the time series <span class="math inline">\((X_t)\)</span> is <strong>stationary</strong> and its model is known. In particular, we denote forecasts by <span class="math inline">\(X^{t}_{t+j}\)</span>, where <span class="math inline">\(t\)</span> represents the time point from which we would like to make a forecast assuming we have an <em>observed</em> time series (e.g. <span class="math inline">\(\mathbf{X} = (X_{1}, X_{2}, \cdots , X_{t-1}, X_t)\)</span>) and <span class="math inline">\(j\)</span> represents the <span class="math inline">\(j^{th}\)</span>-ahead future value we wish to predict. So, <span class="math inline">\(X^{t}_{t+1}\)</span> represents a one-step-ahead prediction of <span class="math inline">\(X_{t+1}\)</span> given data <span class="math inline">\((X_{1}, X_{2}, \cdots, X_{t-1}, X_{t})\)</span>.</p>
<p>Let us now focus on defining a prediction operator and, for this purpose, let us define the Mean Squared Prediction Error (MSPE) as follows:</p>
<p><span class="math display">\[\mathbb{E}[(X_{t+j} - X^{t}_{t+j})^2] .\]</span>
Intuitively, the MPSE measures the square distance (i.e. always positive) between the actual future values and the corresponding predictions. Ideally, we would want this measure to be equal to zero (meaning that we don’t make any prediction errors) but, if this is not possible, we would like to define a predictor that has the smallest MPSE among all possible predictors. The MPSE is not necessarily the best measure of prediction accuracy since, for example, it can be greatly affected by outliers or doesn’t take into account importance of positive or negative misspredictions (e.g. for risks in finance or insurance). Nevertheless, it is a good overall measure of forecast accuracy and the next theorem states what the best predictor is for this measure.</p>

<div class="theorem">
<p><span id="thm:condexp" class="theorem"><strong>Theorem 4.1  (Minimum Mean Squared Error Predictor)  </strong></span>Let us define
<span class="math display">\[X_{t + j}^t \equiv \mathbb{E}\left[ {{X_{t + j}}|{X_t}, \cdots ,{X_1}} \right], j &gt; 0\]</span></p>
Then <span class="math display">\[\mathbb{E}\left[ {{{\left( {{X_{t + j}} - m\left( {{X_1}, \cdots ,{X_t}} \right)} \right)}^2}} \right] \ge \mathbb{E}\left[ {{{\left( {{X_{t + j}} - X_{t + j}^t} \right)}^2}} \right]\]</span> for any function <span class="math inline">\(m(.)\)</span>.
</div>

<p><br></p>
<p>The proof of this theorem can be found in Appendix <a href="proofs.html#appendixc">A.2</a>. Although this theorem defines the best possible predictor, there can be many functional forms for this operator. We first restrict our attention to the set of linear predictors defined as</p>
<p><span class="math display">\[X_{t+j}^t = \sum_{i=1}^t \alpha_i X_i,\]</span>
where <span class="math inline">\(\alpha_i \in \mathbb{R}\)</span>. It can be noticed, for example, that the <span class="math inline">\(\alpha_i\)</span>’s are not always the same based on the values of <span class="math inline">\(t\)</span> and <span class="math inline">\(j\)</span> (i.e. it depends from which time point you want to predict and how far into the future). Another aspect to notice is that, if the time series model underlying the observed time series can be expressed in the form of a linear operator (e.g. a linear process), then we can derive a linear predictor from this framework.</p>
<!-- Considering the above, let us now give the following theorem which provides further insight into linear prediction.  -->
<!-- ```{theorem, label="projtheo", name="Projection Theorem"} -->
<!-- Let $\mathcal{M} \subset \mathcal{L}_2$ be a closed linear subspace of a Hibert space.  -->
<!-- For every $y \in \mathcal{L}_2$, there exists a unique element $\hat{y} \in \mathcal{M}$ that -->
<!-- minimizes $||y - z||^2$ over $z \in \mathcal{M}$. This element is uniquely  -->
<!-- determined by the requirements -->
<!-- 1. $\hat{y} \in \mathcal{M}$ and  -->
<!-- 2. $(y - \hat{y}) \perp \mathcal{M}$. -->
<!-- ``` -->
<!-- Based on this theorem, it is possible to define the Best Linear Predictor (BLP) for stationary processes.  -->
<!-- ```{definition, label="BLP"} -->
<!-- The best linear predictor $X_{t+j}^t = \sum_{i=1}^t \alpha_i X_i$, for $j \geq 1$ is found by solving -->
<!-- \[ \mathbb{E} [(X_{t+h} - \hat{X}_{t+h})X_i ] = 0, \mbox{ for } i = 1, \dots, t.\] -->
<!-- ``` -->
<!-- If we denote $\mathbb{E}(X_{i}, X_{j})$ as $\gamma(|i - j|)$, these prediction equations can alternatively be represented in the following form: -->
<!-- \begin{equation} -->
<!-- \begin{aligned} -->
<!-- \begin{pmatrix} -->
<!-- \gamma(0) & \gamma(1) & \cdots & \gamma(T-1) \\ -->
<!-- \gamma(1) & \gamma(0) & \cdots & \gamma(T-2) \\ -->
<!-- \vdots & \vdots & \ddots & \vdots \\ -->
<!-- \gamma(T-1) & \gamma(T-2) & \cdots &\gamma(0) -->
<!-- \end{pmatrix}_{T \times T} -->
<!-- \begin{pmatrix} -->
<!-- \alpha_1 \\ -->
<!-- \vdots \\ -->
<!-- \alpha_T -->
<!-- \end{pmatrix}_{T \times 1} -->
<!-- &= -->
<!--   \begin{pmatrix} -->
<!-- \gamma(1)  \\ -->
<!-- \vdots \\ -->
<!-- \gamma(T) -->
<!-- \end{pmatrix}_{T \times 1} , -->
<!-- \end{aligned} -->
<!-- \end{equation} -->
<!-- which can be written in a compact matrix form as $\Gamma_T \mathbf{\alpha}_T  = \mathbf{\gamma}_T$. Assuming that $\Gamma_T$ is non-singular, then the values of $\mathbf{\alpha}_T$ are -->
<!-- given by: -->
<!--   $$\mathbf{\alpha}_T  = \Gamma^{-1}_T\mathbf{\gamma}_T.$$ -->
<!-- which delivers the linear predictor -->
<!-- $$X_{t+j}^t = \alpha_T^T \mathbf{X}^* .$$ -->
<!-- where $\mathbf{X}^* = (X_{T}, X_{T-1}, \cdots , X_{2}, X_1)$. We can now examine these predictors for the AR($p$) models that we've studied this far (starting from the AR(1)). -->
<p>Let us consider some examples to understand how these linear predictors can be delivered based on the AR(p) models studied this far. For this purpose, let us start with an AR(1) process.</p>

<div class="example">
<p><span id="exm:ar1forecast" class="example"><strong>Example 4.8  (Forecasting with an AR(1) model)  </strong></span>The AR(1) model is defined as follows:</p>
<p><span class="math display">\[{X_t} = \phi {X_{t - 1}} + {W_t},\]</span></p>
<p>where <span class="math inline">\(W_t \sim WN(0, \sigma^2)\)</span>.</p>
<p>From here, the conditional expected mean and variance are given by</p>
<p><span class="math display">\[\begin{aligned}
  \mathbb{E}\left[ X_{t + j} | \Omega_t \right] &amp;= \mathbb{E}[\phi X_{t+j-1} + W_{t+j}\,| \, \Omega_t ] \\
      &amp;= ... = {\phi ^j}{X_t} \\
  \operatorname{var}\left[ {{X_{t + j}}} | \Omega_t \right] &amp;= \left( {1 + {\phi ^2} + {\phi ^4} +  \cdots  + {\phi ^{2\left( {j - 1} \right)}}} \right){\sigma ^2} \\ 
\end{aligned} \]</span></p>
<p>Within this derivation, it is important to remember that</p>
<p><span class="math display">\[\begin{aligned}
  \mathop {\lim }\limits_{j \to \infty } \mathbb{E}\left[ {{X_{t + j}}} | \Omega_t \right] &amp;= \mathbb{E}\left[ {{X_t}} \right] = 0 \\
  \mathop {\lim }\limits_{j \to \infty } \operatorname{var}_t\left[ {{X_{t + j}}} | \Omega_t \right] &amp;= \operatorname{var} \left( {{X_t}} \right) = \frac{{{\sigma ^2}}}{{1 - {\phi ^2}}}   
\end{aligned} \]</span></p>
<p>From these last results we see that the forecast for an AR(1) process is “mean-reverting” since in general we have that the AR(1) can be written with respect to the mean <span class="math inline">\(\mu\)</span> as follows</p>
<p><span class="math display">\[(X_t - \mu) = \phi (X_{t-1} - \mu) + W_t,\]</span></p>
<p>which gives</p>
<p><span class="math display">\[X_t = \mu + \phi (X_{t-1} - \mu) + W_t,\]</span></p>
<p>and consequently</p>
<p><span class="math display">\[\begin{aligned}
 \mathbb{E}[X_{t+j} | \Omega_t] &amp;= \mu + \mathbb{E}[(X_{t+j} - \mu) | \Omega_t] \\
  &amp;= \mu + \phi^j(X_t - \mu),   
\end{aligned} \]</span></p>
which tends to <span class="math inline">\(\mu\)</span> as <span class="math inline">\(j \to \infty\)</span>
</div>

<p>Let us now consider a more complicated model, i.e. an AR(2) model.</p>

<div class="example">
<p><span id="exm:ar2forecast" class="example"><strong>Example 4.9  (Forecasting with an AR(2) model)  </strong></span>Consider an AR(2) process defined as follows:</p>
<p><span class="math display">\[{X_t} = {\phi _1}{X_{t - 1}} + {\phi _2}{X_{t - 2}} + {W_t},\]</span></p>
<p>where <span class="math inline">\(W_t \sim WN(0, \sigma^2)\)</span>.</p>
<p>Based on this process we are able to find the predictor for each j-step ahead prediction using the following approach:</p>
<p><span class="math display">\[\begin{aligned}
  \mathbb{E}\left[ {{X_{t + 1}}} | \Omega_t \right] &amp;= {\phi _1}{X_t} + {\phi _2}{X_{t - 1}} \\
  \mathbb{E}\left[ {{X_{t + 2}}} | \Omega_t \right] &amp;= \mathbb{E}\left[ {{\phi _1}{X_{t + 1}} + {\phi _2}{X_t}} | \Omega_t \right] = {\phi _1} \mathbb{E}\left[ {{X_{t + 1}}} | \Omega_t \right] + {\phi _2}{X_t} \\
   &amp;= {\phi _1}\left( {{\phi _1}{X_t} + {\phi _2}{X_{t - 1}}} \right) + {\phi _2}{X_t} \\
   &amp;= \left( {\phi _1^2 + {\phi _2}} \right){X_t} + {\phi _1}{\phi _2}{X_{t - 1}}. \\ 
\end{aligned} \]</span></p>
</div>

<p>Having seen how to build a forecast for an AR(1), let us now generalise this method to an AR(p) using matrix notation.</p>

<div class="example">
<p><span id="exm:arpforecast" class="example"><strong>Example 4.10  (Forecasting with an AR(p) model)  </strong></span>Consider AR(p) process defined as:</p>
<p><span class="math display">\[{X_t} = {\phi _1}{X_{t - 1}} + {\phi _2}{X_{t - 2}} + \cdots + {\phi _p}{X_{t - p}} + {W_t},\]</span></p>
<p>where <span class="math inline">\(W_t \sim WN(0, \sigma^2_W)\)</span>.</p>
<p>The process can be rearranged into matrix form as follows:</p>
<p><span class="math display">\[\begin{aligned}
  \underbrace {\left[ {\begin{array}{*{20}{c}}
  {{X_t}} \\ 
   \vdots  \\ 
   \vdots  \\ 
  {{X_{t - p + 1}}} 
\end{array}} \right]}_{{Y_t}} &amp;= \underbrace {\left[ {\begin{array}{*{20}{c}}
  {{\phi _1}}&amp; \cdots &amp;{}&amp;{{\phi _p}} \\ 
  {}&amp;{}&amp;{}&amp;0 \\ 
  {}&amp;{{I_{p - 1}}}&amp;{}&amp; \vdots  \\ 
  {}&amp;{}&amp;{}&amp;0 
\end{array}} \right]}_A\underbrace {\left[ {\begin{array}{*{20}{c}}
  {{X_{t - 1}}} \\ 
   \vdots  \\ 
   \vdots  \\ 
  {{X_{t - p}}} 
\end{array}} \right]}_{{Y_{t - 1}}} + \underbrace {\left[ {\begin{array}{*{20}{c}}
  1 \\ 
  0 \\ 
   \vdots  \\ 
  0 
\end{array}} \right]}_C{W_t} \\
  {Y_t} &amp;= A{Y_{t - 1}} + C{W_t} 
\end{aligned}\]</span></p>
<p>From here, the conditional expectation and variance can be computed as follows:</p>
<p><span class="math display">\[\begin{aligned}
  {E}\left[ {{Y_{t + j}}} | \Omega_t \right] &amp;= {E}\left[ {A{Y_{t + j - 1}} + C{W_{t + j}}} | \Omega_t\right] = {E}\left[ {A{Y_{t + j - 1}}} | \Omega_t \right] + \underbrace {{E}\left[ {C{W_{t + j}}} | \Omega_t \right]}_{ = 0} \\
   &amp;= {E}\left[ {A\left( {A{Y_{t + j - 2}} + C{W_{t + j - 1}}} \right)} | \Omega_t \right] = {E}\left[ {{A^2}{Y_{t + j - 2}}} | \Omega_t\right] = {A^j}{Y_t} \\ 
  {\operatorname{var}}\left[ {{Y_{t + j}}} | | \Omega_t\right] &amp;= {\operatorname{var}}\left[ {A{Y_{t + j - 1}} + C{W_{t + j}}} | \Omega_t \right] \\
   &amp;= {\sigma ^2}C{C^T} + {\operatorname{var}}\left[ {A{Y_{t + j - 1}}} | \Omega_t \right] = {\sigma ^2}A{\operatorname{var}}\left[ {{Y_{t + j - 1}}} | \Omega_t \right]{A^T} \\
   &amp;= {\sigma ^2}C{C^T} + {\sigma ^2}AC{C^T}A + {\sigma ^2}{A^2}{\operatorname{var}}\left[ {{Y_{t + j - 2}}} | \Omega_t \right]{\left( {{A^2}} \right)^T} \\
   &amp;= {\sigma ^2}\sum\limits_{k = 0}^{j - 1} {{A^k}C{C^T}{{\left( {{A^K}} \right)}^T}}  \\ 
\end{aligned} \]</span></p>
Considering the recursive pattern coming from the expressions of the conditional expectation and variance, the predictions can be obtained via the following recursive formulation:
<span class="math display">\[\begin{aligned}
  {E}\left[ {{Y_{t + j}}} | \Omega_t \right] &amp;= A{E}\left[ {{Y_{t + j - 1}}} | \Omega_t\right] \\
  {\operatorname{var}}\left[ {{Y_{t + j}}} | \Omega_t \right] &amp;= {\sigma ^2}C{C^T} + A{\operatorname{var}}\left[ {{Y_{t + j - 1}}} | \Omega_t \right]{A^T} \\ 
\end{aligned} \]</span>
</div>

<p>With this recursive expression we can now compute the conditional expectation and variance of different AR(p) models. Let us therefore revisit the previous AR(2) example using this recursive formula.</p>

<div class="example">
<p><span id="exm:rear2forecast" class="example"><strong>Example 4.11  (Forecasting with an AR(2) in Matrix Form)  </strong></span>We can rewrite our previous example of the predictions for an AR(2) process as follows</p>
<p><span class="math display">\[\begin{aligned} \underbrace {\left[ {\begin{array}{*{20}{c}}
  {{X_t}} \\ 
  {{X_{t - 1}}} 
\end{array}} \right]}_{{Y_t}} &amp;= \underbrace {\left[ {\begin{array}{*{20}{c}}
  {{\phi _1}}&amp;{{\phi _2}} \\ 
  1&amp;0 
\end{array}} \right]}_A\underbrace {\left[ {\begin{array}{*{20}{c}}
  {{X_{t - 1}}} \\ 
  {{X_{t - 2}}} 
\end{array}} \right]}_{{Y_{t - 1}}} + \underbrace {\left[ {\begin{array}{*{20}{c}}
  1 \\ 
  0 
\end{array}} \right]}_C{W_t} \\
  {Y_t} &amp;= A{Y_{t - 1}} + C{W_t} 
\end{aligned}\]</span></p>
<p>Then, we are able to calculate the prediction as</p>
<span class="math display">\[\begin{aligned}
  {E}\left[ {{Y_{t + 2}}} | \Omega_t \right] &amp;= {A^2}{Y_t} = \left[ {\begin{array}{*{20}{c}}
  {{\phi _1}}&amp;{{\phi _2}} \\ 
  1&amp;0 
\end{array}} \right]\left[ {\begin{array}{*{20}{c}}
  {{\phi _1}}&amp;{{\phi _2}} \\ 
  1&amp;0 
\end{array}} \right]\left[ {\begin{array}{*{20}{c}}
  {{X_t}} \\ 
  {{X_{t - 1}}} 
\end{array}} \right] \hfill \\
   &amp;= \left[ {\begin{array}{*{20}{c}}
  {\phi _1^2 + {\phi _2}}&amp;{{\phi _1}{\phi _2}} \\ 
  {{\phi _1}}&amp;{{\phi _2}} 
\end{array}} \right]\left[ {\begin{array}{*{20}{c}}
  {{X_t}} \\ 
  {{X_{t - 1}}} 
\end{array}} \right] = \left[ {\begin{array}{*{20}{c}}
  {\left( {\phi _1^2 + {\phi _2}} \right){X_t} + {\phi _1}{\phi _2}{X_{t - 1}}} \\ 
  {{\phi _1}{X_t} + {\phi _2}{X_{t - 1}}} 
\end{array}} \right] \hfill \\ 
\end{aligned} \]</span>
</div>

<p>The above examples have given insight as to how to compute predictors for the AR(p) models that we have studied this far. Let us now observe the consequences of using such an approach to predict future values from these models. For this purpose, using the recursive formulation seen in the examples above we perform a simulation study where, for a fixed set of parameters, we make 5000 predictions from an observed time series and predict 50 values ahead into the future. The known parameters for the AR(2) process we use for the simulation study are <span class="math inline">\(\phi _1 = 0.75\)</span>, <span class="math inline">\(\phi _2 = 0.2\)</span> and <span class="math inline">\(\sigma^2 = 1\)</span>. The figure below shows the distribution of these predictions starting from the last observation <span class="math inline">\(T = 200\)</span>.</p>
<div class="figure"><span id="fig:predplot"></span>
<img src="ts_files/figure-html/predplot-1.png" alt="Values of the AR(2) predictions with the pink line being the median prediction, red and green lines are respectively 95% and 75% Confidence intervals." width="672" />
<p class="caption">
Figure 4.8: Values of the AR(2) predictions with the pink line being the median prediction, red and green lines are respectively 95% and 75% Confidence intervals.
</p>
</div>
<p>It can be observed that, as hinted by the expressions for the variance of the predictions (in the examples above), the variability of the predictions increases as we try to predict further into the future.</p>
<p>Having now defined the basic concepts for forecasting of time series we can now start another topic of considerable importance for time series analysis. Indeed, the whole discussion on prediction is not only important to derive forecasts for future values of the phenomena one may be interested in, but also in understanding how well a model explains (and predicts) an observed time series. In fact, predictions allow to deliver residuals within the time series setting and, based on these residuals, we can obtain different inference and diagnostic tools.</p>
<p>Given the knowledge on predictions seen above, residuals can be computed as</p>
<p><span class="math display">\[r_{t+j} = X_{t+j} - \hat{X}_{t+j},\]</span>
where <span class="math inline">\(\hat{X}_{t+j}\)</span> represents an estimator of the conditional expectation <span class="math inline">\(E[X_{t+j} | \Omega_{t}]\)</span>. The latter quantity will depend on the model underlying the time series and, assuming we know the true model, we could use the true parameters to obtain a value for this expectation. However, in an applied setting it is improbable that we know the true parameters and therefore <span class="math inline">\(\hat{X}_{t+j}\)</span> can be obtained by estimating the parameters (as seen in the previous sections) and then plugging thmem into the expression of <span class="math inline">\(E[X_{t+j} | \Omega_{t}]\)</span>.</p>
</div>
</div>
<div id="diagnostic-tools-for-time-series" class="section level2">
<h2><span class="header-section-number">4.3</span> Diagnostic Tools for Time Series</h2>
<p>The standard approach to understanding how well a model fits the data is analysing the residuals (e.g. linear regression). The same approach can be taken for time series analysis where, given our knowledge on forecasts from the previous section, we can now deliver residuals. The first step (and possibly the most important) is to use visual tools to check the residuals and also the original time series. Below is a figure that collects different diagnostic tools for time series analysis and is applied to a simulated AR(1) process of length <span class="math inline">\(T = 100\)</span>.</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb18-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">333</span>)</a>
<a class="sourceLine" id="cb18-2" data-line-number="2">true_model =<span class="st"> </span><span class="kw">AR</span>(<span class="dt">phi =</span> <span class="fl">0.8</span>, <span class="dt">sigma2 =</span> <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb18-3" data-line-number="3">Xt =<span class="st"> </span><span class="kw">gen_gts</span>(<span class="dt">n =</span> <span class="dv">100</span>, <span class="dt">model =</span> true_model)</a>
<a class="sourceLine" id="cb18-4" data-line-number="4">model =<span class="st"> </span><span class="kw">estimate</span>(<span class="kw">AR</span>(<span class="dv">1</span>), Xt, <span class="dt">demean =</span> <span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb18-5" data-line-number="5"><span class="kw">check</span>(<span class="dt">model =</span> model)</a></code></pre></div>
<div class="figure" style="text-align: center">
<img src="ts_files/figure-html/diagnostic_plot-1.png" alt="Ljung-Box test p-values for the residuals of the fitted AR(1) model over lags $h = 1, ...., 20$." width="912" />
<p class="caption">
(#fig:diagnostic_plot)Ljung-Box test p-values for the residuals of the fitted AR(1) model over lags <span class="math inline">\(h = 1, ...., 20\)</span>.
</p>
</div>
<p>All plots refer to the residuals of the model-fit and aim at visually assessing whether the fitted time series model captures the dependence structure in the data. The first row contains plots that can be interpreted in the same manner as the residuals from a linear regression. The first plot simply represents the residuals over time and should show if there is any presence of trends, seasonality or heteroskedasticity. The second plot gives a histogram (and smoothed histrogram called a kernel) of the residuals which should be centered in zero and (possibly) have an approximate normal distribution. The latter hypothesis can also be checked using the third plot which consists in a Normal quantile-quantile plot and the residuals should lie on (or close to) the diagonal line representing correspondance between the distributions.</p>
<p>The first plot in the second row in the above figureshows the estimated ACF. It can be seen how the estimated autocorrelations all lie within the blue shaded area representing the confidence intervals. The second plot represents the Partial AutoCorrelation Function (PACF) and is another tool that will be investigated further on. In the meantime, suffice it to say that this tool can be interpreted as a different version of the ACF which measures autocorrelation conditionally on the previous lags (in some sense, it measures the direct impact of <span class="math inline">\(X_t\)</span> on <span class="math inline">\(X{t+h}\)</span> removing the influence of all observations inbetween). In this case, we see that also the estimated PACF lies within the confidence intervals and, along with the intepretation of the ACF, we can state that the model appears to fit the time series reasonably well since the residuals can be considered as following a white noise process. Finally, the last plot visualises the “Ljung-Box Test” (which is a type of Portmanteau test) that tests whether the autocorrelation of the residuals at a set of lags is different from zero. The plot represents the p-value of this test at the different lags and also shows a dashed line representing the common significance level of <span class="math inline">\(\alpha = 0.05\)</span>. If the modelling has done a good job, all p-values should be larger than this level. In this case, using the latter level, we can see that the autocorrelations for the residuals appear to be non-significant and therefore fitting an AR(1) model to the time series (which is the true model in this example) appears to do a reasonably good job.</p>
<div id="the-partial-autocorrelation-function-pacf" class="section level3">
<h3><span class="header-section-number">4.3.1</span> The Partial AutoCorrelation Function (PACF)</h3>
<p>Before further discussing these diagnostic tools, let us focus a little more on the PACF mentioned earlier. As briefly highlighted above, this function measures the degree of linear correlation between two lagged observations by removing the effect of the observations in the intermediate lags. For example, let us assume we have three correlated variables <span class="math inline">\(X\)</span>, <span class="math inline">\(Y\)</span> and <span class="math inline">\(Z\)</span> and we wish to find the direct correlation between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>: we can apply a linear regression of <span class="math inline">\(X\)</span> on <span class="math inline">\(Z\)</span> (to obtain <span class="math inline">\(\hat{X}\)</span>) and <span class="math inline">\(Y\)</span> on <span class="math inline">\(Z\)</span> (to obtain <span class="math inline">\(\hat{Y}\)</span>) and thereby compute <span class="math inline">\(corr(\hat{X}, \hat{Y})\)</span> which represents the partial autocorrelation. In this manner, the effect of <span class="math inline">\(Z\)</span> on the correlation between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is removed. Considering the variables as being indexed by time (i.e. a time series), we can therefore define the following quantities
<span class="math display">\[\hat{X}_{t+h} = \beta_1 X_{t+h-1} + \beta_2 X_{t+h-2} + ... + \beta_{h-1} X_{t+1},\]</span>
and</p>
<p><span class="math display">\[\hat{X}_{t} = \beta_1 X_{t+1} + \beta_2 X_{t+2} + ... + \beta_{h-1} X_{t+h-1},\]</span>
which represent the regressions of <span class="math inline">\(X_{t+h}\)</span> and <span class="math inline">\(X_t\)</span> on all intermediate observations. With these defintions, let us more formally define the PACF.</p>

<div class="definition">
<span id="def:pacf" class="definition"><strong>Definition 4.3  (Partial AutoCorrelation Function)  </strong></span>The partial autocorrelation function of a (weakly) stationary time series <span class="math inline">\((X_t)\)</span> is defined as
<span class="math display">\[\rho_{1,1} = \operatorname{corr}(X_{t+1}, X_t) = \rho(1),\]</span>
and
<span class="math display">\[\rho_{h,h} = \operatorname{corr}((X_{t+h} - \hat{X}_{t+h}), (X_t - \hat{X}_t)), \, h &gt; 1.\]</span>
</div>

<p>Therefore, the PACF is not defined at lag zero while it is the same as the ACF at the first lag since there are no intermediate observations. The defintion of this function is important for different reasons. Firstly, as mentioned earlier, we have an asymptotic distribution for the estimator of the PACF which allows us to deliver confidence intervals as for the ACF. Indeed, representing the empirical PACF along with its confidence intervals can provide a further tool to make sure that the residuals of a model fit can be considered as being white noise. The latter is a reasonable hypothesis if both the empirical ACF and PACF lie within the confidence intervals indicating that there is no significant correlation between lagged variables. Let us focus on the empirical PACF of the residuals we saw earlier in the diagnostic plot.</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb19-1" data-line-number="1">residuals =<span class="st"> </span>model<span class="op">$</span>mod<span class="op">$</span>residuals</a>
<a class="sourceLine" id="cb19-2" data-line-number="2"><span class="kw">plot</span>(<span class="kw">auto_corr</span>(residuals, <span class="dt">pacf =</span> <span class="ot">TRUE</span>))</a></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-16"></span>
<img src="ts_files/figure-html/unnamed-chunk-16-1.png" alt="Empirical PACF on the residuals of the AR(1) model fitted to the simulated time series generated from an AR(1) process." width="576" />
<p class="caption">
Figure 4.9: Empirical PACF on the residuals of the AR(1) model fitted to the simulated time series generated from an AR(1) process.
</p>
</div>
<p>From the plot we see that the estimated PACF at all lags lies within the blue shaded area representing the 95% confidence intervals indicating that the residuals don’t appear to have any form of direct autocorrelation over lags and, hence, that they can be considered as white noise. In addition to delivering a tool to analyse the residuals of a model fit, the PACF is especially useful for diagnostic purposes in order to detect the possible models that have generated an observed time series. With respect to the class of models studied this far (i.e. AR(<span class="math inline">\(p\)</span>) models), the PACF can give important insight to the order of the AR(<span class="math inline">\(p\)</span>) model, more specifically the value of <span class="math inline">\(p\)</span>. Indeed, the ACF of an AR(<span class="math inline">\(p\)</span>) model tends to decrease in a sinusoidal fashion and exponetially fast to zero as the lag <span class="math inline">\(h\)</span> increases thereby giving a hint that the underlying model belongs to the AR(<span class="math inline">\(p\)</span>) family. However, aside from hinting to the fact that the model belongs to the family of AR(<span class="math inline">\(p\)</span>) models, the latter function tends to be less informative with respect to which AR(<span class="math inline">\(p\)</span>) model (i.e. which value of <span class="math inline">\(p\)</span>) is best suited for the observed time series. The PACF on the other hand is non-zero for the first <span class="math inline">\(p\)</span> lags and then becomes zero for <span class="math inline">\(h &gt; p\)</span>, therefore it can give an important indication with respect to which order should be considered to best model the time series.</p>
<p>With the above discussion in mind, let us consider some examples where we study the theoretical ACF and PACF of different AR(<span class="math inline">\(p\)</span>) models. The first is an AR(1) model with parameter <span class="math inline">\(\phi = 0.95\)</span> and its ACF and PACF are shown below.</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb20-1" data-line-number="1"><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</a>
<a class="sourceLine" id="cb20-2" data-line-number="2"><span class="kw">plot</span>(<span class="kw">theo_acf</span>(<span class="dt">ar =</span> <span class="fl">0.95</span>, <span class="dt">ma =</span> <span class="ot">NULL</span>))</a>
<a class="sourceLine" id="cb20-3" data-line-number="3"><span class="kw">plot</span>(<span class="kw">theo_pacf</span>(<span class="dt">ar =</span> <span class="fl">0.95</span>, <span class="dt">ma =</span> <span class="ot">NULL</span>))</a></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-17"></span>
<img src="ts_files/figure-html/unnamed-chunk-17-1.png" alt="Theoretical ACF (left) and PACF (right) of an AR(1) model with parameter $\phi = 0.95$." width="768" />
<p class="caption">
Figure 4.10: Theoretical ACF (left) and PACF (right) of an AR(1) model with parameter <span class="math inline">\(\phi = 0.95\)</span>.
</p>
</div>
<p>We can see that the ACF decreases exponentially fast as we would expect for an AR(p) model while the PACF has the same values of the ACF at lag 1 (which is always the case) and is zero for <span class="math inline">\(h &gt; 1\)</span>. The latter therefore indicates the order <span class="math inline">\(p\)</span> of the AR(<span class="math inline">\(p\)</span>) model which in this case is <span class="math inline">\(p=1\)</span>. Let us now consider the following models as further examples:</p>
<p><span class="math display">\[X_t = 0.5 X_{t-1} + 0.25 X_{t-2} + 0.125 X_{t-3} + W_t\]</span></p>
<p>and</p>
<p><span class="math display">\[X_t = -0.1 X_{t-2} -0.1 X_{t-4} + 0.6 X_{t-5} + W_t.\]</span></p>
<p>The first model is an AR(3) model with positive coefficients while the second is an AR(5) model where <span class="math inline">\(\phi_1 = \phi_3 = 0\)</span>. The ACF and PACF of the first AR(3) model is shown below.</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb21-1" data-line-number="1"><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</a>
<a class="sourceLine" id="cb21-2" data-line-number="2"><span class="kw">plot</span>(<span class="kw">theo_acf</span>(<span class="dt">ar =</span> <span class="kw">c</span>(<span class="fl">0.5</span>, <span class="fl">0.25</span>, <span class="fl">0.125</span>), <span class="dt">ma =</span> <span class="ot">NULL</span>))</a>
<a class="sourceLine" id="cb21-3" data-line-number="3"><span class="kw">plot</span>(<span class="kw">theo_pacf</span>(<span class="dt">ar =</span> <span class="kw">c</span>(<span class="fl">0.5</span>, <span class="fl">0.25</span>, <span class="fl">0.125</span>), <span class="dt">ma =</span> <span class="ot">NULL</span>))</a></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-18"></span>
<img src="ts_files/figure-html/unnamed-chunk-18-1.png" alt="Theoretical ACF (left) and PACF (right) of an AR(3) model with parameters $\phi_1 = 0.95$, $\phi_2 = 0.25$ and $\phi_3 = 0.125$." width="768" />
<p class="caption">
Figure 4.11: Theoretical ACF (left) and PACF (right) of an AR(3) model with parameters <span class="math inline">\(\phi_1 = 0.95\)</span>, <span class="math inline">\(\phi_2 = 0.25\)</span> and <span class="math inline">\(\phi_3 = 0.125\)</span>.
</p>
</div>
<p>Again, we see that the ACF decreases exponentially and also the PACF plot shows a steady decrease until it becomes zero for <span class="math inline">\(h &gt; 3\)</span>. This is because the values of <span class="math inline">\(\phi_i\)</span> are all positive and decreasing as well. Let us now consider the second model (the AR(5) model) whose ACF and PACF plots are represented below.</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb22-1" data-line-number="1"><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</a>
<a class="sourceLine" id="cb22-2" data-line-number="2"><span class="kw">plot</span>(<span class="kw">theo_acf</span>(<span class="dt">ar =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="fl">-0.1</span>, <span class="dv">0</span>, <span class="fl">-0.1</span>, <span class="fl">0.6</span>), <span class="dt">ma =</span> <span class="ot">NULL</span>))</a>
<a class="sourceLine" id="cb22-3" data-line-number="3"><span class="kw">plot</span>(<span class="kw">theo_pacf</span>(<span class="dt">ar =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="fl">-0.1</span>, <span class="dv">0</span>, <span class="fl">-0.1</span>, <span class="fl">0.6</span>), <span class="dt">ma =</span> <span class="ot">NULL</span>))</a></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-19"></span>
<img src="ts_files/figure-html/unnamed-chunk-19-1.png" alt="Theoretical ACF (left) and PACF (right) of an AR(4) model with parameters $\phi_1 = 0$, $\phi_2 = -0.1$, $\phi_3 = 0$ and $\phi_4 = 0$." width="768" />
<p class="caption">
Figure 4.12: Theoretical ACF (left) and PACF (right) of an AR(4) model with parameters <span class="math inline">\(\phi_1 = 0\)</span>, <span class="math inline">\(\phi_2 = -0.1\)</span>, <span class="math inline">\(\phi_3 = 0\)</span> and <span class="math inline">\(\phi_4 = 0\)</span>.
</p>
</div>
<p>In this case we see that the ACF has a sinusoidal-like behaviour where the values of <span class="math inline">\(\phi_1 = \phi_3 = 0\)</span> and the negative <span class="math inline">\(\phi_2 = \phi_4 = -0.1\)</span> values deliver this alternating ACF form which nevertheless decreases as <span class="math inline">\(h\)</span> increases. These values deliver the PACF plot on the right which is negative for the first lags and is greatly positive at <span class="math inline">\(h=5\)</span> only to become zero for <span class="math inline">\(h &gt; 5\)</span>.</p>
<p>These examples therefore give us further insight as to how to interpret the empirical (or estimated) versions of these functions. For this reason, let us study the empirical ACF and PACF of some real time series, the first of which is the data representing the natural logarithm of the annual number of Lynx trappings in Canada between 1821 and 1934. The time series is represented below.</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb23-1" data-line-number="1">lynx_gts =<span class="st"> </span><span class="kw">gts</span>(<span class="kw">log</span>(lynx), <span class="dt">start =</span> <span class="dv">1821</span>, <span class="dt">data_name =</span> <span class="st">&quot;Lynx Trappings&quot;</span>, <span class="dt">unit_time =</span> <span class="st">&quot;year&quot;</span>, <span class="dt">name_ts =</span> <span class="st">&quot;Trappings&quot;</span>)</a>
<a class="sourceLine" id="cb23-2" data-line-number="2"><span class="kw">plot</span>(lynx_gts)</a></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-21"></span>
<img src="ts_files/figure-html/unnamed-chunk-21-1.png" alt="Time series of annual number of lynx data trappings in Canada between 1821 and 1934." width="672" />
<p class="caption">
Figure 4.13: Time series of annual number of lynx data trappings in Canada between 1821 and 1934.
</p>
</div>
<p>We can see that there appears to be a seasonal trend within the data but let us ignore this for the moment and check the ACF and PACF plots below.</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb24-1" data-line-number="1"><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</a>
<a class="sourceLine" id="cb24-2" data-line-number="2"><span class="kw">plot</span>(<span class="kw">auto_corr</span>(lynx_gts))</a>
<a class="sourceLine" id="cb24-3" data-line-number="3"><span class="kw">plot</span>(<span class="kw">auto_corr</span>(lynx_gts, <span class="dt">pacf =</span> <span class="ot">TRUE</span>))</a></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-22"></span>
<img src="ts_files/figure-html/unnamed-chunk-22-1.png" alt="Empirical ACF (left) and PACF (right) of the lynx time series data." width="768" />
<p class="caption">
Figure 4.14: Empirical ACF (left) and PACF (right) of the lynx time series data.
</p>
</div>
<p>The seasonal behaviour also appears in the ACF plot but we see that it decreases in a sinusoidal fashion as the lag increases hinting that an AR(p) model could be a potential candidate for the time series. Looking at the PACF plot on the right we can see that a few partial autocorrelations appear to be significant up to lag <span class="math inline">\(h=11\)</span>. Therefore an AR(11) model could be a possibly good candidate to explain (and predict) this time series.</p>
</div>
<div id="portmanteau-tests" class="section level3">
<h3><span class="header-section-number">4.3.2</span> Portmanteau Tests</h3>
<p>In a similar manner to using the ACF and PACF confidence intervals to understand if the residuals can be considered as white noise, other statistical tests exist to determine if the autocorrelations can be considered as being significant or not. Indeed, the 95% confidence intervals can be extremely useful in detecting significant (partial) autocorrelations but they cannot be considered as an overall test for white noise since (aside from being asymptotic and therefore approximate) they do not consider the multiple testing hypothesis implied by them (i.e. if we’re testing all lags, the actual level of significance should be modified in order to bound type I errors). For this reason, Portmanteau tests have been proposed in order to deliver an overall test that jointly considers a set of lags and determines whether their autocorrelation is significantly different from zero (i.e. whether the residuals can be considered white noise or not).</p>
<p>One of the most popular Portmanteau tests is the Ljung-Box test whose statistic is defined as follows:</p>
<p><span class="math display">\[Q_h = T(T+2) \sum_{j=1}^{h} \frac{\hat{\rho}_j^2}{T - j},\]</span>
where <span class="math inline">\(h\)</span> represents the maximum lag of the set of lags for which we wish to test for serial autocorrelation (i.e. from lag 1 to lag <span class="math inline">\(h\)</span>). Under the null hypothesis <span class="math inline">\(Q_h \sim \chi_h^2\)</span> since asymptotically <span class="math inline">\(\hat{\rho}_j \sim \mathcal{N}\left(0, \frac{1}{T}\right)\)</span> under the null and therefore the statistic is proportional to the sum of squared standard normal variables. The last plot of Figure @ref(fig:diagnostic_plot) can therefore be better interpreted under this framework and is reproduced below.</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb25-1" data-line-number="1">lb_test =<span class="st"> </span><span class="kw">diag_ljungbox</span>(<span class="kw">as.numeric</span>(residuals), <span class="dt">order =</span> <span class="dv">0</span>, <span class="dt">stop_lag =</span> <span class="dv">20</span>, <span class="dt">stdres =</span> <span class="ot">FALSE</span>, <span class="dt">plot =</span> <span class="ot">TRUE</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-24"></span>
<img src="ts_files/figure-html/unnamed-chunk-24-1.png" alt="Ljung-Box p-values for lags $h = 1,...,20$ on the residuals of the fitted AR(1) model." width="576" />
<p class="caption">
Figure 4.15: Ljung-Box p-values for lags <span class="math inline">\(h = 1,...,20\)</span> on the residuals of the fitted AR(1) model.
</p>
</div>
<p>Each p-value represented in the plot above is therefore the p-value for the considered maximum lag. Therefore the first p-value (at lag 1) is the p-value for the null hypothesis that all lags up to lag 1 are equal to zero, the second is for the null hypothesis that all lags up to lag 2 are equal to zero and so on. Hence, each p-value represents a global test on the autocorrelations up to the considered maximum lag <span class="math inline">\(h\)</span> and delivers additional information regarding the dependence structure within the residuals. In this case, as interpreted earlier, all p-values are larger than the <span class="math inline">\(\alpha = 0.05\)</span> level and is an additional indication that the residuals follow a white noise process (and that the fitted model appears to well describe the observed time series).</p>
<p>There are other Portmanteau tests with different advantages (and disadvantages) over the Ljung-Box test but these are beyond the scope of this textbook.</p>
</div>
</div>
<div id="inference-for-arp-models" class="section level2">
<h2><span class="header-section-number">4.4</span> Inference for AR(p) Models</h2>
<p>For all the above methods, it would be necessary to understand how “precise” their estimates are. To do so we would need to obtain confidence intervals for these estimates and this can be done mainly in two manners:</p>
<ul>
<li>using the asymptotic distribution of the parameter estimates;</li>
<li>using parametric bootstrap.</li>
</ul>
<p>The first approach consists in using the asymptotic distribution of the estimators presented earlier to deliver approximations of the confidence intervals which get better as the length of the observed time series increases. Hence, if for example we wanted to find a 95% confidence interval for the parameter <span class="math inline">\(\phi\)</span>, we would use the quantiles of the normal distribution (given that all methods presented earlier present this asymptotic distribution). However, this approach can present some drawbacks, one of which its behaviour when the parameters are close to the boundaries of the parameter space. Suppose we consider a realization of length <span class="math inline">\(T = 100\)</span> of the following AR(1) model:</p>
<p><span class="math display">\[X_t = 0.96 X_{t-1} + W_t, \;\;\;\; W_t \sim \mathcal{N}(0,1),\]</span></p>
<p>which is represented in Figure <a href="the-family-of-autoregressive-moving-average-models.html#fig:simAR1ci">4.16</a></p>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb26-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">55</span>)</a>
<a class="sourceLine" id="cb26-2" data-line-number="2">x =<span class="st"> </span><span class="kw">gen_gts</span>(<span class="dt">n =</span> <span class="dv">100</span>, <span class="kw">AR1</span>(<span class="fl">0.96</span>, <span class="dv">1</span>))</a>
<a class="sourceLine" id="cb26-3" data-line-number="3"><span class="kw">plot</span>(x)</a></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:simAR1ci"></span>
<img src="ts_files/figure-html/simAR1ci-1.png" alt="AR(1) with $\phi$ close to parameter bound" width="672" />
<p class="caption">
Figure 4.16: AR(1) with <span class="math inline">\(\phi\)</span> close to parameter bound
</p>
</div>
<p>It can be seen that the parameter <span class="math inline">\(\phi = 0.96\)</span> respects the condition for stationarity (i.e.<span class="math inline">\(\left| \phi \right| &lt; 1\)</span>) but is very close to its boundary. Using the MLE, we first estimate the parameters and then compute confidence intervals for <span class="math inline">\(\phi\)</span> using the asymptotic normal distribution.</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb27-1" data-line-number="1"><span class="co"># Compute the parameter estimates using MLE</span></a>
<a class="sourceLine" id="cb27-2" data-line-number="2">fit.ML =<span class="st"> </span><span class="kw">estimate</span>(<span class="kw">AR</span>(<span class="dv">1</span>), x, <span class="dt">demean =</span> <span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb27-3" data-line-number="3"><span class="kw">c</span>(<span class="st">&quot;phi&quot;</span> =<span class="st"> </span>fit.ML<span class="op">$</span>mod<span class="op">$</span>coef[<span class="dv">1</span>], <span class="st">&quot;sigma2&quot;</span> =<span class="st"> </span><span class="kw">sqrt</span>(fit.ML<span class="op">$</span>mod<span class="op">$</span>sigma2))</a></code></pre></div>
<pre><code>##   phi.ar1    sigma2 
## 0.9740770 0.8538777</code></pre>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb29-1" data-line-number="1"><span class="co"># Construct asymptotic confidence interval for phi</span></a>
<a class="sourceLine" id="cb29-2" data-line-number="2">fit.ML<span class="op">$</span>mod<span class="op">$</span>coef[<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)<span class="op">*</span><span class="fl">1.96</span><span class="op">*</span><span class="kw">as.numeric</span>(<span class="kw">sqrt</span>(fit.ML<span class="op">$</span>mod<span class="op">$</span>var.coef))</a></code></pre></div>
<pre><code>## [1] 0.9393414 1.0088127</code></pre>
<pre><code>## Fitted model: AR(1)
## 
## Estimated parameters:
## Model Information: 
##        Estimates
## AR     0.9864031
## SIGMA2 0.7644610
## 
## * The initial values of the parameters used in the minimization of the GMWM objective function 
##   were generated by the program underneath seed: 1337. 
## 
## 95 % confidence intervals:
##        Estimates    CI Low  CI High         SE
## AR     0.9864031 0.8688122 1.054986 0.05659269
## SIGMA2 0.7644610 0.5565939 0.926104 0.11232308</code></pre>
<p>From the estimation summary, we can notice that the MLE confidence intervals contain values that would make the AR(1) non-stationary (i.e. values of <span class="math inline">\(\phi\)</span> larger than 1). However, these confidence intervals are based on the (asymptotic) distributions of <span class="math inline">\(\hat{\phi}\)</span> which are shown in Figure <a href="the-family-of-autoregressive-moving-average-models.html#fig:asymIC">4.17</a> along with those based on the GMWM estimator.</p>
<div class="figure"><span id="fig:asymIC"></span>
<img src="ts_files/figure-html/asymIC-1.png" alt="Estimated asymptotic distribution of $\hat{\phi}$ for MLE and GMWM parameter estimates. The dashed vertical line represents the true value of $\phi$, the solid line denotes the upper bound of the parameter space for $\phi$ and the vertical ticks represent the limits of the 95% confidence intervals for both methods." width="864" />
<p class="caption">
Figure 4.17: Estimated asymptotic distribution of <span class="math inline">\(\hat{\phi}\)</span> for MLE and GMWM parameter estimates. The dashed vertical line represents the true value of <span class="math inline">\(\phi\)</span>, the solid line denotes the upper bound of the parameter space for <span class="math inline">\(\phi\)</span> and the vertical ticks represent the limits of the 95% confidence intervals for both methods.
</p>
</div>
<p>Therefore, if we estimate a stationary AR(1) model, it would be convenient to have more “realistic” confidence intervals that give limits for a stationary AR(1) model. A viable solution for this purpose is to use parametric bootstrap. Indeed, parametric bootstrap takes the estimated parameter values and uses them in order to simulate from the assumed model (an AR(1) process in this case). For each simulation, the parameters are estimated and stored in order to obtain an empirical (finite sample) distribution of the estimators. Based on this distribution it is consequently possible to find the <span class="math inline">\(\alpha/2\)</span> and <span class="math inline">\(1-\alpha/2\)</span> empirical quantiles thereby delivering confidence intervals that should not suffer from boundary problems (since the estimation procedure looks for solutions within the admissable regions). The code below gives an example of how this confidence interval is built based on the same estimation procedure but using parametric bootstrap (using <span class="math inline">\(B = 10,000\)</span> bootstrap replicates).</p>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb32-1" data-line-number="1"><span class="co"># Number of Iterations</span></a>
<a class="sourceLine" id="cb32-2" data-line-number="2">B =<span class="st"> </span><span class="dv">10000</span></a>
<a class="sourceLine" id="cb32-3" data-line-number="3"></a>
<a class="sourceLine" id="cb32-4" data-line-number="4"><span class="co"># Set up storage for results</span></a>
<a class="sourceLine" id="cb32-5" data-line-number="5">est.phi.gmwm =<span class="st"> </span><span class="kw">rep</span>(<span class="ot">NA</span>,B)</a>
<a class="sourceLine" id="cb32-6" data-line-number="6">est.phi.ML =<span class="st"> </span><span class="kw">rep</span>(<span class="ot">NA</span>,B)</a>
<a class="sourceLine" id="cb32-7" data-line-number="7"></a>
<a class="sourceLine" id="cb32-8" data-line-number="8"><span class="co"># Model generation statements</span></a>
<a class="sourceLine" id="cb32-9" data-line-number="9">model.gmwm =<span class="st"> </span><span class="kw">AR</span>(<span class="dt">phi =</span> <span class="kw">c</span>(fit.gmwm<span class="op">$</span>mod<span class="op">$</span>estimate[<span class="dv">1</span>]), <span class="dt">sigma2 =</span> <span class="kw">c</span>(fit.gmwm<span class="op">$</span>mod<span class="op">$</span>estimate[<span class="dv">2</span>]))</a>
<a class="sourceLine" id="cb32-10" data-line-number="10">model.mle =<span class="st"> </span><span class="kw">AR</span>(<span class="dt">phi =</span> <span class="kw">c</span>(fit.ML<span class="op">$</span>mod<span class="op">$</span>coef), <span class="dt">sigma2 =</span> <span class="kw">c</span>(fit.ML<span class="op">$</span>mod<span class="op">$</span>sigma2))</a>
<a class="sourceLine" id="cb32-11" data-line-number="11"></a>
<a class="sourceLine" id="cb32-12" data-line-number="12"><span class="co"># Begin bootstrap</span></a>
<a class="sourceLine" id="cb32-13" data-line-number="13"><span class="cf">for</span>(i <span class="cf">in</span> <span class="kw">seq_len</span>(B)){</a>
<a class="sourceLine" id="cb32-14" data-line-number="14"></a>
<a class="sourceLine" id="cb32-15" data-line-number="15">  <span class="co"># Set seed for reproducibility</span></a>
<a class="sourceLine" id="cb32-16" data-line-number="16">  <span class="kw">set.seed</span>(B <span class="op">+</span><span class="st"> </span>i)</a>
<a class="sourceLine" id="cb32-17" data-line-number="17"></a>
<a class="sourceLine" id="cb32-18" data-line-number="18">  <span class="co"># Generate process under MLE parameter estimate</span></a>
<a class="sourceLine" id="cb32-19" data-line-number="19">  x.star =<span class="st"> </span><span class="kw">gen_gts</span>(<span class="dv">100</span>, model.mle)</a>
<a class="sourceLine" id="cb32-20" data-line-number="20"></a>
<a class="sourceLine" id="cb32-21" data-line-number="21">  <span class="co"># Attempt to estimate phi by employing a try</span></a>
<a class="sourceLine" id="cb32-22" data-line-number="22">  est.phi.ML[i] =<span class="st"> </span><span class="kw">tryCatch</span>(<span class="kw">estimate</span>(<span class="kw">AR</span>(<span class="dv">1</span>), x.star, <span class="dt">demean =</span> <span class="ot">FALSE</span>)<span class="op">$</span>mod<span class="op">$</span>coef,</a>
<a class="sourceLine" id="cb32-23" data-line-number="23">                           <span class="dt">error =</span> <span class="cf">function</span>(e) <span class="ot">NA</span>)</a>
<a class="sourceLine" id="cb32-24" data-line-number="24"></a>
<a class="sourceLine" id="cb32-25" data-line-number="25">  <span class="co"># Generate process under GMWM parameter estimate</span></a>
<a class="sourceLine" id="cb32-26" data-line-number="26">  x.star =<span class="st"> </span><span class="kw">gen_gts</span>(<span class="dv">100</span>, model.gmwm)</a>
<a class="sourceLine" id="cb32-27" data-line-number="27">  est.phi.gmwm[i] =<span class="st"> </span><span class="kw">estimate</span>(<span class="kw">AR</span>(<span class="dv">1</span>), x.star, <span class="dt">method =</span> <span class="st">&quot;gmwm&quot;</span>)<span class="op">$</span>mod<span class="op">$</span>estimate[<span class="dv">1</span>]</a>
<a class="sourceLine" id="cb32-28" data-line-number="28">}</a></code></pre></div>
<div class="figure"><span id="fig:paraAR1cigraphs"></span>
<img src="ts_files/figure-html/paraAR1cigraphs-1.png" alt="Parametric bootstrap distributions of $\hat{\phi}$ for the MLE and GMWM parameter estimates. The histograms represent the empirical distributions resulting from the parametric bootstrap while the continuous densities represent the estimated asymptotic distributions. The vertical line represents the true value of $\phi$, the dark solid lines represent the upper bound of the parameter space for $\phi$ and the vertical ticks represent the limits of the 95% confidence intervals for the asymptotic distributions (red and gold) and the empirical distributions (green and blue)." width="864" />
<p class="caption">
Figure 4.18: Parametric bootstrap distributions of <span class="math inline">\(\hat{\phi}\)</span> for the MLE and GMWM parameter estimates. The histograms represent the empirical distributions resulting from the parametric bootstrap while the continuous densities represent the estimated asymptotic distributions. The vertical line represents the true value of <span class="math inline">\(\phi\)</span>, the dark solid lines represent the upper bound of the parameter space for <span class="math inline">\(\phi\)</span> and the vertical ticks represent the limits of the 95% confidence intervals for the asymptotic distributions (red and gold) and the empirical distributions (green and blue).
</p>
</div>
<p>In Figure <a href="the-family-of-autoregressive-moving-average-models.html#fig:paraAR1cigraphs">4.18</a>, we compare the estimated densities for <span class="math inline">\(\hat{\phi}\)</span> using asymptotic results and bootstrap techniques for the MLE and the GMWM estimators. It can be observed that the issue that arose previously with “non-stationary” confidence intervals does not occur with the parametric bootstrap approach since the interval regions of the latter lie entirely within the boundaries of the admissable parameter space.</p>
<p>To further emphasize the advantages of parametric bootstrap, let us consider one additonal example of an AR(2) process. For this example, discussion will focus solely on the behaviour of the MLE. Having said this, the considered AR(2) process is defined as follows:</p>
<span class="math display" id="eq:ciar2">\[\begin{equation}
{X_t} = {1.98}{X_{t - 1}} - {0.99}{X_{t - 2}} + {W_t}
\tag{4.2}
\end{equation}\]</span>
<p>where <span class="math inline">\(W_t \sim \mathcal{N}(0, 1)\)</span>. The generated process is displayed in Figure <a href="the-family-of-autoregressive-moving-average-models.html#fig:CIAR2data">4.19</a>.</p>
<div class="sourceCode" id="cb33"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb33-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">432</span>)</a>
<a class="sourceLine" id="cb33-2" data-line-number="2">Xt =<span class="st"> </span><span class="kw">gen_gts</span>(<span class="dv">500</span>, <span class="kw">AR</span>(<span class="dt">phi =</span> <span class="kw">c</span>(<span class="fl">1.98</span>, <span class="fl">-0.99</span>), <span class="dt">sigma2 =</span> <span class="dv">1</span>))</a>
<a class="sourceLine" id="cb33-3" data-line-number="3"><span class="kw">plot</span>(Xt)</a></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:CIAR2data"></span>
<img src="ts_files/figure-html/CIAR2data-1.png" alt="Generated AR(2) Process" width="672" />
<p class="caption">
Figure 4.19: Generated AR(2) Process
</p>
</div>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb34-1" data-line-number="1">fit =<span class="st"> </span><span class="kw">estimate</span>(<span class="kw">AR</span>(<span class="dv">2</span>), Xt, <span class="dt">demean =</span> <span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb34-2" data-line-number="2">fit</a></code></pre></div>
<pre><code>## Fitted model: AR(2)
## 
## Estimated parameters:
## 
## Call:
## arima(x = as.numeric(Xt), order = c(p, intergrated, q), seasonal = list(order = c(P, 
##     seasonal_intergrated, Q), period = s), include.mean = demean, method = meth)
## 
## Coefficients:
##          ar1      ar2
##       1.9787  -0.9892
## s.e.  0.0053   0.0052
## 
## sigma^2 estimated as 0.8419:  log likelihood = -672.58,  aic = 1351.15</code></pre>
<p>Having estimated the model’s coefficients, we can again perform the parametric bootstrap. It must be noticed that in these simulations there can be various cases where a method may fail to estimate due to numerical issues (e.g. numerically singular covariance matrices, etc.) and therefore we add a check for each estimation to ensure that we preserve those estimations that have succeeded.</p>
<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb36-1" data-line-number="1">B =<span class="st"> </span><span class="dv">500</span></a>
<a class="sourceLine" id="cb36-2" data-line-number="2">est.phi.ML =<span class="st"> </span><span class="kw">matrix</span>(<span class="ot">NA</span>, B, <span class="dv">2</span>)</a>
<a class="sourceLine" id="cb36-3" data-line-number="3">model =<span class="st"> </span><span class="kw">AR</span>(<span class="dt">phi =</span> fit<span class="op">$</span>mod<span class="op">$</span>coef, <span class="dt">sigma2 =</span> fit<span class="op">$</span>mod<span class="op">$</span>sigma2)</a>
<a class="sourceLine" id="cb36-4" data-line-number="4"></a>
<a class="sourceLine" id="cb36-5" data-line-number="5"><span class="cf">for</span>(i <span class="cf">in</span> <span class="kw">seq_len</span>(B)) {</a>
<a class="sourceLine" id="cb36-6" data-line-number="6">  <span class="kw">set.seed</span>(B <span class="op">+</span><span class="st"> </span>i)              <span class="co"># Set Seed for reproducibilty</span></a>
<a class="sourceLine" id="cb36-7" data-line-number="7">  x.star =<span class="st"> </span><span class="kw">gen_gts</span>(<span class="dv">500</span>, model) <span class="co"># Simulate the process</span></a>
<a class="sourceLine" id="cb36-8" data-line-number="8"></a>
<a class="sourceLine" id="cb36-9" data-line-number="9">  <span class="co"># Obtain parameter estimate with protection</span></a>
<a class="sourceLine" id="cb36-10" data-line-number="10">  <span class="co"># that defaults to NA if unable to estimate</span></a>
<a class="sourceLine" id="cb36-11" data-line-number="11">  est.phi.ML[i,] =<span class="st"> </span><span class="kw">tryCatch</span>(<span class="kw">estimate</span>(<span class="kw">AR</span>(<span class="dv">2</span>), x.star, <span class="dt">demean =</span> <span class="ot">FALSE</span>)<span class="op">$</span>mod<span class="op">$</span>coef,</a>
<a class="sourceLine" id="cb36-12" data-line-number="12">  <span class="dt">error =</span> <span class="cf">function</span>(e) <span class="ot">NA</span>)</a>
<a class="sourceLine" id="cb36-13" data-line-number="13">}</a></code></pre></div>
<p>Once this is done, we can compare the bivariate distributions (asymptotic and bootsrapped) of the two estimated parameters <span class="math inline">\(\hat{\phi}_1\)</span> and <span class="math inline">\(\hat{\phi}_2\)</span>.</p>
<div class="figure"><span id="fig:ciAR2d"></span>
<img src="ts_files/figure-html/ciAR2d-1.png" alt="Estimated distributions of $\hat{\phi_1}$ and $\hat{\phi_2}$ based on the MLE using asymptotic and parametric bootstrap techniques. The colored contours represent the density of the distributions and the dark grey lines represent the boundary constraints of $\left|\phi_2\right|&lt;1$ and $\phi_2 = 1 - \phi_1$." width="864" />
<p class="caption">
Figure 4.20: Estimated distributions of <span class="math inline">\(\hat{\phi_1}\)</span> and <span class="math inline">\(\hat{\phi_2}\)</span> based on the MLE using asymptotic and parametric bootstrap techniques. The colored contours represent the density of the distributions and the dark grey lines represent the boundary constraints of <span class="math inline">\(\left|\phi_2\right|&lt;1\)</span> and <span class="math inline">\(\phi_2 = 1 - \phi_1\)</span>.
</p>
</div>
<p>As for any other estimation and inference procedure, these confidence intervals rely on the assumption that the chosen model (AR(2) in this case) is the true model underlying the observed time series. However, if this assumption were not correct, then the derived confidence intervals would not be correct and this could possibly lead to bad conclusions. For this reason, one could decide to use a non-parametric approach to computing these confidence intervals, such as the traditional bootstrap that consists in a simple random sampling with replacement from the original data. However, this approach would not be appropriate in the presence of dependence between observations since this procedure would break the dependence structure and therefore deliver unreliable conclusions. Therefore, to preserve the dependence structure of the original data one option would be to use <em>block bootstrapping</em>, which is a particular form of non-parametric bootstrap. There are many different kinds of block-bootstrap procedures for time series which are all related to the Moving Block Bootstrap (MBB) which is defined as follows.</p>

<div class="definition">
<p><span id="def:mbb" class="definition"><strong>Definition 4.4  </strong></span>Suppose that <span class="math inline">\(\left(X_t\right)\)</span> is a weakly stationary time series with <span class="math inline">\(T\)</span> observations. The procedure is as follows:</p>
<ol style="list-style-type: decimal">
<li>Divide the time series into overlapping blocks <span class="math inline">\(\left\{S_1, \ldots, S_M\right\}\)</span> of length <span class="math inline">\(\ell\)</span>, <span class="math inline">\(1 \le \ell &lt; N\)</span>, resulting in <span class="math inline">\(M = N - \ell + 1\)</span> blocks structured as:</li>
</ol>
<p><span class="math display">\[\begin{aligned}
  {S_1}&amp; = &amp; ({X_1}, &amp;{X_2}, \cdots , {X_\ell}) &amp; &amp;&amp; \\
  {S_2}&amp; = &amp; &amp;( {X_2}, {X_3}, \cdots , {X_{\ell + 1}}) &amp; &amp;&amp; \\
  &amp; \cdots &amp; &amp; {} &amp; \cdots &amp;&amp; \\
  {S_M} &amp; = &amp; &amp; &amp; {} &amp;&amp;( {X_{N-\ell+1}}, {X_{N-\ell+2}}, \cdots , {X_{N}})
  \end{aligned}\]</span></p>
<ol start="2" style="list-style-type: decimal">
<li>Draw <span class="math inline">\(M = \left\lfloor {\frac{N}{\ell}} \right\rfloor\)</span> blocks with replacement
from these <span class="math inline">\(\left\{S_1, \ldots, S_M\right\}\)</span> blocks and place them in order to form
<span class="math inline">\((X_t^*)\)</span>.</li>
<li>Compute the statistic of interest on the simulated
sample <span class="math inline">\((X_t^*)\)</span>.</li>
<li>Repeat Steps 2 and 3 <span class="math inline">\(B\)</span> times where <span class="math inline">\(B\)</span> is sufficiently “large”
(typically <span class="math inline">\(100 \leq B \leq 10,000\)</span>).</li>
<li>Compute the empirical mean and variance on the statistic of interest based on
the <span class="math inline">\(B\)</span> independent replications.
</div></li>
</ol>
<p>The approach taken by MBB ensures that within each block the dependency between observations is preserved. However, one particular issue that now arises is that some inaccuracy is introduced as a result of successive blocks potentially being independent from each other. In reality, this is one of the trade-offs of the MBB approach that can be mitigated by selecting an optimal <span class="math inline">\(\ell\)</span> (to improve the accuracy of the procedure one should choose <span class="math inline">\(\ell = T^{1/3}\)</span> as <span class="math inline">\(T \to \infty\)</span> as being the optimal choice). An earlier variant of MBB is called the Nonoverlapping Block Bootstrap (NBB) which doesn’t allow the blocks to share common data points and is presented below.</p>

<div class="definition">
<p><span id="def:nbb" class="definition"><strong>Definition 4.5  </strong></span>Suppose that <span class="math inline">\(\left(X_t\right)\)</span> is weakly stationary time series with <span class="math inline">\(T\)</span> observations.</p>
<ol style="list-style-type: decimal">
<li>Divide time series into nonoverlapping blocks <span class="math inline">\(\left\{S_1, \ldots, S_M\right\}\)</span> of
length <span class="math inline">\(\ell\)</span>, <span class="math inline">\(1 \le \ell &lt; N\)</span>, resulting in <span class="math inline">\(M = \left\lfloor {\frac{N}{\ell}} \right\rfloor\)</span> blocks structured as:</li>
</ol>
<p><span class="math display">\[\begin{aligned}
  {S_1}&amp; =  ({X_1}, {X_2}, \cdots , {X_\ell})&amp; &amp; &amp;&amp; \\
  {S_2}&amp; =  &amp;( {X_{\ell+1}}, {X_{\ell+2}}, \cdots , {X_{2\ell}}) &amp; &amp;&amp; \\
  &amp; \cdots  &amp; {} &amp; \cdots &amp;&amp; \\
  {S_K} &amp; =  &amp; &amp; {} &amp;&amp;( {X_{\left({N-\ell+1}\right)}}, {X_{N-\ell+2}}, \cdots , {X_{N}})
  \end{aligned}\]</span></p>
<ol start="2" style="list-style-type: decimal">
<li>Draw <span class="math inline">\(M\)</span> blocks with replacement
from these <span class="math inline">\(\left\{S_1, \ldots, S_M\right\}\)</span> blocks and place them in order to form
<span class="math inline">\((X_t^*)\)</span>.</li>
<li>Compute the statistic of interest on the simulated
sample <span class="math inline">\((X_t^*)\)</span>.</li>
<li>Repeat Steps 2 and 3 <span class="math inline">\(B\)</span> times where <span class="math inline">\(B\)</span> is sufficiently “large”
(typically <span class="math inline">\(100 \leq B \leq 10,000\)</span>).</li>
<li>Compute the empirical mean and variance on the statistic of interest based on
the <span class="math inline">\(B\)</span> independent replications.
</div></li>
</ol>
<p>Alternatively, depending on the case one can also use modifications of the MBB that seeks to change how the beginning and end of the time series is weighted such as a Circular Block-Bootstrap (CBB) or a Stationary Bootstrap (SB). The code below implements the MMB to obtain a bootrstrap distribution for the estimated parameters of an AR(1) model.</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb37-1" data-line-number="1">ar1_blockboot =<span class="st"> </span><span class="cf">function</span>(Xt, <span class="dt">block_len =</span> <span class="dv">10</span>, <span class="dt">B =</span> <span class="dv">500</span>) {</a>
<a class="sourceLine" id="cb37-2" data-line-number="2">  n =<span class="st"> </span><span class="kw">length</span>(Xt)            <span class="co"># Length of Time Series</span></a>
<a class="sourceLine" id="cb37-3" data-line-number="3">  res =<span class="st"> </span><span class="kw">rep</span>(<span class="ot">NA</span>, B)          <span class="co"># Bootstrapped Statistics</span></a>
<a class="sourceLine" id="cb37-4" data-line-number="4">  m =<span class="st"> </span><span class="kw">floor</span>(n<span class="op">/</span>block_len)    <span class="co"># Amount of Blocks</span></a>
<a class="sourceLine" id="cb37-5" data-line-number="5"></a>
<a class="sourceLine" id="cb37-6" data-line-number="6">  <span class="cf">for</span> (i <span class="cf">in</span> <span class="kw">seq_len</span>(B)) {   <span class="co"># Begin MMB</span></a>
<a class="sourceLine" id="cb37-7" data-line-number="7">    <span class="kw">set.seed</span>(i <span class="op">+</span><span class="st"> </span><span class="dv">1199</span>)      <span class="co"># Set seed for reproducibility</span></a>
<a class="sourceLine" id="cb37-8" data-line-number="8">    x_star =<span class="st"> </span><span class="kw">rep</span>(<span class="ot">NA</span>, n)     <span class="co"># Setup storage for new TS</span></a>
<a class="sourceLine" id="cb37-9" data-line-number="9"></a>
<a class="sourceLine" id="cb37-10" data-line-number="10">    <span class="cf">for</span> (j <span class="cf">in</span> <span class="kw">seq_len</span>(m)) { <span class="co"># Simulate new time series</span></a>
<a class="sourceLine" id="cb37-11" data-line-number="11">      index =<span class="st"> </span><span class="kw">sample</span>(m, <span class="dv">1</span>)  <span class="co"># Randomize block starting points</span></a>
<a class="sourceLine" id="cb37-12" data-line-number="12">      <span class="co"># Place block into time series</span></a>
<a class="sourceLine" id="cb37-13" data-line-number="13">      x_star[(block_len<span class="op">*</span>(j <span class="op">-</span><span class="st"> </span><span class="dv">1</span>) <span class="op">+</span><span class="st"> </span><span class="dv">1</span>)<span class="op">:</span>(block_len<span class="op">*</span>j)] =</a>
<a class="sourceLine" id="cb37-14" data-line-number="14"><span class="st">      </span>Xt[(block_len<span class="op">*</span>(index <span class="op">-</span><span class="st"> </span><span class="dv">1</span>) <span class="op">+</span><span class="st"> </span><span class="dv">1</span>)<span class="op">:</span>(block_len<span class="op">*</span>index)]</a>
<a class="sourceLine" id="cb37-15" data-line-number="15">    }</a>
<a class="sourceLine" id="cb37-16" data-line-number="16">    </a>
<a class="sourceLine" id="cb37-17" data-line-number="17">  <span class="co"># Calculate parameters with protection</span></a>
<a class="sourceLine" id="cb37-18" data-line-number="18">  res[i] =<span class="st"> </span><span class="kw">tryCatch</span>(<span class="kw">estimate</span>(<span class="kw">AR</span>(<span class="dv">1</span>), x_star, <span class="dt">demean =</span> <span class="ot">FALSE</span>)<span class="op">$</span>mod<span class="op">$</span>coef,</a>
<a class="sourceLine" id="cb37-19" data-line-number="19">  <span class="dt">error =</span> <span class="cf">function</span>(e) <span class="ot">NA</span>)</a>
<a class="sourceLine" id="cb37-20" data-line-number="20">  }</a>
<a class="sourceLine" id="cb37-21" data-line-number="21">  <span class="kw">na.omit</span>(res)              <span class="co"># Deliver results</span></a>
<a class="sourceLine" id="cb37-22" data-line-number="22">}</a></code></pre></div>
<p>Having defined the function above, let us consider a scenario where the model’s assumptions are not respected (i.e. wrong model and/or non-Gaussian time series). Consider an AR(1) and an AR(2) process with the same coefficients for the first lagged variable (i.e. <span class="math inline">\(\phi = \phi_1 = 0.5\)</span>) but with different innovation noise generation procedures:</p>
<p><span class="math display">\[
    \begin{aligned}
  \mathcal{M}_1:&amp;{}&amp; X_t &amp;=0.5 X_{t-1} + W_t,&amp;{}&amp; W_t\sim \mathcal{N} (0,1) \\
  \mathcal{M}_2:&amp;{}&amp; X_t &amp;=0.5 X_{t-1} + 0.25 X_{t-2} + V_t,&amp;{}&amp; V_t\sim t_4
  \end{aligned}
\]</span></p>
<p>where <span class="math inline">\(t_4\)</span> represents the Student-<span class="math inline">\(t\)</span> distribution with 4 degrees of freedom (heavy tails). For the first model (<span class="math inline">\(\mathcal{M}_1\)</span>) we will use the <code>gen_gts()</code> function while for the second model <span class="math inline">\(\mathcal{M}_2\)</span> we will make use the <code>arima.sim()</code> function in which we can assign a vector of innovations from a <span class="math inline">\(t\)</span> distribution.</p>
<div class="sourceCode" id="cb38"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb38-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">1</span>)         <span class="co"># Set seed for reproducibilty</span></a>
<a class="sourceLine" id="cb38-2" data-line-number="2">n =<span class="st"> </span><span class="dv">300</span>             <span class="co"># Sample size</span></a>
<a class="sourceLine" id="cb38-3" data-line-number="3">xt_m1 =<span class="st"> </span><span class="kw">gen_gts</span>(n, <span class="kw">AR1</span>(<span class="dt">phi =</span> <span class="fl">0.5</span>, <span class="dt">sigma2 =</span> <span class="dv">1</span>))   <span class="co"># Gaussian noise only</span></a>
<a class="sourceLine" id="cb38-4" data-line-number="4">xt_m2 =<span class="st"> </span><span class="kw">gts</span>(<span class="kw">arima.sim</span>(<span class="dt">n =</span> n, <span class="kw">list</span>(<span class="dt">ar =</span> <span class="kw">c</span>(<span class="fl">0.5</span>, <span class="fl">0.25</span>), <span class="dt">ma =</span> <span class="dv">0</span>), <span class="co"># Student-t noise</span></a>
<a class="sourceLine" id="cb38-5" data-line-number="5"><span class="dt">rand.gen =</span> <span class="cf">function</span>(n, ...) <span class="kw">rt</span>(n, <span class="dt">df =</span> <span class="dv">4</span>)))</a></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:mbbdatavis"></span>
<img src="ts_files/figure-html/mbbdatavis-1.png" alt="AR(1) processes generated under different noise conditions." width="768" />
<p class="caption">
Figure 4.21: AR(1) processes generated under different noise conditions.
</p>
</div>
<p>From Figure <a href="the-family-of-autoregressive-moving-average-models.html#fig:mbbdatavis">4.21</a>, the two time series appear to be considerably different as a result of the different models and noise processes. Among others, the difference is also related to the variance of the <span class="math inline">\(t\)</span> distribution defined as <span class="math inline">\(\frac{\nu}{\nu-2}\)</span> for <span class="math inline">\(\nu &gt; 2\)</span> (and <span class="math inline">\(\infty\)</span> for <span class="math inline">\(\nu \le 2\)</span>). Therefore, the innovation variance of the i.i.d white noise process is given by <span class="math inline">\(\sigma^2_{\mathcal{M}_1} = 1\)</span> while the noise generated from the <span class="math inline">\(t\)</span> distribution has innovation variance <span class="math inline">\(\sigma^2_{\mathcal{M}_2} = 2\)</span>. To compare the performance of the bootstrap procedures, the function below performs parametric bootstrap for the MLE of an AR(1) process assuming the innovations are Gaussian.</p>
<div class="sourceCode" id="cb39"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb39-1" data-line-number="1">ar1_paraboot =<span class="st"> </span><span class="cf">function</span>(model, <span class="dt">B =</span> <span class="dv">10000</span>) {</a>
<a class="sourceLine" id="cb39-2" data-line-number="2">  est.phi =<span class="st"> </span><span class="kw">rep</span>(<span class="ot">NA</span>,B)    <span class="co"># Define a storage vector</span></a>
<a class="sourceLine" id="cb39-3" data-line-number="3"></a>
<a class="sourceLine" id="cb39-4" data-line-number="4">  <span class="cf">for</span>(i <span class="cf">in</span> <span class="kw">seq_len</span>(B)) { <span class="co"># Perform bootstrap</span></a>
<a class="sourceLine" id="cb39-5" data-line-number="5">    <span class="kw">set.seed</span>(B <span class="op">+</span><span class="st"> </span>i)      <span class="co"># Set seed for reproducibility</span></a>
<a class="sourceLine" id="cb39-6" data-line-number="6">    <span class="co"># Simulate time series underneath the estimated model</span></a>
<a class="sourceLine" id="cb39-7" data-line-number="7">    x.star =<span class="st"> </span><span class="kw">gen_gts</span>(<span class="dv">500</span>, <span class="kw">AR</span>(<span class="dt">phi =</span> model<span class="op">$</span>mod<span class="op">$</span>coef, <span class="dt">sigma2 =</span> model<span class="op">$</span>mod<span class="op">$</span>sigma2))</a>
<a class="sourceLine" id="cb39-8" data-line-number="8"></a>
<a class="sourceLine" id="cb39-9" data-line-number="9">    <span class="co"># Attempt to estimate parameters with recovery</span></a>
<a class="sourceLine" id="cb39-10" data-line-number="10">    est.phi[i] =<span class="st"> </span><span class="kw">tryCatch</span>(<span class="kw">estimate</span>(<span class="kw">AR</span>(<span class="dv">1</span>), x.star, <span class="dt">demean =</span> <span class="ot">FALSE</span>)<span class="op">$</span>mod<span class="op">$</span>coef,</a>
<a class="sourceLine" id="cb39-11" data-line-number="11">    <span class="dt">error =</span> <span class="cf">function</span>(e) <span class="ot">NA</span>)</a>
<a class="sourceLine" id="cb39-12" data-line-number="12">  }</a>
<a class="sourceLine" id="cb39-13" data-line-number="13">  <span class="kw">na.omit</span>(est.phi)       <span class="co"># Return estimated phis</span></a>
<a class="sourceLine" id="cb39-14" data-line-number="14">}</a></code></pre></div>
<p>Now that we have two functions implementing the block and parametric bootstraps, we can apply these procedures to obtain confidence intervals for the (first) <span class="math inline">\(\phi\)</span> parameter for the two time series generated respectively by model <span class="math inline">\(\mathcal{M}_1\)</span> and <span class="math inline">\(\mathcal{M}_2\)</span> (i.e. for <span class="math inline">\(\phi = \phi_1 = 0.5\)</span>).</p>
<div class="sourceCode" id="cb40"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb40-1" data-line-number="1">B =<span class="st"> </span><span class="dv">1000</span></a>
<a class="sourceLine" id="cb40-2" data-line-number="2"></a>
<a class="sourceLine" id="cb40-3" data-line-number="3"><span class="co"># Model 1 </span></a>
<a class="sourceLine" id="cb40-4" data-line-number="4">fit_m1_mle =<span class="st"> </span><span class="kw">estimate</span>(<span class="kw">AR</span>(<span class="dv">1</span>), xt_m1, <span class="dt">demean =</span> <span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb40-5" data-line-number="5">para_m1_phi  =<span class="st"> </span><span class="kw">ar1_paraboot</span>(fit_m1_mle, <span class="dt">B =</span> B)</a>
<a class="sourceLine" id="cb40-6" data-line-number="6">block_m1_phi =<span class="st"> </span><span class="kw">ar1_blockboot</span>(xt_m1, <span class="dt">block_len =</span> <span class="dv">15</span>, <span class="dt">B =</span> B)</a>
<a class="sourceLine" id="cb40-7" data-line-number="7"></a>
<a class="sourceLine" id="cb40-8" data-line-number="8"><span class="co"># Model 2 </span></a>
<a class="sourceLine" id="cb40-9" data-line-number="9">fit_m2_mle =<span class="st"> </span><span class="kw">estimate</span>(<span class="kw">AR</span>(<span class="dv">1</span>), xt_m2, <span class="dt">demean =</span> <span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb40-10" data-line-number="10">para_m2_phi  =<span class="st"> </span><span class="kw">ar1_paraboot</span>(fit_m2_mle, <span class="dt">B =</span> B)</a>
<a class="sourceLine" id="cb40-11" data-line-number="11">block_m2_phi =<span class="st"> </span><span class="kw">ar1_blockboot</span>(xt_m2, <span class="dt">block_len =</span> <span class="dv">15</span>, <span class="dt">B =</span> B)</a></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:blockbootmodels"></span>
<img src="ts_files/figure-html/blockbootmodels-1.png" alt="Estimated parametric and non-parametric blockbootstrap distributions of $\hat{\phi}$ for the MLE parameter estimates. The histogram bars represent the empirical results from the bootstraps with the green representing parametric bootstrap and the red representing the block bootstrap approach. The dashed vertical line represents the true value of $\phi$ and the vertical ticks correspond to the limits of the 95% confidence intervals for both estimation techniques." width="864" />
<p class="caption">
Figure 4.22: Estimated parametric and non-parametric blockbootstrap distributions of <span class="math inline">\(\hat{\phi}\)</span> for the MLE parameter estimates. The histogram bars represent the empirical results from the bootstraps with the green representing parametric bootstrap and the red representing the block bootstrap approach. The dashed vertical line represents the true value of <span class="math inline">\(\phi\)</span> and the vertical ticks correspond to the limits of the 95% confidence intervals for both estimation techniques.
</p>
</div>
<p>The results from the parametric and nonparametric block bootstrapping techniques are displayed in Figure <a href="the-family-of-autoregressive-moving-average-models.html#fig:blockbootmodels">4.22</a>. It can be seen how, for the first model <span class="math inline">\(\mathcal{M}_1\)</span> the parametric boostrap technique defines a confidence interval that includes the true parameter value (0.5) while the non-parametric bootstrap just misses it. In this case, being the model correctly specified for the parametric bootstrap, the latter is the preferred technique. However, it can be seen that for the second model <span class="math inline">\(\mathcal{M}_2\)</span>, the parametric bootstrap approach is greatly affected by the miss-specified model it is based on. Indeed we can see how it shifts towards larger values of <span class="math inline">\(\phi\)</span> (although its confidence intervals still contain the true value) while the MBB remains in the proximity of the true value.</p>
</div>
<div id="model-selection" class="section level2">
<h2><span class="header-section-number">4.5</span> Model Selection</h2>
<p>The diagnostic and inference tools that have been investigated this far are all based on analysing the residuals and checking the parameter confidence intervals to understand if the model is reasonable and fits the data well. However, one may be interested in understanding which, among a set of candidate models, appears to be the “best” to describe and forecast the observed time series. There are different approaches to tackling this problem among which stepwise procedures in which one starts with the simplest (or more complex) model and adds (removes) parameters until a certain statistic (criterion) is optimised. However, these approaches rely on the “paths” chosen to add the parameters (e.g. should one go from an AR(1) to an AR(2) or from an AR(1) to an MA(1)?) while there are other criteria available which are so-called “global” criteria.</p>
<p>The task of selecting an appropriate model is often reffered to as model selection. In fact, this problem is a crucial part of any statistical analysis. Indeed, model selection methods become inevitable in an increasingly large number of applications involving partial theoretical knowledge and vast amounts of information, like in medicine, biology or economics. These techniques are intended to determine which variables are “important” to “explain” a phenomenon under investigation. The terms “important and”explain&quot; can have very different meanings according to the context and, in fact, model selection can be applied to any situation where one tries to balance variability with complexity <span class="citation">(McQuarrie and Tsai <a href="#ref-mcquarrie1998regression">1998</a>)</span>. For example, these techniques can be applied to select “significant” variables in regression problems, to determine the order of an AR process or simply to construct histograms.</p>
<p>Let us consider a general criterion that we would wish a model to respect in order to be considered a “good” model. Such a criterion, that we call <span class="math inline">\(C\)</span>, could be the following:</p>
<span class="math display" id="eq:theocp">\[\begin{equation}
C = \mathbb{E}\left[ {{\mathbb{E}_0}\left[ {\left\| {{X_0} - \hat X} \right\|_2^2} \right]} \right],
 \tag{4.3}
\end{equation}\]</span>
<p>where <span class="math inline">\(\|X\|_2^2 \equiv X^T X\)</span> represents the squared L2-norm, <span class="math inline">\(\hat{X}\)</span> represents the predictions from a given (estimated) model, <span class="math inline">\(X_0\)</span> represents future unobserved values of the time series <span class="math inline">\((X_t)\)</span> and <span class="math inline">\(\mathbb{E}_0\)</span> represents the out-of-sample expectation (i.e. the expectation of my future values <span class="math inline">\(X_0\)</span>). In general terms, this criterion is measuring the expected difference between my predicted values and the expected future values of my time series. Therefore, a “good” model would be a model that minimizes this criterion thereby minimising the expected “prediction error” of the model. In fact, the theoretical quantity defined in <a href="the-family-of-autoregressive-moving-average-models.html#eq:theocp">(4.3)</a> is related to Mallow’s <span class="math inline">\(C_p\)</span> <span class="citation">(see Mallows <a href="#ref-mallows1973some">1973</a>)</span>, which is still nowadays one of the most commonly used model selection criterion for regression. Indeed, let us consider the following theorem whose proof is given in Appendix <a href="proofs.html#appendixoptim">A.3</a>.</p>

<div class="theorem">
<span id="thm:optimthm" class="theorem"><strong>Theorem 4.2  </strong></span><span class="math display">\[C = \mathbb{E}\left[ {{\mathbb{E}_0}\left[ {\left\| {{X_0} - \hat X} \right\|_2^2} \right]} \right] = \mathbb{E}\left[ {\left\| {X - \hat X} \right\|_2^2} \right] + 2tr\left( {\text{cov}\left( {X,\hat X} \right)} \right).\]</span>
</div>

<p>Then, in the context of linear regression we have</p>
<span class="math display">\[\begin{equation*}
\mathbf{Y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\varepsilon},
\end{equation*}\]</span>
<p>and when considering the least-squares estimator we obtain</p>
<span class="math display">\[\begin{equation*}
\hat{\mathbf{Y}} = \mathbf{X} \hat{\boldsymbol{\beta}} = \mathbf{X} \left(\mathbf{X}^T \mathbf{X}\right)^{-1} \mathbf{X} \mathbf{Y} = \mathbf{H} \mathbf{Y},
\end{equation*}\]</span>
<p>where <span class="math inline">\(\mathbf{H} \equiv \mathbf{X} \left(\mathbf{X}^T \mathbf{X}\right)^{-1} \mathbf{X}\)</span> denotes the “Hat” matrix (as it puts a 🎩 on <span class="math inline">\(\mathbf{Y}\)</span>). Then, the theoretical <span class="math inline">\(C\)</span> becomes:</p>
<span class="math display">\[\begin{equation*}
C = \mathbb{E}\left[ \mathbb{E}_0 \left[ \left\| \mathbf{Y}_0 - \hat{\mathbf{Y}} \right\|_2^2 \right] \right],
\end{equation*}\]</span>
<p>and using Theorem <a href="the-family-of-autoregressive-moving-average-models.html#thm:optimthm">4.2</a>, we obtain (why? 🤔)</p>
<span class="math display">\[\begin{align*}
C &amp;= \mathbb{E}\left[  \left\| \mathbf{Y} - \hat{\mathbf{Y}} \right\|_2^2 \right] + 2tr \left( \text{cov} \left( \mathbf{Y}, \hat{\mathbf{Y}} \right) \right) = \mathbb{E}\left[  \left\| \mathbf{Y} - \hat{\mathbf{Y}} \right\|_2^2 \right] + 2k \sigma^2,
\end{align*}\]</span>
<p>where <span class="math inline">\(k\)</span> is the number of colunms of the matrix <span class="math inline">\(\mathbf{X}\)</span> and <span class="math inline">\(\sigma^2\)</span> the variance of the residuals <span class="math inline">\(\boldsymbol{\varepsilon}\)</span>. Thus, an <strong>unbiased</strong> estimator of <span class="math inline">\(C\)</span> is simply</p>
<span class="math display">\[\begin{align*}
\hat{C} &amp;=  \left\| \mathbf{Y} - \hat{\mathbf{Y}} \right\|_2^2 + 2k \sigma^2,
\end{align*}\]</span>
<p>which is known as Mallow’s <span class="math inline">\(C_p\)</span>.</p>
<p>Alternatively, several researchers in the 1970’s used a different approach based on the Kullback-Leibler disprepancy, which is often called the <em>Information Theory</em> approach. While a theoretical discussion of this method is clearly beyond the scope of this book, the general idea is essentially the same as Mallow’s <span class="math inline">\(C_p\)</span>, where one would construct a theoretical quantity (such as <span class="math inline">\(C\)</span> in our previous example) and compute/derive a suitable estimator of this quantity. The most famous information theory-based model selection crierion is the Akaike Information Criterion or AIC <span class="citation">(see Akaike <a href="#ref-akaike1974new">1974</a>)</span>, which is probably the most commonly used model selection crierion for time series data. In the late 1970’s there was an explosion of work in this field and many model selection criteria were proposed including the Bayesian Information Criterion or BIC proposed by <span class="citation">Schwarz and others (<a href="#ref-schwarz1978estimating">1978</a>)</span> and the HQ criterion proposed by <span class="citation">Hannan and Quinn (<a href="#ref-hannan1979determination">1979</a>)</span>.</p>
<p>All these criteria in some way represent (under different forms) the general criterion <span class="math inline">\(C\)</span> defined earlier. Assuming that <span class="math inline">\(C\)</span> is a good criterion to evaluate the “goodness” of a model, most of the quantites defining this criterion are not directly available and estimators are needed. Denoting <span class="math inline">\(\text{logL}(\hat{\boldsymbol{\theta}})\)</span> as being the log-likelihood function evaluated at the estimated parameters and <span class="math inline">\(k\)</span> as the number of parameters in the model (i.e. the length of the vector <span class="math inline">\(\boldsymbol{\theta}\)</span>), the three criteria higlighted in the previous paragraph aim at estimating this quantity as follows:</p>
<span class="math display">\[\begin{align*}
  \text{AIC} &amp;=  -2 \text{logL}(\hat{\boldsymbol{\theta}}) + 2k\\
  \text{BIC} &amp;=  -2 \text{logL}(\hat{\boldsymbol{\theta}}) + \log(T) k\\
  \text{HQ}  &amp;=  -2 \text{logL}(\hat{\boldsymbol{\theta}}) + 2\log(\log(T)) k .
\end{align*}\]</span>
<p>As you can notice, all three criteria have a common structure to them composed of two terms: (i) the first is a quantity that is based the value of the likelihood function while (ii) the second is a value that depends on the number of parameters in the model <span class="math inline">\(k\)</span> (as well as on the length of the time series <span class="math inline">\(T\)</span>). To interpret these terms, one can consider that the first term decreases as the likelihood increases which is true, among others, when more complex models are fitted to the observed time series. This implies that, all other things being equal, the criterion is minimized when the value of the likelihood is the largest possible which is (always) true when the most complex model is fitted to the time series. If we were to base our choice only on this term, we would always choose the most complex model for our time series thereby leading to the problem of “overfitting” which can introduce unnecessary variability if using this model to predict future values of the time series. In order to compensate for this, the second term can be seen as a penalty-term for model complexity. Indeed, the second term increases as more parameters are added to the model.</p>
<p>From a theoretical standpoint it is difficult to determine whether the AIC, BIC or HQ (or many other criterion) is better. Essentially, statisticians have considered different ways of assessing the field of model selection. The first one is known as <em>model selection efficiency</em> and is largely due to <span class="citation">Shibata (<a href="#ref-shibata1980asymptotically">1980</a>)</span>. This paradigm relies on the common assumption (in both time series and regression) that the data generating process (or true model) is of <strong>infinite</strong> dimension and/or that the sets of candidate models (i.e. the set of models we are considering) do not include the true model. Under this setting, the goal of model selection is to find the candidate that is the ``closest’’ to the true model. <span class="citation">Shibata (<a href="#ref-shibata1980asymptotically">1980</a>)</span> defined model selection to be <em>asymptotically efficienct</em> if this method chooses the candidate model with the smallest prediction error (in some sense) as the sample size goes to infinity. Under some typical conditions, the AIC as well as Mallow’s <span class="math inline">\(C_p\)</span> are both asymptotically efficient but the BIC and HQ are not. The second approach to model selection relies on <em>consistency</em> and is based on the idea that the true model is of <strong>finite</strong> dimension and that it is included in the set of candidate models. Under this assumption the goal of model selection is to correctly select this true model among the candidates models. Moreover, a model selection criterion is said to be <em>model selection consistent</em> if it identifies the correct model with probability equal to one as the sample size goes to infinity. The BIC and HQ are both <em>model selection consistent</em> but this is not the case for the AIC and <span class="math inline">\(C_p\)</span>. The assumption that the true model is among the candidate models is often considered rather strong by statisticans. In practice, the efficiency approach tends to be more popular but there is little agreement and the choice between these two philosophies remains highly subjective. To make matters more confusing, both consistency and efficiency are asymptotic properties which are not “observable” in small samples. In this section, we will consider various simulations to better understand the small sample properties of AIC, BIC and HQ.</p>
<p>With this in mind, let us consider the use of these criteria in the selection of AR(<span class="math inline">\(p\)</span>) models seen this far. As we saw in the previous sections, the PACF is a very useful tool to identify the order of the AR(<span class="math inline">\(p\)</span>) model that is underlying the observed time series. However, the choice based on the PACF can possibly lead to selecting models that may adequately describe the observed time series but do not necessarily perform well in terms of predictions. For this reason, consider distinct AR(<span class="math inline">\(p\)</span>) models with different values of <span class="math inline">\(p\)</span>:</p>
<p><span class="math display">\[\mathcal{M}_j: {X_t} = \sum\limits_{i = 1}^j {{\phi _i}{X_{t - i}}}  + {W_t},\]</span></p>
<p>where <span class="math inline">\(\mathcal{M}_j\)</span> is used to denote an AR(<span class="math inline">\(j\)</span>) model, with <span class="math inline">\(j = 1,...,p^*\)</span> where <span class="math inline">\(p^*\)</span> represents the maximum order of the AR(<span class="math inline">\(p\)</span>) model considered for a given observed time series (e.g. <span class="math inline">\(p^*\)</span> could be the maximum lag at which the PACF appears to be significant). Let us illustrate the properties of these model selection criteria through the following example of an AR(3) model for which we show the PACF of a possible realization from this process (<span class="math inline">\(T = 500\)</span>).</p>
<div class="sourceCode" id="cb41"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb41-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb41-2" data-line-number="2">true_model =<span class="st"> </span><span class="kw">AR</span>(<span class="dt">phi =</span> <span class="kw">c</span>(<span class="fl">1.2</span>, <span class="fl">-0.9</span>, <span class="fl">0.3</span>), <span class="dt">sigma2 =</span> <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb41-3" data-line-number="3">Xt =<span class="st"> </span><span class="kw">gen_gts</span>(<span class="dt">n =</span> <span class="dv">500</span>, <span class="dt">model =</span> true_model)</a>
<a class="sourceLine" id="cb41-4" data-line-number="4"><span class="kw">plot</span>(<span class="kw">auto_corr</span>(Xt, <span class="dt">pacf =</span> <span class="ot">TRUE</span>))</a></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-25"></span>
<img src="ts_files/figure-html/unnamed-chunk-25-1.png" alt="Estimated PACF of a time series from an AR(3) model with parameters $\phi_1 = 1.2$, $\phi_2 = -0.9$ and $\phi_3 = 0.3$." width="576" />
<p class="caption">
Figure 4.23: Estimated PACF of a time series from an AR(3) model with parameters <span class="math inline">\(\phi_1 = 1.2\)</span>, <span class="math inline">\(\phi_2 = -0.9\)</span> and <span class="math inline">\(\phi_3 = 0.3\)</span>.
</p>
</div>
<p>From the PACF plot one sees that the first three lags appear to be significant indicating that an AR(3) model would indeed appear to be a good candidate model for this observed time series. However, there is another value of the PACF that appears to be marginally significant at lag 10. Let us therefore suppose that we want to consider all possible AR(p) models with <span class="math inline">\(p^* = 10\)</span>. The <code>select()</code> function from the <code>simts</code> package allows to evaluate and plot the value of the three criteria presented earlier in order to understand the value of <span class="math inline">\(p\)</span> that minimizes each of them.</p>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb42-1" data-line-number="1">best_model =<span class="st"> </span><span class="kw">select</span>(<span class="kw">AR</span>(<span class="dv">10</span>), Xt, <span class="dt">include.mean =</span> <span class="ot">FALSE</span>) </a></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-27"></span>
<img src="ts_files/figure-html/unnamed-chunk-27-1.png" alt="Plot of the AIC, BIC and HQ model selection criteria from all models included in an AR(10) model. The observed time series was simulated from an AR(3) model." width="672" />
<p class="caption">
Figure 4.24: Plot of the AIC, BIC and HQ model selection criteria from all models included in an AR(10) model. The observed time series was simulated from an AR(3) model.
</p>
</div>
<p>The output of this funtion is a plot of the values of each criterion over all possible AR(<span class="math inline">\(p\)</span>) models with <span class="math inline">\(p = 1,...,10\)</span> and, as can be seen from the plot, all criteria are minimized at the value <span class="math inline">\(p=3\)</span> which shows how all criteria (in this particular case) agree and select the true underlying model. The function also delivers the MLE estimates of the parameters of the selected model:</p>
<div class="sourceCode" id="cb43"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb43-1" data-line-number="1">best_model<span class="op">$</span>coef</a></code></pre></div>
<pre><code>##        ar1        ar2        ar3 
##  1.1726319 -0.8830923  0.2752257</code></pre>
<p>It can be seen how these estimated parameters are close to the true parameter values of the AR(3) model which was used to simulate the observed time series. Therefore the model selection criteria presented above appear to be good criteria to select the true underlying model, but they all do so in different manners. To further investigate the properties of these criteria let us consider the following simulation study.</p>

<div class="example">
<p><span id="exm:unnamed-chunk-29" class="example"><strong>Example 4.12  (Simulation Study: Strong Signal vs Weak Signal)  </strong></span>In this simulation study, we consider the following three models:
<span class="math display">\[\begin{align*}
  X_t &amp;= 0.9X_{t-1} + W_t,\\
  X_t &amp;= 0.6X_{t-1} + 0.3X_{t-2} + W_t,\\
  X_t &amp;= 0.5X_{t-1} + 0.25X_{t-2} + 0.125X_{t-3} + 0.0625X_{t-4} + W_t,\\
  X_t &amp;= 0.9 W_{t-1} + W_t,
\end{align*}\]</span></p>
<p>where <span class="math inline">\(W_t \overset{iid}{\sim} \mathcal{N}(0,1).\)</span></p>
<p>The first two time series correspond to (relatively) “strong” signals (generally meaning that the values of the parameters are above 0.5) while the third is called a “weak” signal (since the parameters are mostly quite close to 0). Finally, the fourth time series is an example of an AR process of infinte dimension which corresponds to an MA(1) (which is discussed and justified further on in this chapter).</p>
<p>Based on the theoretical (i.e. asymptotic) properties of the considered model selection criteria, we expect the BIC to perform very well for the first two models (strong signals where the true model is among the candidate models), while the AIC should give “good” results on the last two models (weak signals with models of “larger” dimensions).</p>
For the purpose of the simulation we consider a sample size <span class="math inline">\(T = 100\)</span> and <span class="math inline">\(B = 1000\)</span> bootstrap replications. The code used for the simulation study (being lengthy) is omitted for practical exposition purposes.
</div>

<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-30"></span>
<img src="ts_files/figure-html/unnamed-chunk-30-1.png" alt="Barplots representing the frequency with which each considered model selection criterion (AIC, BIC, HQ) selects each of the possible candidate models within an AR(9) in each of the four simulation settings described in the text." width="960" />
<p class="caption">
Figure 4.25: Barplots representing the frequency with which each considered model selection criterion (AIC, BIC, HQ) selects each of the possible candidate models within an AR(9) in each of the four simulation settings described in the text.
</p>
</div>
<p>As anticipated, it can be seen how the BIC (green bars) often selects a small dimensional model for the first two settings, which corresponds to the truth. In these cases the AIC (red bars) performs less well compared to the other two criteria where the HQ (blue bars) appears to do better than the others for the “strong” signal AR(2). However, as the signal becomes weaker and the true model size increases (last two settings) the AIC does a better job than the BIC and the HQ since it selects higher dimensional models (which corresponds to the truth) with a higher frequency with respect to the HQ and even more so with respect to the BIC. Overall, it would appear that if a “strong” signal is present and the model dimension is low (finite), the BIC is the best choice. However, when the signal is “weaker” and the model dimension is larger (infinite), the AIC tends to select “better” models. Finally, the HQ criterion appears to be inbetween the AIC and BIC in terms of performance for the considered simulation setting.</p>
<p>Having studied th performance of these three criteria through the described simultation study, let us now consider an example we considered when explaining the usefulness of the PACF to determine the order of an AR(p) model. For this reason, let us again check the PACF of the Lynx trapping data set represented below.</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb45-1" data-line-number="1">cor_lynx =<span class="st"> </span><span class="kw">corr_analysis</span>(lynx_gts)</a></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-31"></span>
<img src="ts_files/figure-html/unnamed-chunk-31-1.png" alt="Empirical PACF of the yearly lynx trapping time series." width="864" />
<p class="caption">
Figure 4.26: Empirical PACF of the yearly lynx trapping time series.
</p>
</div>
<p>As we saw earlier, the PACF appears to be significant at lag <span class="math inline">\(h = 11\)</span> therefore leading to the conclusion that an AR(11) model would be good to describe/forecast the observed time series. However, let us check if we reach the same conclusion using the three model selection criteria described above. For this reason, let us use an AR(16) as the maximum order for the candidate AR(p) models.</p>
<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb46-1" data-line-number="1">lynx_select =<span class="st"> </span><span class="kw">select</span>(<span class="kw">AR</span>(<span class="dv">16</span>), lynx_gts, <span class="dt">include.mean =</span> <span class="ot">TRUE</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-33"></span>
<img src="ts_files/figure-html/unnamed-chunk-33-1.png" alt="Values of the three model selection criteria described in the text for all candidate models included in an AR(16) model for the yearly lynx trapping time series." width="672" />
<p class="caption">
Figure 4.27: Values of the three model selection criteria described in the text for all candidate models included in an AR(16) model for the yearly lynx trapping time series.
</p>
</div>
<p>In this case, we see that the AIC and HQ select an AR(11) model in accordance with the selection procedure based on the PACF. However the BIC selects a much smaller model (AR(3)). This is in some way consistent with the simulation study seen earlier where the BIC generally tends to select lower-dimensional models. Moreover, the true model for the lynx data may not be among the candidate models and therefore the BIC would not be a good choice. Hence, for this case the AIC or HQ are probably better choices.</p>
<p>To summarize, all these techniques aim at estimating the out-of-sample prediction error of a given model. However, they strongly rely on the chosen likelihood function and, if this does not adequately represent the model that generated the observed time series, then they could deliver misleading conclusions. For this reason, one could consider another approach to model-selection based on model forecasting accuracy. Indeed, one could select a model based on the following procedure:</p>
<ol style="list-style-type: decimal">
<li>Split the observed time series of length <span class="math inline">\(T\)</span> into two sub-series. The first one (i.e. training set) goes from 1 to <span class="math inline">\(n\)</span> (where <span class="math inline">\(n\)</span> is for example <span class="math inline">\(0.8T\)</span>) and the second one (i.e. testing set) goes from <span class="math inline">\(n + 1\)</span> to <span class="math inline">\(T.\)</span></li>
<li>Estimate the model you wish to evaluate on the training set and forecast the next observation (i.e. <span class="math inline">\(X_{n+1}^n\)</span>). Compute the difference between your forecast and the actual value <span class="math inline">\(X_{n+1}\)</span>.</li>
<li>Add the observation <span class="math inline">\(X_{n+1}\)</span> to the training set and let <span class="math inline">\(n = n + 1\)</span>. Go to Step 2 until <span class="math inline">\(n = T\)</span>.</li>
<li>Compute a suitable “score” to asses the quality of your model based on the
empirical “prediction errors” vector.</li>
</ol>
<p>There are many “scores” that can assess the quality of the model using the empirical prediction errors such as the Root Mean Squared Error (RMSE), mean absolute percentage error, etc. A generally used and common score is the Median Absolute Prediction Error which is defined as follows:</p>
<p><span class="math display">\[\text{MAPE} = \text{median}\left(\Big|X_{t+j} - X_{t+j}^t\Big|\right).\]</span></p>
<p>The MAPE is a score function that assesses how far the model’s predictions are from the actual values while ensuring that a possible few outlying values can affect that evaluation of a model’s performance (the median is what guarantees this). This score can also be used to assess a model’s prediction performance and is implemented in the <code>MAPE()</code> function in the <code>simts</code> package (by default this function uses the first <span class="math inline">\(0.8T\)</span> observations as training data). Let us study a few simulated examples of how the MAPE can be a “good” model selection (and evaluation) criterion. In the first example we simulate a time series from an AR(3) model and compute the MAPE on all possible candidate models within an AR(8) model.</p>
<div class="sourceCode" id="cb47"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb47-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">5</span>)</a>
<a class="sourceLine" id="cb47-2" data-line-number="2">Xt =<span class="st"> </span><span class="kw">gen_gts</span>(<span class="dv">500</span>, <span class="kw">AR</span>(<span class="dt">phi =</span> <span class="kw">c</span>(<span class="fl">0.1</span>, <span class="dv">0</span>, <span class="fl">0.8</span>), <span class="dt">sigma2 =</span> <span class="dv">1</span>))</a>
<a class="sourceLine" id="cb47-3" data-line-number="3"><span class="kw">MAPE</span>(<span class="kw">AR</span>(<span class="dv">8</span>), Xt)</a></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-34"></span>
<img src="ts_files/figure-html/unnamed-chunk-34-1.png" alt="Estimated MAPE for the candidate models included in an AR(8) model. The true underlying model is an AR(3)." width="672" />
<p class="caption">
Figure 4.28: Estimated MAPE for the candidate models included in an AR(8) model. The true underlying model is an AR(3).
</p>
</div>
<p>From the plot we can see the value of the MAPE for all models up to the AR(8) model. Aside from the MAPE itself we can also see the confidence intervals for each value of the MAPE (representing the distance of one standard deviation from the estimated MAPE). In this example we see that the MAPE is minimized for the AR(6) (red dot) although there are other values of the MAPE that lie below the upper confidence interval of the smallest MAPE. In this case, the MAPE of the smallest model whose value lies below this bound is indeed the one corresponding to the AR(3) model and, in the optic of choosing simple models, our choice would probably go on the latter model. Another example is simply another time series from the same model as above.</p>
<div class="sourceCode" id="cb48"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb48-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">7</span>)</a>
<a class="sourceLine" id="cb48-2" data-line-number="2">Xt =<span class="st"> </span><span class="kw">gen_gts</span>(<span class="dv">500</span>, <span class="kw">AR</span>(<span class="dt">phi =</span> <span class="kw">c</span>(<span class="fl">0.1</span>, <span class="dv">0</span>, <span class="fl">0.8</span>), <span class="dt">sigma2 =</span> <span class="dv">1</span>))</a>
<a class="sourceLine" id="cb48-3" data-line-number="3"><span class="kw">MAPE</span>(<span class="kw">AR</span>(<span class="dv">8</span>), Xt)</a></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-35"></span>
<img src="ts_files/figure-html/unnamed-chunk-35-1.png" alt="Estimated MAPE for the candidate models included in an AR(8) model. The true underlying model is an AR(3)." width="672" />
<p class="caption">
Figure 4.29: Estimated MAPE for the candidate models included in an AR(8) model. The true underlying model is an AR(3).
</p>
</div>
<p>In this case we see that the smallest value of the MAPE corresponds to the AR(3) which is also the smallest model below the upper confidence interval of the smallest value of the MAPE (hence the green and red dots correspond, leaving only the red dot). Having studied the MAPE in a couple of examples, let us check the model selected for the lynx trappings data seen earlier when using this approach using the first <span class="math inline">\(0.8 \cdot T\)</span> observations to estimate the model and the last <span class="math inline">\(0.2 \cdot T\)</span> to test the estimated model.</p>
<div class="sourceCode" id="cb49"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb49-1" data-line-number="1">lynx_mape =<span class="st"> </span><span class="kw">MAPE</span>(<span class="kw">AR</span>(<span class="dv">16</span>), lynx_gts)</a></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-36"></span>
<img src="ts_files/figure-html/unnamed-chunk-36-1.png" alt="Estimated MAPE for the lynx dataset." width="672" />
<p class="caption">
Figure 4.30: Estimated MAPE for the lynx dataset.
</p>
</div>
<p>As we can see, an AR(11) model is selected which is equivalent to the model selected by the AIC and HQ criteria. Moveover, it can observed that MAPE is quite variable and that several models have a MAPE value close to the one of the selected model. As the MAPE is prone to overfitting, a reasonable approach is to select a model based on the “one standard deviation rule” which in this case would correspond to an AR(3), the same as the model selected by the BIC. Therefore also this selection method would appear to indicate that either an AR(11) or an AR(3) appear to be “good” models for this time series.</p>
<!--
As we can see, an AR(12) model is selected indicating that a more complicated model may be required for this time series data. However it can be seen how the value of the MAPE is extremely close to that of the previously selected AR(8) and lies abundantly within the confidence intervals of the MAPE for the AR(12). Therefore, in addition to the PACF and the model selection criteriea presented earlier, also this selection method would appear to indicate that the AR(8) model is a good candidate for this time series data. 
-->

</div>
</div>
<h3><span class="header-section-number">C</span> Proofs</h3>
<div id="refs" class="references">
<div id="ref-mcquarrie1998regression">
<p>McQuarrie, Allan DR, and Chih-Ling Tsai. 1998. <em>Regression and Time Series Model Selection</em>. World Scientific.</p>
</div>
<div id="ref-mallows1973some">
<p>Mallows, Colin L. 1973. “Some Comments on c P.” <em>Technometrics</em> 15 (4). Taylor &amp; Francis Group:661–75.</p>
</div>
<div id="ref-akaike1974new">
<p>Akaike, Hirotugu. 1974. “A New Look at the Statistical Model Identification.” <em>IEEE Transactions on Automatic Control</em> 19 (6). Ieee:716–23.</p>
</div>
<div id="ref-schwarz1978estimating">
<p>Schwarz, Gideon, and others. 1978. “Estimating the Dimension of a Model.” <em>The Annals of Statistics</em> 6 (2). Institute of Mathematical Statistics:461–64.</p>
</div>
<div id="ref-hannan1979determination">
<p>Hannan, Edward J, and Barry G Quinn. 1979. “The Determination of the Order of an Autoregression.” <em>Journal of the Royal Statistical Society. Series B (Methodological)</em>. JSTOR, 190–95.</p>
</div>
<div id="ref-shibata1980asymptotically">
<p>Shibata, Ritei. 1980. “Asymptotically Efficient Selection of the Order of the Model for Estimating Parameters of a Linear Process.” <em>The Annals of Statistics</em>. JSTOR, 147–64.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="fundtimeseries.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="sdzlj.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/SMAC-Group/ts/edit/master/03-arma.Rmd",
"text": "Edit"
},
"download": ["ts.pdf", "ts.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
