<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Applied Time Series Analysis with R</title>
  <meta name="description" content="Applied Time Series Analysis with R">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Applied Time Series Analysis with R" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="SMAC-Group/ts" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Applied Time Series Analysis with R" />
  
  
  

<meta name="author" content="Stéphane Guerrier, Roberto Molinari and Haotian Xu">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  <link rel="shortcut icon" href="favicon.ico" type="image/x-icon">
<link rel="prev" href="fundtimeseries.html">
<link rel="next" href="references.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Applied Time Series Analysis with R</a></li>

<li class="divider"></li>
<li class="part"><span><b>I Foundation</b></span></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#conventions"><i class="fa fa-check"></i><b>1.1</b> Conventions</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#bibliographic-note"><i class="fa fa-check"></i><b>1.2</b> Bibliographic Note</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i><b>1.3</b> Acknowledgements</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#license"><i class="fa fa-check"></i><b>1.4</b> License</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="basic-elements-of-time-series.html"><a href="basic-elements-of-time-series.html"><i class="fa fa-check"></i><b>2</b> Basic Elements of Time Series</a><ul>
<li class="chapter" data-level="2.1" data-path="basic-elements-of-time-series.html"><a href="basic-elements-of-time-series.html#the-wold-decomposition"><i class="fa fa-check"></i><b>2.1</b> The Wold Decomposition</a><ul>
<li class="chapter" data-level="2.1.1" data-path="basic-elements-of-time-series.html"><a href="basic-elements-of-time-series.html#the-deterministic-component-signal"><i class="fa fa-check"></i><b>2.1.1</b> The Deterministic Component (Signal)</a></li>
<li class="chapter" data-level="2.1.2" data-path="basic-elements-of-time-series.html"><a href="basic-elements-of-time-series.html#the-random-component-noise"><i class="fa fa-check"></i><b>2.1.2</b> The Random Component (Noise)</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="basic-elements-of-time-series.html"><a href="basic-elements-of-time-series.html#eda"><i class="fa fa-check"></i><b>2.2</b> Exploratory Data Analysis for Time Series</a></li>
<li class="chapter" data-level="2.3" data-path="basic-elements-of-time-series.html"><a href="basic-elements-of-time-series.html#dependence-in-time-series"><i class="fa fa-check"></i><b>2.3</b> Dependence in Time Series</a></li>
<li class="chapter" data-level="2.4" data-path="basic-elements-of-time-series.html"><a href="basic-elements-of-time-series.html#basicmodels"><i class="fa fa-check"></i><b>2.4</b> Basic Time Series Models</a><ul>
<li class="chapter" data-level="2.4.1" data-path="basic-elements-of-time-series.html"><a href="basic-elements-of-time-series.html#wn"><i class="fa fa-check"></i><b>2.4.1</b> White Noise</a></li>
<li class="chapter" data-level="2.4.2" data-path="basic-elements-of-time-series.html"><a href="basic-elements-of-time-series.html#rw"><i class="fa fa-check"></i><b>2.4.2</b> Random Walk</a></li>
<li class="chapter" data-level="2.4.3" data-path="basic-elements-of-time-series.html"><a href="basic-elements-of-time-series.html#ar1"><i class="fa fa-check"></i><b>2.4.3</b> First-Order Autoregressive Model</a></li>
<li class="chapter" data-level="2.4.4" data-path="basic-elements-of-time-series.html"><a href="basic-elements-of-time-series.html#ma1"><i class="fa fa-check"></i><b>2.4.4</b> Moving Average Process of Order 1</a></li>
<li class="chapter" data-level="2.4.5" data-path="basic-elements-of-time-series.html"><a href="basic-elements-of-time-series.html#drift"><i class="fa fa-check"></i><b>2.4.5</b> Linear Drift</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="basic-elements-of-time-series.html"><a href="basic-elements-of-time-series.html#lts"><i class="fa fa-check"></i><b>2.5</b> Composite Stochastic Processes</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="fundtimeseries.html"><a href="fundtimeseries.html"><i class="fa fa-check"></i><b>3</b> Fundamental Properties of Time Series</a><ul>
<li class="chapter" data-level="3.1" data-path="fundtimeseries.html"><a href="fundtimeseries.html#the-autocorrelation-and-autocovariance-functions"><i class="fa fa-check"></i><b>3.1</b> The Autocorrelation and Autocovariance Functions</a><ul>
<li class="chapter" data-level="3.1.1" data-path="fundtimeseries.html"><a href="fundtimeseries.html#a-fundamental-representation"><i class="fa fa-check"></i><b>3.1.1</b> A Fundamental Representation</a></li>
<li class="chapter" data-level="3.1.2" data-path="fundtimeseries.html"><a href="fundtimeseries.html#admissible-autocorrelation-functions"><i class="fa fa-check"></i><b>3.1.2</b> Admissible Autocorrelation Functions 😱</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="fundtimeseries.html"><a href="fundtimeseries.html#stationary"><i class="fa fa-check"></i><b>3.2</b> Stationarity</a><ul>
<li class="chapter" data-level="3.2.1" data-path="fundtimeseries.html"><a href="fundtimeseries.html#assessing-weak-stationarity-of-time-series-models"><i class="fa fa-check"></i><b>3.2.1</b> Assessing Weak Stationarity of Time Series Models</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="fundtimeseries.html"><a href="fundtimeseries.html#estimation-of-moments-stationary-processes"><i class="fa fa-check"></i><b>3.3</b> Estimation of Moments (Stationary Processes)</a><ul>
<li class="chapter" data-level="3.3.1" data-path="fundtimeseries.html"><a href="fundtimeseries.html#estimation-of-the-mean-function"><i class="fa fa-check"></i><b>3.3.1</b> Estimation of the Mean Function</a></li>
<li class="chapter" data-level="3.3.2" data-path="fundtimeseries.html"><a href="fundtimeseries.html#sample-autocovariance-and-autocorrelation-functions"><i class="fa fa-check"></i><b>3.3.2</b> Sample Autocovariance and Autocorrelation Functions</a></li>
<li class="chapter" data-level="3.3.3" data-path="fundtimeseries.html"><a href="fundtimeseries.html#robustness-issues"><i class="fa fa-check"></i><b>3.3.3</b> Robustness Issues</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="the-family-of-autoregressive-moving-average-models.html"><a href="the-family-of-autoregressive-moving-average-models.html"><i class="fa fa-check"></i><b>4</b> The Family of Autoregressive Moving Average Models</a><ul>
<li class="chapter" data-level="4.1" data-path="the-family-of-autoregressive-moving-average-models.html"><a href="the-family-of-autoregressive-moving-average-models.html#linear-processes"><i class="fa fa-check"></i><b>4.1</b> Linear Processes</a></li>
<li class="chapter" data-level="4.2" data-path="the-family-of-autoregressive-moving-average-models.html"><a href="the-family-of-autoregressive-moving-average-models.html#autoregressive-models---arp"><i class="fa fa-check"></i><b>4.2</b> Autoregressive Models - AR(p)</a><ul>
<li class="chapter" data-level="4.2.1" data-path="the-family-of-autoregressive-moving-average-models.html"><a href="the-family-of-autoregressive-moving-average-models.html#properties-of-arp-models"><i class="fa fa-check"></i><b>4.2.1</b> Properties of AR(p) models</a></li>
<li class="chapter" data-level="4.2.2" data-path="the-family-of-autoregressive-moving-average-models.html"><a href="the-family-of-autoregressive-moving-average-models.html#estimation-of-arp-models"><i class="fa fa-check"></i><b>4.2.2</b> Estimation of AR(p) models</a></li>
<li class="chapter" data-level="4.2.3" data-path="the-family-of-autoregressive-moving-average-models.html"><a href="the-family-of-autoregressive-moving-average-models.html#forecasting-arp-models"><i class="fa fa-check"></i><b>4.2.3</b> Forecasting AR(p) Models</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="appendixa.html"><a href="appendixa.html"><i class="fa fa-check"></i><b>A</b> Proofs</a><ul>
<li class="chapter" data-level="A.1" data-path="appendixa.html"><a href="appendixa.html#proof-of-theorem-1"><i class="fa fa-check"></i><b>A.1</b> Proof of Theorem 1 😱</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="appendixb.html"><a href="appendixb.html"><i class="fa fa-check"></i><b>B</b> Robust Regression Methods</a><ul>
<li class="chapter" data-level="B.1" data-path="appendixb.html"><a href="appendixb.html#the-classical-least-squares-estimator"><i class="fa fa-check"></i><b>B.1</b> The Classical Least-Squares Estimator</a></li>
<li class="chapter" data-level="B.2" data-path="appendixb.html"><a href="appendixb.html#robust-estimators-for-linear-regression-models"><i class="fa fa-check"></i><b>B.2</b> Robust Estimators for Linear Regression Models</a></li>
<li class="chapter" data-level="B.3" data-path="appendixb.html"><a href="appendixb.html#applications-of-robust-estimation"><i class="fa fa-check"></i><b>B.3</b> Applications of Robust Estimation</a></li>
</ul></li>
<li class="chapter" data-level="C" data-path="appendixc.html"><a href="appendixc.html"><i class="fa fa-check"></i><b>C</b> Proofs</a><ul>
<li class="chapter" data-level="C.1" data-path="appendixc.html"><a href="appendixc.html#proof-of-theorem-refcondexp"><i class="fa fa-check"></i><b>C.1</b> Proof of Theorem @ref(condexp) 😱</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/SMAC-Group/ts" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Applied Time Series Analysis with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="the-family-of-autoregressive-moving-average-models" class="section level1">
<h1><span class="header-section-number">Chapter 4</span> The Family of Autoregressive Moving Average Models</h1>
<p>In this chapter we introduce a class of time series models that is considerably flexible and among the most commonly used to describe stationary time series. This class is represented by the Seasonal AutoRegressive Integrated Moving Average (SARIMA) models which, among others, combine and include the autoregressive and moving average models seen in the previous chapter. To introduce this class of models, we start by describing a sub-class called AutoRegressive Moving Average (ARMA) models which represent the backbone on which the SARIMA class is built. The importance of ARMA models resides in their flexibility as well as their capacity of describing (or closely approximating) almost all the features of a stationary time series. The autoregressive parts of these models describe how consecutive observations in time influence each other while the moving average parts capture some possible unobserved shocks thereby allowing to model different phenomena which can be observed in various fields going from biology to finance.</p>
<p>With this premise, the first part of this chapter introduces and explains the class of ARMA models in the following manner. First of all we will discuss the class of linear processes, which ARMA models belong to, and we will then proceed to a detailed description of autoregressive models in which we review their definition, explain their properties, introduce the main estimation methods for their parameters and highlight the diagnostic tools which can help understand if the estimated models appear to be appropriate or sufficient to well describe the observed time series. Once this is done, we will then use most of the results given for the autoregressive models to further describe and discuss moving average models, for which we underline the property of invertibility, and finally the ARMA models. Indeed, the properties and estimation methods for the latter class are directly inherited from the discussions on the autoregressive and moving average models.</p>
<p>The second part of this chapter introduces the general class of SARIMA models, passing through the class of ARIMA models. These models allow to apply the ARMA modeling framework also to time series that have particular non-stationary components to them such as, for example, linear and/or seasonal trends. Extending ARMA modeling to these cases allows SARIMA models to be an extremely flexible class of models that can be used to describe a wide range of phenomena.</p>
<div id="linear-processes" class="section level2">
<h2><span class="header-section-number">4.1</span> Linear Processes</h2>
<p>In order to discuss the classes of models mentioned above, we first present the class of linear processes which underlie many of the most common time series models.</p>

<div class="definition">
<p><span id="def:lp" class="definition"><strong>Definition 4.1  (Linear Process)  </strong></span>A time series, <span class="math inline">\((X_t)\)</span>, is defined to be a linear process if it can be expressed as a linear combination of white noise as follows:</p>
<p><span class="math display">\[{X_t} = \mu + \sum\limits_{j =  - \infty }^\infty  {{\psi _j}{W_{t - j}}} \]</span></p>
where <span class="math inline">\(W_t \sim WN(0, \sigma^2)\)</span> and <span class="math inline">\(\sum\limits_{j = - \infty }^\infty {\left| {{\psi _j}} \right|} &lt; \infty\)</span>.
</div>
<p></p>
<p>Note, the latter assumption is required to ensure that the series has a limit. Furthermore, the set of coefficients <span class="math display">\[{( {\psi _j}) _{j =  - \infty , \cdots ,\infty }}\]</span> can be viewed as a linear filter. These coefficients do not have to be all equal nor symmetric as later examples will show. Generally, the properties of a linear process related to mean and variance are given by:</p>
<p><span class="math display">\[\begin{aligned}
\mu_{X} &amp;= \mu \\
\gamma_{X}(h) &amp;= \sigma _W^2\sum\limits_{j =  - \infty }^\infty  {{\psi _j}{\psi _{h + j}}}
\end{aligned}\]</span></p>
<p>The latter is derived from</p>
<p><span class="math display">\[\begin{aligned}
  \gamma \left( h \right) &amp;= Cov\left( {{x_t},{x_{t + h}}} \right) \\
   &amp;= Cov\left( {\mu  + \sum\limits_{j =  - \infty }^\infty  {{\psi _j}{w_{t - j}}} ,\mu  + \sum\limits_{j =  - \infty }^\infty  {{\psi _j}{w_{t + h - j}}} } \right) \\
   &amp;= Cov\left( {\sum\limits_{j =  - \infty }^\infty  {{\psi _j}{w_{t - j}}} ,\sum\limits_{j =  - \infty }^\infty  {{\psi _j}{w_{t + h - j}}} } \right) \\
   &amp;= \sum\limits_{j =  - \infty }^\infty  {{\psi _j}{\psi _{j + h}}Cov\left( {{w_{t - j}},{w_{t - j}}} \right)}  \\
   &amp;= \sigma _w^2\sum\limits_{j =  - \infty }^\infty  {{\psi _j}{\psi _{j + h}}}  \\ 
\end{aligned} \]</span></p>
<p>Within the above derivation, the key is to realize that <span class="math inline">\(Cov\left( {{w_{t - j}},{w_{t + h - j}}} \right) = 0\)</span> if <span class="math inline">\(t - j \ne t + h - j\)</span>.</p>
<p>Lastly, another convenient way to formalize the definition of a linear process is through the use of the <strong>backshift operator</strong> (or lag operator) which is itself defined as follows:</p>
<p><span class="math display">\[B\,X_t = X_{t-1}.\]</span></p>
<p>The properties of the backshift operator allow us to create composite functions of the type</p>
<p><span class="math display">\[B^2 \, X_t = B (B \, X_t) = B \, X_{t-1} = X_{t-2}\]</span> which allows to generalize as follows</p>
<p><span class="math display">\[B^k \, X_t = X_{t-k}.\]</span> Moreover, we can apply the inverse operator to it (i.e. <span class="math inline">\(B^{-1} \, B = 1\)</span>) thereby allowing us to have, for example:</p>
<p><span class="math display">\[X_t = B^{-1} \, B X_t = B^{-1} X_{t-1}\]</span></p>

<div class="example">
<span id="exm:backdiff" class="example"><strong>Example 4.1  (d-order Differences)  </strong></span>We can re-express <span class="math inline">\(X_t - X_{t-1}\)</span> as <span class="math display">\[\delta X_t = (1 - B) X_t\]</span> or a second order difference as <span class="math display">\[\delta^2 X_t = (1 - B)^2 X_t\]</span> thereby generalizing to a d-order difference as follows: <span class="math display">\[\delta^d X_t = (1 - B)^d X_t.\]</span>
</div>
<p></p>
<p>Having defined the backshift operator, we can now provide an alternative definition of a linear process as follows:</p>
<p><span class="math display">\[{X_t} = \mu + \psi \left( B \right){W_t}\]</span></p>
<p>where <span class="math inline">\(\psi ( B )\)</span> is a polynomial function in <span class="math inline">\(B\)</span> whose coefficients are given by the linear filters <span class="math inline">\((\psi_j)\)</span> (we’ll describe these polynomials further on).</p>

<div class="example">
<p><span id="exm:lpwn" class="example"><strong>Example 4.2  (Linear Process of White Noise)  </strong></span> The white noise process <span class="math inline">\((X_t)\)</span>, defined in <a href="basic-elements-of-time-series.html#wn">2.4.1</a>, can be expressed as a linear process as follows:</p>
<p><span class="math display">\[\psi _j = \begin{cases}
      1 , &amp;\mbox{ if } j = 0\\
      0 , &amp;\mbox{ if } |j| \ge 1
\end{cases}.\]</span></p>
<p>and <span class="math inline">\(\mu = 0\)</span>.</p>
Therefore, <span class="math inline">\(X_t = W_t\)</span>, where <span class="math inline">\(W_t \sim WN(0, \sigma^2_W)\)</span>
</div>
<p></p>

<div class="example">
<p><span id="exm:lpma1" class="example"><strong>Example 4.3  (Linear Process of Moving Average Order 1)  </strong></span> Similarly, consider <span class="math inline">\((X_t)\)</span> to be a MA(1) process, given by <a href="basic-elements-of-time-series.html#ma1">2.4.4</a>. The process can be expressed linearly through the following filters:</p>
<p><span class="math display">\[\psi _j = \begin{cases}
      1, &amp;\mbox{ if } j = 0\\
      \theta , &amp;\mbox{ if } j = 1 \\
      0, &amp;\mbox{ if } j \ge 2
\end{cases}.\]</span></p>
<p>and <span class="math inline">\(\mu = 0\)</span>.</p>
Thus, we have: <span class="math inline">\(X_t = W_t + \theta W_{t-1}\)</span>
</div>
<p></p>

<div class="example">
<p><span id="exm:lpsma" class="example"><strong>Example 4.4  (Linear Process and Symmetric Moving Average)  </strong></span> Consider a symmetric moving average given by:</p>
<p><span class="math display">\[{X_t} = \frac{1}{{2q + 1}}\sum\limits_{j =  - q}^q {{W_{t + j}}} \]</span></p>
<p>Thus, <span class="math inline">\((X_t)\)</span> is defined for <span class="math inline">\(q + 1 \le t \le n-q\)</span>. The above process would be a linear process since:</p>
<p><span class="math display">\[\psi _j = \begin{cases}
      \frac{1}{{2q + 1}} , &amp;\mbox{ if } -q \le j \le q\\
      0 , &amp;\mbox{ if } |j| &gt; q
\end{cases}.\]</span></p>
<p>and <span class="math inline">\(\mu = 0\)</span>.</p>
<p>In practice, if <span class="math inline">\(q = 1\)</span>, we would have:</p>
<span class="math display">\[{X_t} = \frac{1}{3}\left( {{W_{t - 1}} + {W_t} + {W_{t + 1}}} \right)\]</span>
</div>
<p></p>

<div class="example">
<p><span id="exm:lpar1" class="example"><strong>Example 4.5  (Autoregressive Process of Order 1)  </strong></span>If <span class="math inline">\(\left\{X_t\right\}\)</span> follows an AR(1) model defined in <a href="basic-elements-of-time-series.html#ar1">2.4.3</a>, the linear filters are a function of the time lag:</p>
<p><span class="math display">\[\psi _j = \begin{cases}
      \phi^j , &amp;\mbox{ if } j \ge 0\\
      0 , &amp;\mbox{ if } j &lt; 0
\end{cases}.\]</span></p>
and <span class="math inline">\(\mu = 0\)</span>. We would require the condition that <span class="math inline">\(\left| \phi \right| &lt; 1\)</span> in order to respect the condition on the filters (i.e. <span class="math inline">\(\sum\limits_{j = - \infty }^\infty {\left| {{\psi _j}} \right|} &lt; \infty\)</span>).
</div>
<p></p>
</div>
<div id="autoregressive-models---arp" class="section level2">
<h2><span class="header-section-number">4.2</span> Autoregressive Models - AR(p)</h2>
<p>The class of autoregressive models is based on the idea that previous values in the time series are needed to explain current values in the series. For this class of models, we assume that the <span class="math inline">\(p\)</span> previous observations are needed for this purpose and we therefore denote this class as AR(<span class="math inline">\(p\)</span>). In the previous chapter, the model we introduced was an AR(1) in which only the immediately previous observation is needed to explain the following one and therefore represents a particular model which is part of the more general class of AR(p) models.</p>

<div class="definition">
<span id="def:arp" class="definition"><strong>Definition 4.2  (Autoregressive Models of Order p)  </strong></span>The AR(p) models can be formally represented as follows <span class="math display">\[(X_t) = {\phi_1}{X_{t - 1}} + ... + {\phi_p}{X_{t - p}} + {W_t},\]</span> where <span class="math inline">\(\phi_i \neq 0\)</span> (for <span class="math inline">\(i = 1, ..., p\)</span>) and <span class="math inline">\(W_t\)</span> is a (Gaussian) white noise process with variance <span class="math inline">\(\sigma^2\)</span>.
</div>
<p></p>
<p>As earlier in this book, we will assume that the expectation of the process <span class="math inline">\(({X_t})\)</span>, as well as that of the following ones in this chapter, is zero. The reason for this simplification is that if <span class="math inline">\(\mathbb{E} [ X_t ] = \mu\)</span>, we can define an AR process <em>around</em> <span class="math inline">\(\mu\)</span> as follows:</p>
<p><span class="math display">\[X_t - \mu = \sum_{i = 1}^p \phi_i \left(X_{t-i} - \mu \right) + W_t,\]</span></p>
<p>which is equivalent to</p>
<p><span class="math display">\[X_t  = \mu^{\star} +  \sum_{i = 1}^p \phi_i X_{t-i}  + W_t,\]</span></p>
<p>where <span class="math inline">\(\mu^{\star} = \mu (1 - \sum_{i = 1}^p \phi_i)\)</span>. Therefore, to simplify the notation we will generally consider only zero mean processes, since adding means (as well as other deterministic trends) is easy.</p>
<p>A useful way of representing AR(p) processes is through the backshift operator introduced in the previous section and is as follows</p>
<p><span class="math display">\[\begin{aligned}
  {X_t} &amp;= {\phi_1}{X_{t - 1}} + ... + {\phi_p}{X_{t - p}} + {W_t} \\
   &amp;= {\phi_1}B{X_t} + ... + {\phi_p}B^p{X_t} + {W_t} \\
   &amp;= ({\phi_1}B + ... + {\phi_p}B^p){X_t} + {W_t} \\ 
\end{aligned},\]</span></p>
<p>which finally yields</p>
<p><span class="math display">\[(1 - {\phi _1}B - ... - {\phi_p}B^p){X_t} = {W_t},\]</span></p>
<p>which, in abbreviated form, can be expressed as</p>
<p><span class="math display">\[\phi(B){X_t} = W_t.\]</span></p>
<p>We will see that <span class="math inline">\(\phi(B)\)</span> is important to establish the stationarity of these processes and is called the <em>autoregressive</em> operator. Moreover, this quantity is closely related to another important property of AR(p) processes called <em>causality</em>. Before formally defining this new property we consider the following example which provides an intuitive illustration of its importance.</p>
<p><strong>Example:</strong> Consider a classical AR(1) model with <span class="math inline">\(|\phi| &gt; 1\)</span>. Such a model could be expressed as</p>
<p><span class="math display">\[X_t = \phi^{-1} X_{t+1} - \phi^{-1} W_t = \phi^{-k} X_{t+k} - \sum_{i = 1}^{k-1} \phi^{-i} W_{t+i}.\]</span></p>
<p>Since <span class="math inline">\(|\phi| &gt; 1\)</span>, we obtain</p>
<p><span class="math display">\[X_t = - \sum_{j = 1}^{\infty} \phi^{-j} W_{t+j},\]</span></p>
<p>which is a linear process and therefore is stationary. Unfortunately, such a model is useless because we need the future to predict the future. These processes are called non-causal.</p>
<div id="properties-of-arp-models" class="section level3">
<h3><span class="header-section-number">4.2.1</span> Properties of AR(p) models</h3>
<p>In this section we will describe the main property of the AR(p) model which has already been mentioned in the previous paragraphs and therefore let us now introduce the property of causality in a more formal manner.</p>
<strong>Definition:</strong> An AR(p) model is <em>causal</em> if the time series <span class="math inline">\((X_t)_{-\infty}^{\infty}\)</span> can be written as a one-sided linear process:
<span class="math display" id="eq:causal">\[\begin{equation}
    X_t = \sum_{j = 0}^{\infty} \psi_j W_{t-j} = \frac{1}{\phi(B)} W_t = \psi(B) W_t,
\tag{4.1}
\end{equation}\]</span>
<p>where <span class="math inline">\(\phi(B) = \sum_{j = 0}^{\infty} \phi_j B^j\)</span>, and <span class="math inline">\(\sum_{j=0}^{\infty}|\phi_j| &lt; \infty\)</span> and setting <span class="math inline">\(\phi_0 = 1\)</span>.</p>
<p>As discussed earlier this condition implies that only the past values of the time series can explain the future values of it and not viceversa. Moreover, given the expression of the linear filters given by <span class="math display">\[\frac{1}{\phi(B)}\]</span> it is obvious that a solution exists only when <span class="math inline">\(\phi(B) = \sum_{j = 0}^{\infty} \phi_j B^j \neq 0\)</span> (thereby implying causality). A condition for this to be respected is for the roots of <span class="math inline">\(\phi(B) = 0\)</span> to lie outside the unit circle.</p>
<!-- However, it might be difficult and not obvious to show the causality of an AR(p) process by using the above definitions directly, thus the following properties are useful in practice.  -->
<!-- **Causality** -->
<!-- If an AR(p) model is causal, then the coefficients of the one-sided linear process given in \@ref(eq:causal) can be obtained by solving -->
<!-- \begin{equation*} -->
<!--     \psi(z) = \frac{1}{\sum_{j=0}^{\infty} \phi_j z^j} = \frac{1}{\phi(z)}, \mbox{ } |z| \leq 1. -->
<!-- \end{equation*} -->
<!-- It can be seen how there is no solution to the above equation if $\phi(z) = 0$ and therefore an AR(p) is causal if and only if $\phi(z) \neq 0$. A condition for this to be respected is for the roots of $\phi(z) = 0$ to lie outside the unit circle. -->

<div class="example">
<p><span id="exm:AR2asLP" class="example"><strong>Example 4.6  (Transform an AR(2) into a Linear Process)  </strong></span>Consider an AR(2) process <span class="math display">\[X_t = 1.3 X_{t-1} - 0.4 X_{t-2} + W_t,\]</span> which we would like to transform into a linear process. This can be done using the following approach:</p>
<ul>
<li><p>Step 1: The autoregressive operator of this model can be expressed as <span class="math display">\[
\phi(B) = 1-1.3B+0.4B^2 = (1-0.5B)(1-0.8B),
\]</span> and has roots 2 and 1.25, both <span class="math inline">\(&gt;1\)</span>. Thus, we should be able to convert it into a linear process.</p></li>
<li><p>Step 2: We know that if an AR(p) process has all its roots outside the unit circle, then we can write <span class="math inline">\(X_t = \frac{1}{\phi(B)} W_t\)</span>. By applying the partial fractions trick, we can inverse the autoregressive operator <span class="math inline">\(\phi(B)\)</span> as follows: <span class="math display">\[ \begin{aligned}
\phi^{-1}(B) &amp;= \frac{1}{(1-0.5B)(1-0.8B)} = \frac{c_1}{(1-0.5B)} + \frac{c_2}{(1-0.8B)} \\
&amp;= \frac{c_2(1-0.5B) + c_1(1-0.8B)}{(1-0.5B)(1-0.8B)} = \frac{(c_1 + c_2)-(0.8c_1+0.5c_2)B}{(1-0.5B)(1-0.8B)}.
\end{aligned} \]</span></p></li>
</ul>
<p>To solve for <span class="math inline">\(c_1\)</span> and <span class="math inline">\(c_2\)</span>: <span class="math display">\[ \begin{cases}
      c_1 + c_2 &amp;=1\\
      0.8c_1+0.5c_2 &amp;=0
\end{cases} \to 
\begin{cases}
      c_1 &amp;= -5/3\\
      c_2 &amp;= 8/3.
\end{cases} \]</span></p>
<p>So we obtain <span class="math display">\[
\phi^{-1}(B) = \frac{-5}{3(1-0.5B)} + \frac{8}{3(1-0.8B)}.
\]</span></p>
<ul>
<li>Step 3: Using the Geometric series, i.e. <span class="math inline">\(a\sum_{j=0}^{\infty} r^j = \frac{a}{1-r}\)</span> if <span class="math inline">\(|r| &lt;1\)</span>, we have <span class="math display">\[ \begin{cases}
  \frac{-5}{3(1-0.5B)} = -\frac{5}{3} \sum_{j=0}^\infty 0.5^j B^j, &amp;\mbox{ if } |B| &lt; 2 \\
  \frac{8}{3(1-0.8B)} = \frac{8}{3} \sum_{j=0}^\infty 0.8^j B^j, &amp;\mbox{ if } |B| &lt; 1.25.
\end{cases} \]</span></li>
</ul>
<p>So we can express <span class="math inline">\(\phi^{-1}(B)\)</span> as <span class="math display">\[
\phi^{-1}(B) = \sum_{j=0}^\infty \Big[ -\frac{5}{3} (0.5)^j  + \frac{8}{3} (0.8)^j \Big] B^j, \;\;\; \text{if  } |B|&lt;1.25.
\]</span></p>
<ul>
<li>Step 4: Finally, we obtain <span class="math display">\[ \begin{aligned}
X_t &amp;= \phi(B)^{-1} W_t = \sum_{j=0}^\infty \Big[ -\frac{5}{3} (0.5)^j  + \frac{8}{3} (0.8)^j \Big] B^j W_t \\
&amp;= \sum_{j=0}^\infty \Big[ -\frac{5}{3} (0.5)^j  + \frac{8}{3} (0.8)^j \Big] W_{t-j},
\end{aligned} \]</span> which verifies that the AR(2) is causal, and therefore is stationary.
</div>
</li>
</ul>

<div class="example">
<p><span id="exm:AR2causalcond" class="example"><strong>Example 4.7  (Causal Conditions for an AR(2) Process)  </strong></span>We already know that an AR(1) is causal with the simple condition <span class="math inline">\(|\phi_1|&lt;1\)</span>. It seems natural to believe that an AR(2) should be causal (and therefore stationary) with the condition that <span class="math inline">\(|\phi_i| &lt;1, \; i=1,2\)</span>. However, this is actually not the case as we illustrate below.</p>
<p>We can express an AR(2) process as <span class="math display">\[
X_t = \phi_1 X_{t-1} + \phi_2 X_{t-2} + W_t = \phi_1 BX_t + \phi_2 B^2 X_t + W_t,
\]</span> thereby delivering the following autoregressive operator: <span class="math display">\[
\phi(B) = 1-\phi_1 B - \phi_2 B^2 = \Big( 1-\frac{B}{\lambda_1} \Big) \Big( 1-\frac{B}{\lambda_2} \Big)
\]</span> where <span class="math inline">\(\lambda_1\)</span> and <span class="math inline">\(\lambda_2\)</span> are the roots of <span class="math inline">\(\phi(B)\)</span> such that <span class="math display">\[ \begin{aligned}
\phi_1 &amp;=  \frac{1}{\lambda_1} + \frac{1}{\lambda_2}, \\
\phi_2 &amp;= - \frac{1}{\lambda_1} \frac{1}{\lambda_2}.
\end{aligned} \]</span></p>
<p>That is, <span class="math display">\[\begin{aligned}
\lambda_1 &amp;= \frac{\phi_1 + \sqrt{\phi_1^2 + 4\phi_2}}{-2\phi_2}, \\
\lambda_2 &amp;= \frac{\phi_1 - \sqrt{\phi_1^2 + 4\phi_2}}{-2\phi_2}.
\end{aligned} \]</span></p>
<p>In order to ensure the causality of the model, we need the roots of <span class="math inline">\(\phi(B)\)</span>, i.e. <span class="math inline">\(\lambda_1\)</span> and <span class="math inline">\(\lambda_2\)</span>, to lie outside the unit circle.</p>
<p><span class="math display">\[ \begin{cases}
|\lambda_1| &amp;&gt; 1 \\
|\lambda_2| &amp;&gt; 1,
\end{cases} \]</span> if and only if <span class="math display">\[ \begin{cases}
\phi_1 + \phi_2 &amp;&lt; 1 \\
\phi_2 - \phi_1 &amp;&lt; 1 \\
|\phi_2| &amp;&lt;1.
\end{cases} \]</span></p>
<p>We can show the <em>if</em> part of the statement as follows: <span class="math display">\[ \begin{aligned}
&amp; \phi_1 + \phi_2 = \frac{1}{\lambda_1} + \frac{1}{\lambda_2} - \frac{1}{\lambda_1 \lambda_2} = \frac{1}{\lambda_1} \Big(1-\frac{1}{\lambda_2} \Big) + \frac{1}{\lambda_2} &lt; 1 - \frac{1}{\lambda_2} + \frac{1}{\lambda_2} = 1 \;\; \text{since } 1-\frac{1}{\lambda_2} &gt; 0, \\
&amp; \phi_2 - \phi_1 = -\frac{1}{\lambda_1 \lambda_2} - \frac{1}{\lambda_1} - \frac{1}{\lambda_2} = -\frac{1}{\lambda_1} \Big( \frac{1}{\lambda_2} +1 \Big) - \frac{1}{\lambda_2} &lt; \frac{1}{\lambda_2}+1-\frac{1}{\lambda_2} = 1 \;\; \text{since } \frac{1}{\lambda_2}+1 &gt; 0, \\
&amp; |\phi_2| = \frac{1}{|\lambda_1| |\lambda_2|} &lt; 1.
\end{aligned} \]</span></p>
<p>We can also show the <em>only if</em> part of the statement as follows:</p>
<p>Since <span class="math inline">\(\lambda_1 = \frac{\phi_1 + \sqrt{\phi_1^2 + 4\phi_2}}{-2\phi_2}\)</span> and <span class="math inline">\(\phi_2 - 1 &lt; \phi_1 &lt; 1- \phi_2\)</span>, we have <span class="math display">\[
\lambda_1^2 = \frac{(\phi_1 + \sqrt{\phi_1^2 + 4\phi_2})^2}{4\phi_2^2} &lt; \frac{\Big( (1-\phi_2)+ \sqrt{(1-\phi_2)^2 + 4\phi_2} \Big)^2}{4\phi_2^2} = \frac{4}{4\phi_2^2} \leq 1. 
\]</span></p>
<p>Since <span class="math inline">\(\lambda_2 = \frac{\phi_1 - \sqrt{\phi_1^2 + 4\phi_2}}{-2\phi_2}\)</span> and <span class="math inline">\(\phi_2 - 1 &lt; \phi_1 &lt; 1- \phi_2\)</span>, we have <span class="math display">\[
\lambda_2^2 = \frac{(\phi_1 - \sqrt{\phi_1^2 + 4\phi_2})^2}{4\phi_2^2} &lt; \frac{\Big( (\phi_2-1)+ \sqrt{(\phi_2-1)^2 + 4\phi_2} \Big)^2}{4\phi_2^2} = \frac{4\phi_2^2}{4\phi_2^2} = 1. 
\]</span></p>
Finally, the causal region of an AR(2) is demonstrated as
</div>
<p></p>
<div class="figure" style="text-align: center"><span id="fig:correxample2"></span>
<img src="images/causal_AR2.png" alt="Causal Region for Parameters of an AR(2) Process"  />
<p class="caption">
Figure 4.1: Causal Region for Parameters of an AR(2) Process
</p>
</div>
</div>
<div id="estimation-of-arp-models" class="section level3">
<h3><span class="header-section-number">4.2.2</span> Estimation of AR(p) models</h3>
<p>Given the above defined properties of AR(p) models, we will now discuss how these models can be estimated, more specifically how the <span class="math inline">\(p+1\)</span> parameters can be obtained from an observed time series. Indeed, a reliable estimation of these models is necessary in order to intepret and describe different natural phenomena and/or forecast possible future values of the time series.</p>
A first approach builds upon the earlier definition of AR(<span class="math inline">\(p\)</span>) models being a linear process. Recall that
<span class="math display">\[\begin{equation}
    X_t = \sum_{j = 1}^{p} \phi_j X_{t-j}
\end{equation}\]</span>
which delivers the following autocovariance function
<span class="math display">\[\begin{equation}
    \gamma(h) = \text{cov}(X_{t+h}, X_t) = \text{cov}\left(\sum_{j = 1}^{p} \phi_j X_{t+h-j}, X_t\right) = \sum_{j = 1}^{p} \phi_j \gamma(h-j), \mbox{ } h \geq 1.
\end{equation}\]</span>
Rearranging the above expressions we obtain the following general equations
<span class="math display">\[\begin{equation}
    \gamma(h) - \sum_{j = 1}^{p} \phi_j \gamma(h-j) = 0, \mbox{ } h \geq 1
\end{equation}\]</span>
and, recalling that <span class="math inline">\(\gamma(h) = \gamma(-h)\)</span>,
<span class="math display">\[\begin{equation}
    \gamma(0) - \sum_{j = 1}^{p} \phi_j \gamma(j) = \sigma_w^2.
\end{equation}\]</span>
<p>We can now define the Yule-Walker equations.</p>
<strong>Definition:</strong> The Yule-Walker equations are given by
<span class="math display">\[\begin{equation}
    \gamma(h) = \phi_1 \gamma(h-1) + ... + \phi_p \gamma(h-p), \mbox{ } h = 1,...,p
\end{equation}\]</span>
and
<span class="math display">\[\begin{equation}
    \sigma_w^2 = \gamma(0) - \phi_1 \gamma(1) - ... - \phi_p \gamma(p).
\end{equation}\]</span>
which in matrix notation can be defined as follows
<span class="math display">\[\begin{equation}
    \Gamma_p \mathbf{\phi} = \mathbf{\gamma}_p \,\, \text{and} \,\, \sigma_w^2 = \gamma(0) - \mathbf{\phi}&#39;\mathbf{\gamma}_p
\end{equation}\]</span>
<p>where <span class="math inline">\(\Gamma_p\)</span> is the <span class="math inline">\(p\times p\)</span> matrix containing the autocovariances <span class="math inline">\(\gamma(k-j)\)</span>, where <span class="math inline">\(j,k = 1, ...,p\)</span>, while <span class="math inline">\(\mathbf{\phi} = (\phi_1,...,\phi_p)&#39;\)</span> and <span class="math inline">\(\mathbf{\gamma}_p = (\gamma(1),...,\gamma(p))&#39;\)</span> are <span class="math inline">\(p\times 1\)</span> vectors.</p>
Considering the Yule-Walker equations, it is possible to use a method of moments approach and simply replace the theoretical quantities given in the previous definition with their empirical (estimated) counterparts that we saw in the previous chapter. This gives us the following Yule-Walker estimators
<span class="math display">\[\begin{equation}
    \hat{\mathbf{\phi}} = \hat{\Gamma}_p^{-1}\hat{\mathbf{\gamma}}_p \,\, \text{and} \,\, \hat{\sigma}_w^2 = \hat{\gamma}(0) - \hat{\mathbf{\gamma}}_p&#39;\hat{\Gamma}_p^{-1}\hat{\mathbf{\gamma}}_p .
\end{equation}\]</span>
<p>These estimators have the following asymptotic properties.</p>
<p><strong>Consistency and Asymptotic Normality of Yule-Walker estimators:</strong> The Yule-Walker estimators for a causal AR(p) model have the following asymptotic properties:</p>
<span class="math display">\[\begin{equation*}
\sqrt{T}(\hat{\mathbf{\phi}}- \mathbf{\phi}) \xrightarrow{\mathcal{D}} \mathcal{N}(\mathbf{0},\sigma_w^2\Gamma_p^{-1}) \,\, \text{and} \,\, \hat{\sigma}_w^2 \xrightarrow{\mathcal{P}} \sigma_w^2 .
\end{equation*}\]</span>
Therefore the Yule-Walker estimators have an asymptotically normal distribution and the estimator of the innovation variance is consistent. Moreover, these estimators are also optimal for AR(p) models, meaning that they are also efficient. However, there is also another method which allows to achieve this efficiency (also for general ARMA models that will be tackled further on) and this is the Maximum Likelihood Estimation (MLE) method. Considering an AR(1) model as an example, and assuming without loss of generality that its expectation is zero, we have the following representation of the AR(1) model
<span class="math display">\[\begin{equation*}
X_t = \phi X_{t-1} + W_t
\end{equation*}\]</span>
where <span class="math inline">\(|\phi|&lt;1\)</span> and <span class="math inline">\(W_t \overset{iid}{\sim} \mathcal{N}(0,\sigma_w^2)\)</span>. Supposing we have observations <span class="math inline">\((x_t)_{t=1,...,T}\)</span> issued from this model, then the likelihood function for this setting is given by
<span class="math display">\[\begin{equation*}
L(\phi,\sigma_w^2) = f(\phi,\sigma_w^2|x_1,...,x_T)
\end{equation*}\]</span>
which, for an AR(1) model, can be rewritten as follows
<span class="math display">\[\begin{equation*}
L(\phi,\sigma_w^2) = f(x_1)f(x_2|x_1)\cdot \cdot \cdot f(x_T|x_{T-1}).
\end{equation*}\]</span>
If we define <span class="math inline">\(\Omega_t^p\)</span> as the information contained in the previous <span class="math inline">\(p\)</span> observations (before time <span class="math inline">\(t\)</span>), the above expression can be generalized for an AR(p) model as follows
<span class="math display">\[\begin{equation*}
L(\phi,\sigma_w^2) = f(x_1,...,x_p)f(x_{p+1}|\Omega_{p+1}^p)\cdot \cdot \cdot f(x_T|\Omega_{T}^p)
\end{equation*}\]</span>
where <span class="math inline">\(f(x_1,...,x_p)\)</span> is the joint probability distribution of the first <span class="math inline">\(p\)</span> observations. Going back to the AR(1) setting, based on our assumption on <span class="math inline">\((W_t)\)</span> we know that <span class="math inline">\(x_t|x_{t-1} \sim \mathcal{N}(\phi x_{t-1},\sigma_w^2)\)</span> and therefore we have that
<span class="math display">\[\begin{equation*}
f(x_t|x_{t-1}) = f_w(x_t - \phi x_{t-1})
\end{equation*}\]</span>
where <span class="math inline">\(f_w(\cdot)\)</span> is the distribution of <span class="math inline">\(W_t\)</span>. This rearranges the likelihood function as follows
<span class="math display">\[\begin{equation*}
L(\phi,\sigma_w^2) = f(x_1)\prod_{t=2}^T f_w(x_t - \phi x_{t-1})
\end{equation*}\]</span>
where <span class="math inline">\(f(x_1)\)</span> can be found through the causal representation
<span class="math display">\[\begin{equation*}
x_1 = \sum_{j=0}^{\infty} \phi^j w_{1-j} 
\end{equation*}\]</span>
which implies that <span class="math inline">\(x_1\)</span> follows a normal distribution with zero expectation and a variance given by <span class="math inline">\(\frac{\sigma_w^2}{(1-\phi^2)}\)</span>. Based on this, the likelihood function of an AR(1) finally becomes
<span class="math display">\[\begin{equation*}
L(\phi,\sigma_w^2) = (2\pi \sigma_w^2)^{-\frac{T}{2}} (1 - \phi^2)^{\frac{1}{2}} \exp \left(-\frac{S(\phi)}{2 \sigma_w^2}\right)
\end{equation*}\]</span>
with <span class="math inline">\(S(\phi) = (1-\phi^2) x_1^2 + \sum_{t=2}^T (x_t -\phi x_{t-1})^2\)</span>. Once the derivative of the logarithm of the likelihood is taken, the minimization of the negative of this function is usually done numerically. However, if we condition on the initial values, the AR(p) models are linear and, for example, we can then define the conditional likelihood of an AR(1) as
<span class="math display">\[\begin{equation*}
L(\phi,\sigma_w^2|x_1) = (2\pi \sigma_w^2)^{-\frac{T-1}{2}} \exp \left(-\frac{S_c(\phi)}{2 \sigma_w^2}\right)
\end{equation*}\]</span>
where
<span class="math display">\[\begin{equation*}
S_c(\phi) = \sum_{t=2}^T (x_t -\phi x_{t-1})^2 .
\end{equation*}\]</span>
The latter is called the conditional sum of squares and <span class="math inline">\(\phi\)</span> can be estimated as a straightforward linear regression problem. Once an estimate <span class="math inline">\(\hat{\phi}\)</span> is obtained, this can be used to obtain the conditional maximum likelihood estimate of <span class="math inline">\(\sigma_w^2\)</span>
<span class="math display">\[\begin{equation*}
\hat{\sigma}_w^2 = \frac{S_c(\hat{\phi})}{(T-1)} .
\end{equation*}\]</span>
The estimation methods presented so far are standard for these kind of models. Nevertheless, if the data suffers from some form of contamination, these methods can become highly biased. For this reason, some robust estimators are available to limit this problematic if there are indeed outliers in the observed time series. One of these methods relies on the estimator proposed in Kunsch (1984) who underlines that the MLE score function of an AR(p) is given by
<span class="math display">\[\begin{equation*}
 \kappa(\mathbf{\theta}|x_j,...x_{j+p}) = \frac{\partial}{\partial \mathbf{\theta}} (x_{j+p} - \sum_{k=1}^p \phi_k x_{j+p-k})^2
\end{equation*}\]</span>
where <span class="math inline">\(\theta\)</span> is the parameter vector containing, in the case of an AR(1) model, the two parameters <span class="math inline">\(\phi\)</span> and <span class="math inline">\(\sigma_w^2\)</span> (i.e. <span class="math inline">\(\theta = [\phi \,\, \sigma_w^2]\)</span>). This delivers the estimating equation
<span class="math display">\[\begin{equation*}
\sum_{j=1}^{n-p} \kappa (\hat{\mathbf{\theta}}|x_j,...x_{j+p}) = 0 .
\end{equation*}\]</span>
The score function <span class="math inline">\(\kappa(\cdot)\)</span> is clearly not bounded, in the sense that if we arbitrarily move a value of <span class="math inline">\((x_t)\)</span> to infinity then the score function also goes to infinity thereby delivering a biased estimation procedure. To avoid that outlying observations bias the estimation excessively, a bounded score function can be used to deliver an M-estimator given by
<span class="math display">\[\begin{equation*}
\sum_{j=1}^{n-p} \psi (\hat{\mathbf{\theta}}|x_j,...x_{j+p}) = 0,
\end{equation*}\]</span>
where <span class="math inline">\(\psi(\cdot)\)</span> is a function of bounded variation. When conditioning on the first <span class="math inline">\(p\)</span> observations, this problem can be brought back to a linear regression problem which can be applied in a robust manner using the robust regression tools available in <code>R</code> such as <code>rlm</code> or <code>lmrob</code>. However, another available tool in <code>R</code> which does not require a strict specification of the distribution function (also for general ARMA models) is the <code>gmwm</code> function (in the <code>gmwm</code> package) in which it is possible to specify the option <code>robust = TRUE</code>. This function makes use of a quantity called the wavelet variance (denoted as <span class="math inline">\(\boldsymbol{\nu}\)</span>) which is estimated robustly and then used to retrieve the parameters <span class="math inline">\(\theta\)</span> of the time series model. The robust estimate is obtained by solving the following minimization problem
<span class="math display">\[\begin{equation*}
\hat{\boldsymbol{\theta}} = \underset{\boldsymbol{\theta} \in \boldsymbol{\Theta}}{\text{argmin}} (\hat{\boldsymbol{\nu}} - \boldsymbol{\nu}(\boldsymbol{\theta}))^T\boldsymbol{\Omega}(\hat{\boldsymbol{\nu}} - \boldsymbol{\nu}({\boldsymbol{\theta}})),
\end{equation*}\]</span>
<p>where <span class="math inline">\(\hat{\boldsymbol{\nu}}\)</span> is the robustly estimated wavelet variance, <span class="math inline">\(\boldsymbol{\nu}({\boldsymbol{\theta}})\)</span> is the theoretical wavelet variance (implied by the model we want to estimate) and <span class="math inline">\(\boldsymbol{\Omega}\)</span> is a positive definite weighting matrix. Below we show some simulation studies where we present the results of the above estimation procedures in absence and in presence of contamination in the data. As a reminder, so far we have mainly discussed three estimators for the parameters of AR(<span class="math inline">\(p\)</span>) models (i.e. Yule-Walker, maximum likelihod, and RGMWM estimators).</p>
<p>In <code>R</code> the first three estimators can be computed as follows:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mod =<span class="st"> </span><span class="kw">ar</span>(Xt, <span class="dt">order.max =</span> p, <span class="dt">method =</span> select_method, <span class="dt">demean =</span> <span class="ot">TRUE</span>, <span class="dt">aic =</span> <span class="ot">FALSE</span>)</code></pre></div>
<p>In the above sample code <code>Xt</code> denotes the time series (a vector of length <span class="math inline">\(T\)</span>), <code>p</code> is the order of the AR(<span class="math inline">\(p\)</span>) and <code>demean = TRUE</code> indicates that the mean of the process should be estimated (if this is not the case, then use <code>demean = FALSE</code>). The <code>select_method</code> input can be (among others) <code>&quot;mle&quot;</code> for the maximum likelihood and <code>&quot;yule-walker&quot;</code> for the Yule-Walker estimator. For example, if you would like to estimate a zero mean AR(3) with the MLE you can use the code:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mod =<span class="st"> </span><span class="kw">ar</span>(Xt, <span class="dt">order.max =</span> <span class="dv">3</span>, <span class="dt">method =</span> <span class="st">&quot;mle&quot;</span>, <span class="dt">demean =</span> <span class="ot">FALSE</span>, <span class="dt">aic =</span> <span class="ot">FALSE</span>)</code></pre></div>
<p>On the other hand, the RGMWM is implemented in the <code>gmwm2</code> R package which can be downloaded and installed as follows:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">devtools::<span class="kw">install_github</span>(<span class="st">&quot;smac-group/gmwm2&quot;</span>)</code></pre></div>
<p>Once this package is installed, you can estimate robust AR models using the following syntax:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mod =<span class="st"> </span><span class="kw">gmwm</span>(<span class="kw">AR</span>(p), Xt, <span class="dt">robust =</span> <span class="ot">TRUE</span>)</code></pre></div>
<p>Removing the mean is not strictly necessary for the <code>gmwm</code> function since it won’t estimate it and can consistently estimate the parameters of the time series model anyway. However, if you want to estimate the mean assuming the time series is stationary, this can be done via the sample mean (or median) which can then be removed from the observed time series to obtain a zero mean <code>Xt</code>. For example, to estimate a robust AR(3) you can use the code:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mod =<span class="st"> </span><span class="kw">gmwm</span>(<span class="kw">AR</span>(<span class="dv">3</span>), Xt, <span class="dt">robust =</span> <span class="ot">TRUE</span>)</code></pre></div>
<p>We now have the necessary <code>R</code> functions to deliver the above mentioned estimators and we can now proceed to the simulation study. In particular, we simulate three different processes <span class="math inline">\(X_t, Y_t, Z_t\)</span> by using the first as an uncontaminated process defined as <span class="math display">\[X_t = 0.5 X_{t-1} - 0.25 X_{t-2} + W_t,\]</span> with <span class="math inline">\(W_t \overset{iid}{\sim} N(0, 1)\)</span>. This first process <span class="math inline">\((X_t)\)</span> is uncontaminated while the other two processes are contaminated versions of the first that can often be observed in practice. The first type of contamination can be seen in <span class="math inline">\((Y_t)\)</span> and is delivered by replacing a portion of the original process with a process defined as <span class="math display">\[U_t = 0.90 U_{t-1} - 0.40 U_{t-2} + V_t,\]</span> where <span class="math inline">\(V_t \overset{iid}{\sim} N(0, 9)\)</span>. The second form of contamination can be seen in <span class="math inline">\((Z_t)\)</span> and consists in the so-called point-wise contamination where randomly selected points from <span class="math inline">\(X_t\)</span> are replaced with <span class="math inline">\(N_t \overset{iid}{\sim} N(0, 9)\)</span>.</p>
<p>The code below performs the simulation study where it can be seen how the contaminated processes <span class="math inline">\((Y_t)\)</span> and <span class="math inline">\((Z_t)\)</span> are generated. Once this is done, for each simultation the code estimates the parameters of the AR(2) model using the three different estimation methods.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Load gmwm2</span>
<span class="kw">library</span>(gmwm2)

<span class="co"># Number of bootstrap iterations</span>
B =<span class="st"> </span><span class="dv">250</span>

<span class="co"># Sample size</span>
n =<span class="st"> </span><span class="dv">500</span>

 <span class="co"># Proportion of contamination</span>
eps =<span class="st"> </span><span class="fl">0.05</span>       

<span class="co"># Number of contaminated observations</span>
cont =<span class="st"> </span><span class="kw">round</span>(eps*n)   

<span class="co"># Simulation storage</span>
res.Xt.MLE =<span class="st"> </span>res.Xt.YW =<span class="st"> </span>res.Xt.RGMWM =<span class="st"> </span><span class="kw">matrix</span>(<span class="ot">NA</span>, B, <span class="dv">3</span>)
res.Yt.MLE =<span class="st"> </span>res.Yt.YW =<span class="st"> </span>res.Yt.RGMWM =<span class="st"> </span><span class="kw">matrix</span>(<span class="ot">NA</span>, B, <span class="dv">3</span>)
res.Zt.MLE =<span class="st"> </span>res.Zt.YW =<span class="st"> </span>res.Zt.RGMWM =<span class="st"> </span><span class="kw">matrix</span>(<span class="ot">NA</span>, B, <span class="dv">3</span>)
  
<span class="co"># Begin bootstrap</span>
for (i in <span class="kw">seq_len</span>(B)){
  <span class="co"># Set seed for reproducibility</span>
  <span class="kw">set.seed</span>(<span class="dv">1982</span> +<span class="st"> </span>i)
  
  <span class="co"># Generate processes</span>
  Xt =<span class="st"> </span><span class="kw">gen_gts</span>(n, <span class="kw">AR</span>(<span class="dt">phi =</span> <span class="kw">c</span>(<span class="fl">0.5</span>, <span class="fl">0.25</span>), <span class="dt">sigma2 =</span> <span class="dv">1</span>))
  Yt =<span class="st"> </span>Zt =<span class="st"> </span>Xt
  
  <span class="co"># Generate Ut contamination process that replaces a portion of original signal</span>
  index_start =<span class="st"> </span><span class="kw">sample</span>(<span class="dv">1</span>:(n-cont<span class="dv">-1</span>), <span class="dv">1</span>)
  index_end =<span class="st"> </span>index_start +<span class="st"> </span>cont -<span class="st"> </span><span class="dv">1</span>
  Yt[index_start:index_end] =<span class="st"> </span><span class="kw">gen_gts</span>(cont, <span class="kw">AR</span>(<span class="dt">phi =</span> <span class="kw">c</span>(<span class="fl">0.9</span>,-<span class="fl">0.4</span>), <span class="dt">sigma2 =</span> <span class="dv">9</span>))
  
  <span class="co"># Generate Nt contamination that inject noise at random</span>
  Zt[<span class="kw">sample</span>(n, cont, <span class="dt">replace =</span> <span class="ot">FALSE</span>)] =<span class="st"> </span><span class="kw">gen_gts</span>(cont, <span class="kw">WN</span>(<span class="dt">sigma2 =</span> <span class="dv">9</span>))
  
  <span class="co"># Fit Yule-Walker estimators on the three time series</span>
  mod.Xt.YW =<span class="st"> </span><span class="kw">ar</span>(Xt, <span class="dt">order.max =</span> <span class="dv">2</span>, <span class="dt">method =</span> <span class="st">&quot;yule-walker&quot;</span>, 
                 <span class="dt">demean =</span> <span class="ot">FALSE</span>, <span class="dt">aic =</span> <span class="ot">FALSE</span>)
  mod.Yt.YW =<span class="st"> </span><span class="kw">ar</span>(Yt, <span class="dt">order.max =</span> <span class="dv">2</span>, <span class="dt">method =</span> <span class="st">&quot;yule-walker&quot;</span>, 
                 <span class="dt">demean =</span> <span class="ot">FALSE</span>, <span class="dt">aic =</span> <span class="ot">FALSE</span>)
  mod.Zt.YW =<span class="st"> </span><span class="kw">ar</span>(Zt, <span class="dt">order.max =</span> <span class="dv">2</span>, <span class="dt">method =</span> <span class="st">&quot;yule-walker&quot;</span>, 
                 <span class="dt">demean =</span> <span class="ot">FALSE</span>, <span class="dt">aic =</span> <span class="ot">FALSE</span>)
  
  <span class="co"># Store results</span>
  res.Xt.YW[i, ] =<span class="st"> </span><span class="kw">c</span>(mod.Xt.YW$ar, mod.Xt.YW$var.pred)
  res.Yt.YW[i, ] =<span class="st"> </span><span class="kw">c</span>(mod.Yt.YW$ar, mod.Yt.YW$var.pred)
  res.Zt.YW[i, ] =<span class="st"> </span><span class="kw">c</span>(mod.Zt.YW$ar, mod.Zt.YW$var.pred)
  

  <span class="co"># Fit MLE on the three time series</span>
  mod.Xt.MLE =<span class="st"> </span><span class="kw">ar</span>(<span class="kw">as.vector</span>(Xt), <span class="dt">order.max =</span> <span class="dv">2</span>, <span class="dt">method =</span> <span class="st">&quot;mle&quot;</span>, 
                  <span class="dt">demean =</span> <span class="ot">FALSE</span>, <span class="dt">aic =</span> <span class="ot">FALSE</span>)
  mod.Yt.MLE =<span class="st"> </span><span class="kw">ar</span>(<span class="kw">as.vector</span>(Yt), <span class="dt">order.max =</span> <span class="dv">2</span>, <span class="dt">method =</span> <span class="st">&quot;mle&quot;</span>, 
                  <span class="dt">demean =</span> <span class="ot">FALSE</span>, <span class="dt">aic =</span> <span class="ot">FALSE</span>)
  mod.Zt.MLE =<span class="st"> </span><span class="kw">ar</span>(<span class="kw">as.vector</span>(Zt), <span class="dt">order.max =</span> <span class="dv">2</span>, <span class="dt">method =</span> <span class="st">&quot;mle&quot;</span>, 
                  <span class="dt">demean =</span> <span class="ot">FALSE</span>, <span class="dt">aic =</span> <span class="ot">FALSE</span>)
  
  <span class="co"># Store results</span>
  res.Xt.MLE[i, ] =<span class="st"> </span><span class="kw">c</span>(mod.Xt.MLE$ar, mod.Xt.MLE$var.pred)
  res.Yt.MLE[i, ] =<span class="st"> </span><span class="kw">c</span>(mod.Yt.MLE$ar, mod.Yt.MLE$var.pred)
  res.Zt.MLE[i, ] =<span class="st"> </span><span class="kw">c</span>(mod.Zt.MLE$ar, mod.Zt.MLE$var.pred)
  
  <span class="co"># Fit RGMWM on the three time series</span>
  res.Xt.RGMWM[i, ] =<span class="st"> </span><span class="kw">gmwm</span>(<span class="kw">AR</span>(<span class="dv">2</span>), Xt, <span class="dt">robust =</span> <span class="ot">TRUE</span>)$estimate
  res.Yt.RGMWM[i, ] =<span class="st"> </span><span class="kw">gmwm</span>(<span class="kw">AR</span>(<span class="dv">2</span>), Yt, <span class="dt">robust =</span> <span class="ot">TRUE</span>)$estimate
  res.Zt.RGMWM[i, ] =<span class="st"> </span><span class="kw">gmwm</span>(<span class="kw">AR</span>(<span class="dv">2</span>), Zt, <span class="dt">robust =</span> <span class="ot">TRUE</span>)$estimate
}</code></pre></div>
<p>Having performed the estimation, we should now have 250 estimates for each AR(2) parameter and each estimation method. The code below takes the results of the simulation and shows them in the shape of boxplots along with the true values of the parameters. The estimation methods that are denoted as follows:</p>
<ul>
<li><strong>YW</strong>: Yule-Walker estimator</li>
<li><strong>MLE</strong>: Maximum Likelihood Estimator</li>
<li><strong>RGMWM</strong>: the robust version of the GMWM estimator</li>
</ul>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-7"></span>
<img src="ts_files/figure-html/unnamed-chunk-7-1.png" alt="Boxplots of the empirical distribution functions of the Yule-Walker (YW), MLE and GMWM estimators for the parameters of the AR(2) model when using the Xt process (first row of boxplots), Yt process (second row of boxplots) and Zt (third row of boxplots)." width="864" />
<p class="caption">
Figure 4.2: Boxplots of the empirical distribution functions of the Yule-Walker (YW), MLE and GMWM estimators for the parameters of the AR(2) model when using the Xt process (first row of boxplots), Yt process (second row of boxplots) and Zt (third row of boxplots).
</p>
</div>
<p>It can be seen how all methods appear to properly estimate the true parameter values on average when they are applied to the simulated time series from the uncontaminated process <span class="math inline">\((X_t)\)</span>. However, the MLE appears to be slightly more efficient (less variable) compared to the other methods and, in addition, the robust method (RGMWM) appears to be less efficient than the other two estimators. The latter is a known result since robust estimators usually pay a price in terms of efficiency (as an insurance against bias).</p>
<p>On the other hand, when checking the performance of the same methods when applied to the two contaminated processes <span class="math inline">\((Y_t)\)</span> and <span class="math inline">\((Z_t)\)</span> it can be seen that the standard estimators appear to be (highly) biased for most of the estimated parameters (with one exception) while the robust estimator remains close (on average) to the true parameter values that we are aiming to estimate. Therefore, when there’s a suspicion that there could be some (small) contamination in the observed time series, it may be more appropriate to use a robust estimator.</p>
<p>To conclude this section on estimation, we now compare the above studied estimators in different applied settings where we can highlight how to assess which estimator is more appropriate according to the type of setting. For this purpose, let us start with an example we have already checked in the previous chapter when discussing standard and robust estimators of the ACF, more specifically the data on monthly precipitations. As mentioned before when discussing this example, the importance of modelling precipitation data lies in the fact that its usually used to successively model the entire water cycle. Common models for this purpose are either the white noise (WN) model or the AR(1) model. Let us compare the standard and robust ACF again to understand which of these two models seems more appropriate for the data at hand.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">compare_acf</span>(hydro)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-8"></span>
<img src="ts_files/figure-html/unnamed-chunk-8-1.png" alt="Standard (left) and robust (left) estimates of the ACF function on the monthly precipitation data (hydro)" width="864" />
<p class="caption">
Figure 4.3: Standard (left) and robust (left) estimates of the ACF function on the monthly precipitation data (hydro)
</p>
</div>
<p>As we had underlined in the previous chapter, the standard ACF estimates would suggest that there appears to be no correlation among lags and consequently, the WN model would be the most appropriate. However, the robust ACF estimates depict an entirely different picture where it can be seen that there appears to be a significant autocorrelation over different lags which exponentially decay. Although there appears to be some seasonality in the plot, we will assume that the correct model for this data is an AR(1) since that’s what hydrology theory suggests. Let us therefore estimate the parameters of this model by using a standard estimator (MLE) and a robust estimator (RGMWM). The estimates for the MLE are the following:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mle_hydro =<span class="st"> </span><span class="kw">ar</span>(<span class="kw">as.vector</span>(hydro), <span class="dt">order.max =</span> <span class="dv">1</span>, <span class="dt">method =</span> <span class="st">&quot;mle&quot;</span>, <span class="dt">demean =</span> <span class="ot">TRUE</span>, <span class="dt">aic =</span> <span class="ot">FALSE</span>)

<span class="co"># MLE Estimates</span>
<span class="kw">c</span>(mle_hydro$ar, mle_hydro$var.pred)</code></pre></div>
<pre><code>##        ar1            
## 0.06497549 0.22205713</code></pre>
<p>From these estimates it would appear that the autocorrelation between lagged variables (i.e. lags of order 1) is extremely low and that (as suggested by the standard ACF plot) a WN model may be more appropriate. Considering the robust ACF however, it is possible that the MLE estimates may not be reliable in this setting. Hence, let us use the RGMWM to estimate the same parameters.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">rgmwm_hydro =<span class="st"> </span><span class="kw">gmwm</span>(<span class="kw">AR</span>(<span class="dv">1</span>), hydro, <span class="dt">robust =</span> <span class="ot">TRUE</span>)$estimate

<span class="co"># RGMWM Estimates</span>
<span class="kw">t</span>(rgmwm_hydro)</code></pre></div>
<pre><code>##                  AR    SIGMA2
## Estimates 0.4048702 0.1065875</code></pre>
<p>In this case, we see how the autocorrelation between lagged values is much higher (0.4 compared to 0.06) indicating that there is a stronger dependence in the data than what is suggested by standard estimators. Moreover, the innovation variance is smaller compared to that of the MLE. This is also a known phenomenon when there’s contamination in the data since it leads to less dependence and more variability being detected by non-robust estimators. This estimate of the variance also has a considerable impact on forecast precision (as we’ll see in the next section).</p>
<p>A final applied example that highlights the (potential) difference between estimators according to the type of setting is given by the “Recruitment” data set (in the <code>astsa</code> library). This data refers to the presence of new fish in the population of the Pacific Ocean and is often linked to the currents and temperatures passing through the ocean. As for the previous data set, let us take a look at the data itself and then analyse the standard and robust estimations of the ACF.</p>
<pre><code>## 
## Attaching package: &#39;astsa&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:simts&#39;:
## 
##     sales</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(astsa)

<span class="co"># Format data</span>
fish =<span class="st"> </span><span class="kw">gts</span>(rec, <span class="dt">start =</span> <span class="dv">1950</span>, <span class="dt">freq =</span> <span class="dv">12</span>, <span class="dt">unit_time =</span> <span class="st">&#39;month&#39;</span>, <span class="dt">name_ts =</span> <span class="st">&#39;Recruitment&#39;</span>)

<span class="co"># Plot data</span>
<span class="kw">plot</span>(fish)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-12"></span>
<img src="ts_files/figure-html/unnamed-chunk-12-1.png" alt="Plot of the time series on fish recruitment monthly data in the Pacific Ocean from 1950 to 1987" width="768" />
<p class="caption">
Figure 4.4: Plot of the time series on fish recruitment monthly data in the Pacific Ocean from 1950 to 1987
</p>
</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">compare_acf</span>(fish)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-13"></span>
<img src="ts_files/figure-html/unnamed-chunk-13-1.png" alt="Standard (left) and robust (left) estimates of the ACF function on the monthly fish recruitment data (rec)" width="864" />
<p class="caption">
Figure 4.5: Standard (left) and robust (left) estimates of the ACF function on the monthly fish recruitment data (rec)
</p>
</div>
<p>We can see that there appears to be a considerable dependence between the lagged variables which decays (in a similar way to the ACF of an AR(<span class="math inline">\(p\)</span>)). Also in this case we see a seasonality in the data but we won’t consider this for the purpose of this example. Given that there doesn’t appear to be any significant contamination in the data, let us consider the Yule-Walker and MLE estimators. The MLE highly depends on the assumed parametric distribution of the time series (i.e. usually Gaussian) and, if this is not respected, the resulting estimations could be unreliable. Hence, a first difference of the time series can often give an idea of the marginal distribution of the time series.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Take first differencing of the recruitment data</span>
diff_fish =<span class="st"> </span><span class="kw">gts</span>(<span class="kw">diff</span>(rec), <span class="dt">start =</span> <span class="dv">1950</span>, <span class="dt">freq =</span> <span class="dv">12</span>, <span class="dt">unit_time =</span> <span class="st">&#39;month&#39;</span>, <span class="dt">name_ts =</span> <span class="st">&#39;Recruitment&#39;</span>)

<span class="co"># Plot first differencing of the recruitment data</span>
<span class="kw">plot</span>(diff_fish)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-15"></span>
<img src="ts_files/figure-html/unnamed-chunk-15-1.png" alt="Plot of the first difference of the time series on fish recruitment monthly data in the Pacific Ocean from 1950 to 1987" width="768" />
<p class="caption">
Figure 4.6: Plot of the first difference of the time series on fish recruitment monthly data in the Pacific Ocean from 1950 to 1987
</p>
</div>
<p>From the plot we can see that observations appear to be collected around a constant value and fewer appear to be further from this value (as would be the case for a normal distribution). However, various of these “more extreme” observations appear to be quite frequent suggesting that the underlying distribution may have a heavier tail compared to the normal distribution. Let us compare the standard and robust ACF estimates of this new time series.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">compare_acf</span>(diff_fish)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-16"></span>
<img src="ts_files/figure-html/unnamed-chunk-16-1.png" alt="Standard (left) and robust (left) estimates of the ACF function on the first difference of the monthly fish recruitment data (rec)" width="864" />
<p class="caption">
Figure 4.7: Standard (left) and robust (left) estimates of the ACF function on the first difference of the monthly fish recruitment data (rec)
</p>
</div>
<p>In this case we see that the patterns appear to be the same but the values between the standard and robust estimates are slightly (to moderately) different over different lags. This would suggest that there could be some contamination in the data or, in any case, that the normal assumption may not hold exactly. With this in mind, let us estimate an AR(2) model for this data using the Yule-Walker and MLE.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># MLE of Recruitment data</span>
yw_fish =<span class="st"> </span><span class="kw">ar</span>(rec, <span class="dt">order.max =</span> <span class="dv">2</span>, <span class="dt">method =</span> <span class="st">&quot;yule-walker&quot;</span>, <span class="dt">demean =</span> <span class="ot">TRUE</span>, <span class="dt">aic =</span> <span class="ot">FALSE</span>)

<span class="co"># MLE of Recruitment data</span>
mle_fish =<span class="st"> </span><span class="kw">ar</span>(rec, <span class="dt">order.max =</span> <span class="dv">2</span>, <span class="dt">method =</span> <span class="st">&quot;mle&quot;</span>, <span class="dt">demean =</span> <span class="ot">TRUE</span>, <span class="dt">aic =</span> <span class="ot">FALSE</span>)

<span class="co"># Compare estimates</span>
<span class="co"># Yule-Walker Estimation</span>
<span class="kw">c</span>(yw_fish$ar, yw_fish$var.pred)</code></pre></div>
<pre><code>## [1]  1.3315874 -0.4445447 94.7991188</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># MLE Estimation</span>
<span class="kw">c</span>(mle_fish$ar, mle_fish$var.pred)</code></pre></div>
<pre><code>##        ar1        ar2            
##  1.3512809 -0.4612736 89.3359654</code></pre>
<p>It can be seen that, in this setting, the two estimators deliver very similar results (at least in terms of the <span class="math inline">\(\phi_1\)</span> and <span class="math inline">\(\phi_2\)</span> coefficients). Indeed, there doesn’t appear to be a strong need for robust estimation and the choice of a standard estimator is justified by the will to obtain efficient estimations. The only slight difference between the two estimations is in the innovation variance parameter <span class="math inline">\(\sigma^2\)</span> and this could be (evenutally) due to the normality assumption that the MLE estimator upholds in this case. If there is therefore a doubt on the fact that the Gaussian assumption does not hold for this data, then it is probably more convenient to use the Yule-Walker estimates.</p>
<p>Until now we have focussed on estimation based on an assumed model. However, how do we choose a model? How can we make inference on the models and their parameters? To perform all these tasks we will need to compute residuals (as, for example, in the linear regression framework). In order to obtain residuals, we need to be able to predict (forecast) values of the time series and, consequently, the next section focuses on forecasting time series.</p>
</div>
<div id="forecasting-arp-models" class="section level3">
<h3><span class="header-section-number">4.2.3</span> Forecasting AR(p) Models</h3>
<p>One of the most interesting aspects of time series analysis is to predict the future unobserved values based on the values that have been observed up to now. However, this is not possible if the underlying (parametric) model is unknown, thus in this section we assume the time series <span class="math inline">\((X_t)\)</span> is <strong>stationary</strong> and its model is known. In particular, we denote forecasts by <span class="math inline">\(X^{T}_{T+m}\)</span>, where <span class="math inline">\(T\)</span> represents the length of the <em>observed</em> time series (e.g. <span class="math inline">\(\mathbf{X} = (X_{1}, X_{2}, \cdots , X_{T-1}, X_T)\)</span>) and <span class="math inline">\(m\)</span> represents the <span class="math inline">\(m^{th}\)</span>-ahead future value we wish to predict. So, <span class="math inline">\(X^{T}_{T+1}\)</span> represents a one-step-ahead prediction of <span class="math inline">\(X_{T+1}\)</span> given data <span class="math inline">\((X_{1}, X_{2}, \cdots, X_{T-1}, X_{T})\)</span>.</p>
<p>Let us now focus on defining a prediction operator and, for this purpose, let us define the Mean Squared Prediction Error (MSPE) as follows:</p>
<p><span class="math display">\[\mathbb{E}[(X_{t+j} - X^{t}_{t+j})^2] .\]</span> Intuitively, the MPSE measures the square distance (i.e. always positive) between the actual future values and the corresponding predictions. Ideally, we would want this measure to be equal to zero (meaning that we don’t make any prediction errors) but, if this is not possible, we would like to define a predictor that has the smallest MPSE among all possible predictors. The next theorem states what the best predictor is for this measure.</p>

<div class="theorem">
<p><span id="thm:condexp" class="theorem"><strong>Theorem 4.1  (Minimum Mean Squared Error Predictor)  </strong></span>Let us define <span class="math display">\[X_{t + j}^t = E\left[ {{X_{t + j}}|{X_t}, \cdots ,{X_1}} \right]  \equiv {E_t}\left[ {{X_{t + j}}} \right],j &gt; 0\]</span></p>
Then <span class="math display">\[E\left[ {{{\left( {{X_{t + j}} - m\left( {{X_1}, \cdots ,{X_t}} \right)} \right)}^2}} \right] \ge E\left[ {{{\left( {{X_{t + j}} - X_{t + j}^t} \right)}^2}} \right]\]</span> for any function <span class="math inline">\(m(.)\)</span>.
</div>
<p></p>
<p>The proof of this theorem can be found in Appendix @ref(#appendixc). Although this theorem defines the best possible predictor, there can be many functional forms for this operator. We first restrict our attention to the set of linear predictors defined as</p>
<p><span class="math display">\[X_{t+j}^t = \sum_{i=1}^t \alpha_i X_i\]</span> where <span class="math inline">\(\alpha_i \in \mathbb{R}\)</span>. It can be noticed, for example, that the <span class="math inline">\(\alpha_i\)</span>’s are not always the same based on the values of <span class="math inline">\(t\)</span> and <span class="math inline">\(j\)</span> (i.e. it depends from which time point you want to predict and how far into the future). Another aspect to notice is that, if the time series model underlying the observed time series can be expressed in the form of a linear operator (e.g. a linear process), then we can derive a linear predictor from this framework.</p>
<p>Considering the above, let us now give the following theorem which provides further insight into linear prediction.</p>

<div class="theorem">
<p><span id="thm:projtheo" class="theorem"><strong>Theorem 4.2  (Projection Theorem)  </strong></span>Let <span class="math inline">\(\mathcal{M} \subset \mathcal{L}_2\)</span> be a closed linear subspace of a Hibert space. For every <span class="math inline">\(y \in \mathcal{L}_2\)</span>, there exists a unique element <span class="math inline">\(\hat{y} \in \mathcal{M}\)</span> that minimizes <span class="math inline">\(||y - z||^2\)</span> over <span class="math inline">\(z \in \mathcal{M}\)</span>. This element is uniquely determined by the requirements</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\hat{y} \in \mathcal{M}\)</span> and</li>
<li><span class="math inline">\((y - \hat{y}) \perp \mathcal{M}\)</span>.
</div>
</li>
</ol>
<p>Based on this theorem, it is possible to define the Best Linear Predictor (BLP) for stationary processes.</p>

<div class="definition">
<span id="def:BLP" class="definition"><strong>Definition 4.3  </strong></span>The best linear predictor <span class="math inline">\(X_{t+j}^t = \sum_{i=1}^t \alpha_i X_i\)</span>, for <span class="math inline">\(j \geq 1\)</span> is found by solving <span class="math display">\[ \mathbb{E} [(X_{t+h} - \hat{X}_{t+h})X_i ] = 0, \mbox{ for } i = 1, \dots, t.\]</span>
</div>
<p></p>
<p>If we denote <span class="math inline">\(\mathbb{E}(X_{i}, X_{j})\)</span> as <span class="math inline">\(\gamma(|i - j|)\)</span>, these prediction equations can alternatively be represented in the following form:</p>
<span class="math display">\[\begin{equation}
\begin{aligned}
\begin{pmatrix}
\gamma(0) &amp; \gamma(1) &amp; \cdots &amp; \gamma(T-1) \\
\gamma(1) &amp; \gamma(0) &amp; \cdots &amp; \gamma(T-2) \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\gamma(T-1) &amp; \gamma(T-2) &amp; \cdots &amp;\gamma(0)
\end{pmatrix}_{T \times T}
\begin{pmatrix}
\alpha_1 \\
\vdots \\
\alpha_T
\end{pmatrix}_{T \times 1}
&amp;=
  \begin{pmatrix}
\gamma(1)  \\
\vdots \\
\gamma(T)
\end{pmatrix}_{T \times 1} ,

\end{aligned}
\end{equation}\]</span>
<p>which can be written in a compact matrix form as <span class="math inline">\(\Gamma_T \mathbf{\alpha}_T = \mathbf{\gamma}_T\)</span>. Assuming that <span class="math inline">\(\Gamma_T\)</span> is non-singular, then the values of <span class="math inline">\(\mathbf{\alpha}_T\)</span> are given by:</p>
<p><span class="math display">\[\mathbf{\alpha}_T  = \Gamma^{-1}_T\mathbf{\gamma}_T.\]</span></p>
<p>which delivers the linear predictor</p>
<p><span class="math display">\[X_{t+j}^t = \alpha_T^T \mathbf{X}^* .\]</span> where <span class="math inline">\(\mathbf{X}^* = (X_{T}, X_{T-1}, \cdots , X_{2}, X_1)\)</span>. We can now examine these predictors for the AR(<span class="math inline">\(p\)</span>) models that we’ve studied this far (starting from the AR(1)).</p>

<div class="example">
<p><span id="exm:ar1forecast" class="example"><strong>Example 4.8  (Forecasting with an AR(1) model)  </strong></span>We begin the forecasting examples by considering an AR(1) process.</p>
<p><span class="math display">\[{X_t} = \phi {X_{t - 1}} + {W_t}\]</span></p>
<p>where <span class="math inline">\(W_t \sim WN(0, \sigma^2)\)</span>.</p>
<p>From here, the conditional expected mean and variance are given by:</p>
<p><span class="math display">\[\begin{aligned}
  {E_t}\left[ {{X_{t + j}}} \right] &amp;= {\phi ^j}{X_t} \\
  Va{r_t}\left[ {{X_{t + j}}} \right] &amp;= \left( {1 + {\phi ^2} + {\phi ^4} +  \cdots  + {\phi ^{2\left( {j - 1} \right)}}} \right){\sigma ^2} \\ 
\end{aligned} \]</span></p>
<p>Within this derivation, it is important to remember that:</p>
<p><span class="math display">\[\begin{aligned}
  \mathop {\lim }\limits_{j \to \infty } {E_t}\left[ {{X_{t + j}}} \right] &amp;= 0 = E\left[ {{X_t}} \right] \\
  \mathop {\lim }\limits_{j \to \infty } Va{r_t}\left[ {{X_{t + j}}} \right] &amp;= \frac{{{\sigma ^2}}}{{1 - {\phi ^2}}} = \operatorname{var} \left( {{X_t}} \right)  
\end{aligned} \]</span></p>
</div>
<p></p>

<div class="example">
<p><span id="exm:ar2forecast" class="example"><strong>Example 4.9  (Forecasting with an AR(2) model)  </strong></span>Consider an AR(2) process defined as follows:</p>
<p><span class="math display">\[{X_t} = {\phi _1}{X_{t - 1}} + {\phi _2}{X_{t - 2}} + {W_t}\]</span></p>
<p>where <span class="math inline">\(W_t \sim WN(0, \sigma^2)\)</span>.</p>
<p>Based on this process we are able to find the BLP for each m-step ahead prediction using the following approach:</p>
<p><span class="math display">\[\begin{aligned}
  {E_t}\left[ {{X_{t + 1}}} \right] &amp;= {\phi _1}{X_t} + {\phi _2}{X_{t - 1}} \\
  {E_t}\left[ {{X_{t + 2}}} \right] &amp;= {E_t}\left[ {{\phi _1}{X_{t + 1}} + {\phi _2}{X_t}} \right] = {\phi _1}{E_t}\left[ {{X_{t + 1}}} \right] + {\phi _2}{X_t} \\
   &amp;= {\phi _1}\left( {{\phi _1}{X_t} + {\phi _2}{X_{t - 1}}} \right) + {\phi _2}{X_t} \\
   &amp;= \left( {\phi _1^2 + {\phi _2}} \right){X_t} + {\phi _1}{\phi _2}{X_{t - 1}} \\ 
\end{aligned} \]</span></p>
</div>
<p></p>

<div class="example">
<p><span id="exm:arpforecast" class="example"><strong>Example 4.10  (Forecasting with an AR(p) model)  </strong></span>Consider AR(p) process given as:</p>
<p><span class="math display">\[{X_t} = {\phi _1}{X_{t - 1}} + {\phi _2}{X_{t - 2}} + \cdots + {\phi _p}{X_{t - p}} + {W_t}\]</span></p>
<p>where <span class="math inline">\(W_t \sim WN(0, \sigma^2_W)\)</span>.</p>
<p>The process can be rearranged into matrix form as follows:</p>
<p><span class="math display">\[\begin{aligned}
  \underbrace {\left[ {\begin{array}{*{20}{c}}
  {{X_t}} \\ 
   \vdots  \\ 
   \vdots  \\ 
  {{X_{t - p + 1}}} 
\end{array}} \right]}_{{Y_t}} &amp;= \underbrace {\left[ {\begin{array}{*{20}{c}}
  {{\phi _1}}&amp; \cdots &amp;{}&amp;{{\phi _p}} \\ 
  {}&amp;{}&amp;{}&amp;0 \\ 
  {}&amp;{{I_{p - 1}}}&amp;{}&amp; \vdots  \\ 
  {}&amp;{}&amp;{}&amp;0 
\end{array}} \right]}_A\underbrace {\left[ {\begin{array}{*{20}{c}}
  {{X_{t - 1}}} \\ 
   \vdots  \\ 
   \vdots  \\ 
  {{X_{t - p}}} 
\end{array}} \right]}_{{Y_{t - 1}}} + \underbrace {\left[ {\begin{array}{*{20}{c}}
  1 \\ 
  0 \\ 
   \vdots  \\ 
  0 
\end{array}} \right]}_C{W_t} \\
  {Y_t} &amp;= A{Y_{t - 1}} + C{W_t} 
\end{aligned}\]</span></p>
<p>From here, the conditional expectation and variance can be computed as follows:</p>
<p><span class="math display">\[\begin{aligned}
  {E_t}\left[ {{Y_{t + j}}} \right] &amp;= {E_t}\left[ {A{Y_{t + j - 1}} + C{W_{t + j}}} \right] = {E_t}\left[ {A{Y_{t + j - 1}}} \right] + \underbrace {{E_t}\left[ {C{W_{t + j}}} \right]}_{ = 0} \\
   &amp;= {E_t}\left[ {A\left( {A{Y_{t + j - 2}} + C{W_{t + j - 1}}} \right)} \right] = {E_t}\left[ {{A^2}{Y_{t + j - 2}}} \right] = {A^j}{Y_t} \\ 
  {\operatorname{var} _t}\left( {{Y_{t + j}}} \right) &amp;= {\operatorname{var} _t}\left( {A{Y_{t + j - 1}} + C{W_{t + j}}} \right) \\
   &amp;= {\sigma ^2}C{C^T} + {\operatorname{var} _t}\left( {A{Y_{t + j - 1}}} \right) = {\sigma ^2}A{\operatorname{var} _t}\left( {{Y_{t + j - 1}}} \right){A^T} \\
   &amp;= {\sigma ^2}C{C^T} + {\sigma ^2}AC{C^T}A + {\sigma ^2}{A^2}{\operatorname{var} _t}\left( {{Y_{t + j - 2}}} \right){\left( {{A^2}} \right)^T} \\
   &amp;= {\sigma ^2}\sum\limits_{k = 0}^{j - 1} {{A^k}C{C^T}{{\left( {{A^K}} \right)}^T}}  \\ 
\end{aligned} \]</span></p>
Considering the recursive pattern coming from the experssions of the conditional expectation and variance, the predictions can be obtained via the following recursive formulation: <span class="math display">\[\begin{aligned}
  {E_t}\left[ {{Y_{t + j}}} \right] &amp;= A{E_t}\left[ {{Y_{t + j - 1}}} \right] \\
  {\operatorname{var} _t}\left[ {{Y_{t + j}}} \right] &amp;= {\sigma ^2}C{C^T} + A{\operatorname{var} _t}\left( {{Y_{t + j - 1}}} \right){A^T} \\ 
\end{aligned} \]</span>
</div>
<p></p>

<div class="example">
<p><span id="exm:rear2forecast" class="example"><strong>Example 4.11  (Forecasting with an AR(2) in Matrix Form)  </strong></span>Using the recursive matrix form defined in the previous example, we can revisit our previous example of the predictions for an AR(2) process as follows:</p>
<p><span class="math display">\[\begin{aligned} \underbrace {\left[ {\begin{array}{*{20}{c}}
  {{X_t}} \\ 
  {{X_{t - 1}}} 
\end{array}} \right]}_{{Y_t}} &amp;= \underbrace {\left[ {\begin{array}{*{20}{c}}
  {{\phi _1}}&amp;{{\phi _2}} \\ 
  1&amp;0 
\end{array}} \right]}_A\underbrace {\left[ {\begin{array}{*{20}{c}}
  {{X_{t - 1}}} \\ 
  {{X_{t - 2}}} 
\end{array}} \right]}_{{Y_{t - 1}}} + \underbrace {\left[ {\begin{array}{*{20}{c}}
  1 \\ 
  0 
\end{array}} \right]}_C{W_t} \\
  {Y_t} &amp;= A{Y_{t - 1}} + C{W_t} 
\end{aligned}\]</span></p>
<p>Then, we are able to calculate the BLP as:</p>
<span class="math display">\[\begin{aligned}
  {E_t}\left[ {{Y_{t + 2}}} \right] &amp;= {A^2}{Y_t} = \left[ {\begin{array}{*{20}{c}}
  {{\phi _1}}&amp;{{\phi _2}} \\ 
  1&amp;0 
\end{array}} \right]\left[ {\begin{array}{*{20}{c}}
  {{\phi _1}}&amp;{{\phi _2}} \\ 
  1&amp;0 
\end{array}} \right]\left[ {\begin{array}{*{20}{c}}
  {{X_t}} \\ 
  {{X_{t - 1}}} 
\end{array}} \right] \hfill \\
   &amp;= \left[ {\begin{array}{*{20}{c}}
  {\phi _1^2 + {\phi _2}}&amp;{{\phi _1}{\phi _2}} \\ 
  {{\phi _1}}&amp;{{\phi _2}} 
\end{array}} \right]\left[ {\begin{array}{*{20}{c}}
  {{X_t}} \\ 
  {{X_{t - 1}}} 
\end{array}} \right] = \left[ {\begin{array}{*{20}{c}}
  {\left( {\phi _1^2 + {\phi _2}} \right){X_t} + {\phi _1}{\phi _2}{X_{t - 1}}} \\ 
  {{\phi _1}{X_t} + {\phi _2}{X_{t - 1}}} 
\end{array}} \right] \hfill \\ 
\end{aligned} \]</span>
</div>
<p></p>
<p>The above examples have given insight as to how to compute predictors for the AR(p) models that we have studied this far. Let us now observe the consequences of using such an approach to predict future values from these models. For this purpose, using the recursive formulation seen in the examples above we perform a simulation study where, for a fixed set of parameters, we make 5000 predictions from an observed time series and predict 50 values ahead into the future. The known parameters for the AR(2) process we use for the simulation study are <span class="math inline">\(\phi _1 = 0.75\)</span>, <span class="math inline">\(\phi _2 = 0.2\)</span> and <span class="math inline">\(\sigma^2 = 1\)</span>. The figure below shows the distribution of these predictions starting from the last observation <span class="math inline">\(T = 200\)</span>.</p>
<div class="figure"><span id="fig:predplot"></span>
<img src="ts_files/figure-html/predplot-1.png" alt="Values of the AR(2) predictions with the pink line being the median prediction, red and green lines are respectively 95% and 75% Confidence intervals." width="672" />
<p class="caption">
Figure 4.8: Values of the AR(2) predictions with the pink line being the median prediction, red and green lines are respectively 95% and 75% Confidence intervals.
</p>
</div>
<p>It can be observed that, as hinted by the expressions for the variance of the predictions (in the examples above), the variability of the predictions increases as we try to predict further into the future. With this in mind, let is now define some empirical measures that can assess how good our predictions are.</p>
<!-- #### Measuring Forecast Accuracy -->
<!-- Within time series, it is often difficult to apply traditional methodologies -->
<!-- relating to the concepts of "training" and "test" data set. The reason why these concepts are of difficult application in time series is due to the temporal dependence of the data. For instance, if we randomly sampled points within the time series, then there would be some missing values that would need to be imputed. Therefore, to obtain -->
<!-- an out-of-sample test, the only viable option would be to obtain a window period, -->
<!-- say $t = 1, \cdots , T − 1$, to predict $t = T$. That is to say, the training  -->
<!-- sample runs from the beginning up until "yesterday" and then the "hold-out" data is  -->
<!-- consequently the observed value of "today".  -->
<!-- Thus, time series models are validated based on "rolling forecasts". However, depending on the context, they may also be referred to as "walk forward optimization", "rolling horizon" or simply "moving origin". The traditional rule of thumb in terms of training data and test data sizes is to respectively pick $\frac{2}{3}$ and $\frac{1}{3}$ of the length of the observed time series. -->
<!-- There are different forecast measures that should be used to measure the strength -->
<!-- of the window and, subsequently, the model. There are two prevalent measures -->
<!-- that will be emphasized within this text: Median Prediction Error (MAPE) and -->
<!-- Mean-squared Prediction Error (MSPE). Both are defined as follows: -->
<!-- \begin{align*} -->
<!-- MAPE &= \mathop {median}\limits_{t = 1, \cdots ,t - 1} \left| {{{\hat E}_t}\left[ {{X_{t + 1}}} \right] - {X_{t + 1}}} \right| = \mathop {median}\limits_{t = 1, \cdots ,t - 1} \left| {{{\hat E}_t}\left[ {r_i}\right]} \right| \\ -->
<!-- MSPE &= \frac{1}{{n - m}}\sum\limits_{t = m,n}^{n - 1} {{{\left( {{{\hat E}_t}\left[ {{X_{t + 1}}} \right] - {X_{t + 1}}} \right)}^2}}  = \frac{1}{{n - m}}\sum\limits_{t = m,n}^{n - 1} {{r_i}^2} -->
<!-- \end{align*} -->
<!-- ```{proposition, name="Rolling Forecasting Origin"} -->
<!-- The rolling forecast origin algorithm can be described as follows: -->
<!-- 1. Divide the data into a "training data" set that ranges from $t = 1, \ldots, m$ where $m$ is obtained by $m=\left \lfloor \frac{2N}{3} \right \rfloor$ and a testing data set $t = m+1, \ldots, N-1$ -->
<!-- 2. Compute $\hat{\theta}$ on $X_t$ where $t = 1, \ldots, m$ -->
<!-- 3. Using $\hat{\theta}$, compute ${\hat E_m}\left[ {{X_{m + 1}}} \right] = {\hat \phi _1}{X_m} +  \cdots  + {\hat \phi _p}{X_{m + 1 - p}}$. -->
<!-- 4. ${r_i} = {\hat E_m}\left[ {{X_{m + 1}}} \right] - {X_{m + 1}}$ -->
<!-- 5. $i = i + 1$, $m = m + 1$, go to Step 2. until $m = N - 1$ then go to 6. -->
<!-- 6. Compute desired forcast measure  -->
<!-- ``` -->
<!-- Given all of the above discussion, there is one caveat that would enable data -->
<!-- to be separate completely. The caveat is based upon the ability to take  -->
<!-- distinct temporal sets such as year 1, year 2, and so on to fit individual models -->
<!-- to each time period. The stability of parameter estimates could then be -->
<!-- compared across years. -->
<p>The whole discussion on prediction is not only important to derive forecasts for future values of the phenomena one may be interested in, but also in understanding how well a model explains (and predicts) an observed time series. Indeed, predictions allow to deliver residuals within the time series setting and, based on these residuals, we can obtain different inference and diagnostic tools.</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="fundtimeseries.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="references.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/SMAC-Group/ts/edit/master/03-arma.Rmd",
"text": "Edit"
},
"download": ["ts.pdf", "ts.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
