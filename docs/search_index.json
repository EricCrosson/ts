[
["under-development.html", "Chapter 5 Under development 5.1 Moving Average Models ⚠️", " Chapter 5 Under development 5.1 Moving Average Models ⚠️ The class of AR(\\(p\\)) models is a very general class that allows to take into account different forms of linear dependence between past and future observations. However, there are some forms of linear dependence that can appear as “shocks” in the innovation noise of a time series. In this sense, we have already seen a model that describes a certain form of this dependence which is the MA(1) defined as \\[X_t = \\theta W_{t-1} + W_t\\] where \\((W_t)\\) is a white noise process. As can be seen, the observed time series \\((X_t)\\) is a linear combination of the innovation process. In this section we generalise this process to the class of MA(\\(q\\)) processes that are defined as follows. Definition 5.1 (Moving Average of Order Q) A Moving Average of Order \\(q\\) or MA(\\(q\\)) model is defined as follows: \\[{X_t} = \\theta_1 W_{t-1} + ... + \\theta_q W_{t-q} + W_t,\\] where \\(\\theta_q \\neq 0\\). If we make use of the backshift operator defined earlier in this chapter, we can rewrite this model as: \\[\\begin{aligned} X_t &amp;= \\theta_1 B W_t + ... + \\theta_q B^q W_t + W_t \\\\ &amp;= (\\theta_1 B + ... + \\theta_q B^q + 1) W_t .\\\\ \\end{aligned} \\] Based on this we can deliver the following definition. Definition 5.2 (Moving Average Operator) The moving average operator is defined as \\[\\theta(B) \\equiv 1 + \\theta_1 B + ... + \\theta_q B^q.\\] This allows us to write an MA(\\(q\\)) process as \\[X_t = \\theta (B) W_t .\\] Following this definition, it is possible to see that an MA(\\(q\\)) process is always stationary. Indeed, an MA(\\(q\\)) respects the defintion of a linear process (see Definition ??) where \\(\\psi_0 = 1\\), \\(\\psi_j = \\theta_j\\) for \\(j = 1, ..., q\\), and \\(\\psi_j = 0\\) for \\(j &gt; q\\) and, based on this, we can show its stationarity. Example 5.1 (Stationarity of an MA(q) Process) Based on the above definitions, we can rewrite an MA(q) process as \\(X_t = \\sum\\limits_{i = 0}^q {{\\theta _i}{W_{t - i}}}\\), where \\(\\theta_0 = 1\\) and assume the condition that \\(\\sum\\limits_{j = 0}^q {\\theta _j^2} &lt; \\infty\\) (if \\(\\theta_j &lt; \\infty\\) for all \\(j\\) and \\(q &lt; \\infty\\), this condition is verified). Using this expression we start verifying (weak) stationarity by checking the expectation of an MA(q) process which is given by \\[\\mathbb{E}\\left[ X_t \\right] = \\mathbb{E}\\left[ \\sum\\limits_{i = 0}^q {{\\theta _i}{W_{t - i}}} \\right] = \\sum\\limits_{i = 0}^q {{\\theta _i}} \\mathbb{E}\\left[ {W_{t - i}} \\right] = 0,\\] which confirms that the expectation is constant. We now have to verify the conditions on the autocovariance function which is derived as follows: \\[\\begin{align} \\text{cov}\\left( {{X_{t + h}},{X_t}} \\right) &amp;= \\text{cov} \\left( {\\sum\\limits_{j = 0}^q {{\\theta _j}{W_{t + h - j}}} ,\\sum\\limits_{i = 0}^q {{\\theta _i}{W_{t - i}}} } \\right) \\\\ &amp;= \\sum\\limits_{j = 0}^q {\\sum\\limits_{i = 0}^q {{\\theta _j}{\\theta _i} \\;\\text{cov} \\left( {{W_{t + h - j}},{W_{t - i}}} \\right)} } \\\\ &amp;= \\sum\\limits_{j = 0}^q {\\sum\\limits_{i = 0}^q {{\\theta _j}{\\theta _i}\\; \\underbrace {\\text{cov} \\left( {{W_{t + h - j}},{W_{t - i}}} \\right)}_{ = 0 \\, \\text{for} \\, i = j - h}} } + {1_{\\left\\{ {\\left| h \\right| \\leqslant q} \\right\\}}}\\sum\\limits_{j = 0}^{q - \\left| h \\right|} {{\\theta _{j + \\left| h \\right|}}{\\theta _j} \\;\\underbrace{\\text{cov} \\left( {{W_t},{W_t}} \\right)}_{= \\sigma^2}} \\\\ &amp;= {1_{\\left\\{ {\\left| h \\right| \\leqslant q} \\right\\}}}{\\sigma ^2}\\sum\\limits_{j = 0}^{q - \\left| h \\right|} {{\\theta _{j + \\left| h \\right|}}{\\theta _j}} \\end{align}\\] As a result, we have: \\[{1_{\\left\\{ {\\left| h \\right| \\leqslant q} \\right\\}}}{\\sigma ^2}\\sum\\limits_{j = 0}^{q - \\left| h \\right|} {{\\theta _{j + \\left| h \\right|}}{\\theta _j}} \\leqslant {\\sigma ^2}\\sum\\limits_{j = 0}^q {\\theta _j^2} &lt; \\infty. \\] Given these results, we also have \\[\\text{var}(X_t) = {\\sigma ^2}\\sum\\limits_{j = 0}^q {\\theta _j^2},\\] which (under our assumption) is finite and does not depend on time. Moreover, the covariance only depends on the lag \\(h\\) (not on the time \\(t\\)) and, therefore, an MA(\\(q\\)) process is weakly stationary. Although an MA(\\(q\\)) process is always weakly stationary, it is important to well define these models since they can be characterized by certain parametrizations that don’t allow them to be uniquely identified. Indeed, the latter issues can be referred to as identifiability in which different MA(\\(q\\)) models (of the same order \\(q\\)) can produce identical autocovariance functions. Example 5.2 (Non-uniqueness of MA models) For example, consider the following two MA(1) processes: \\[ \\begin{aligned} X_t &amp;= \\frac{1}{\\theta}W_{t-1} + W_t,&amp;{}&amp; W_t \\overset{iid}{\\sim} \\mathcal{N} (0, \\sigma^2\\theta^2), \\\\ Y_t &amp;= \\theta Z_{t-1} + Z_t,&amp;{}&amp; Z_t \\overset{iid}{\\sim} \\mathcal{N} (0,\\sigma^2). \\end{aligned} \\] By observation, one can note that the models share the same expectation: \\[\\mathbb{E}\\left[ {{X_t}} \\right] = \\mathbb{E}\\left[ {{Y_t}} \\right] = 0.\\] However, for the autocovariance, the process requires a bit more effort, \\[\\begin{align} \\text{cov} \\left( {{X_t},{X_{t + h}}} \\right) &amp;= \\text{cov} \\left( {{W_t} + \\frac{1}{\\theta }{W_{t - 1}},{W_{t + h}} + \\frac{1}{\\theta }{W_{t + h - 1}}} \\right)\\\\ &amp;= {\\boldsymbol{1}_{\\left\\{ {h = 0} \\right\\}}}{\\sigma^2}{\\theta ^2} + {\\boldsymbol{1}_{\\left\\{ {\\left| h \\right| = 1} \\right\\}}}\\frac{{{\\sigma ^2}{\\theta ^2}}}{\\theta }\\\\ &amp;= {\\sigma ^2}\\left( {{\\boldsymbol{1}_{\\left\\{ {h = 0} \\right\\}}}{\\theta^2} + {\\boldsymbol{1}_{\\left\\{ {\\left| h \\right| = 1} \\right\\}}}\\theta } \\right) \\\\[12pt] \\text{cov} \\left( {{Y_t},{Y_{t + h}}} \\right) &amp;= \\text{cov} \\left( {{Z_t} + \\theta {Z_{t - 1}},{Z_{t + h}} + \\theta {Z_{t + h - 1}}} \\right)\\\\ &amp;= {\\boldsymbol{1}_{\\left\\{ {h = 0} \\right\\}}}{\\sigma ^2}{\\theta ^2} + {\\boldsymbol{1}_{\\left\\{ {\\left| h \\right| = 1} \\right\\}}}{\\sigma ^2}\\theta\\\\ &amp;= {\\sigma ^2}\\left( {{\\boldsymbol{1}_{\\left\\{ {h = 0} \\right\\}}}{\\theta ^2} + {1_{\\left\\{ {\\left| h \\right| = 1} \\right\\}}}\\theta } \\right). \\end{align}\\] From this we can see that \\(\\text{cov} \\left( {{X_t},{X_{t + h}}} \\right) = \\text{cov} \\left( {{Y_t},{Y_{t + h}}} \\right)\\), for all \\(h \\in \\mathbb{Z}\\). Using this result and since the innovations of the two processes are Gaussian we have: \\[\\mathbf{X}_T \\sim \\mathcal{N} \\left(\\mathbf{0}, \\boldsymbol{\\Sigma}\\right), \\;\\;\\; \\text{and} \\;\\;\\; \\mathbf{Y}_T \\sim \\mathcal{N} \\left(\\mathbf{0}, \\boldsymbol{\\Sigma}\\right)\\] where \\[\\mathbf{X}_T \\equiv \\left[ {\\begin{array}{*{20}{c}} {{X_1}} \\\\ \\vdots \\\\ {{X_T}} \\end{array}} \\right],\\mathbf{Y}_T \\equiv \\left[ {\\begin{array}{*{20}{c}} {{Y_1}} \\\\ \\vdots \\\\ {{Y_T}} \\end{array}} \\right],\\] and where \\[\\boldsymbol{\\Sigma} \\equiv {\\sigma ^2}\\left[ {\\begin{array}{*{20}{c}} {\\left( {1 + {\\theta ^2}} \\right)}&amp;\\theta &amp;0&amp; \\cdots &amp;0 \\\\ \\theta &amp;{\\left( {1 + {\\theta ^2}} \\right)}&amp;\\theta &amp;{}&amp; \\vdots \\\\ 0&amp;\\theta &amp;{\\left( {1 + {\\theta ^2}} \\right)}&amp;{}&amp;{} \\\\ \\vdots &amp;{}&amp;{}&amp; \\ddots &amp;{} \\\\ 0&amp; \\cdots &amp;{}&amp;{}&amp;{\\left( {1 + {\\theta ^2}} \\right)} \\end{array}} \\right].\\] Therefore the two process are completely indistinguishable as they have the same distribution (as highlighted in Chapter 2). In Statistics, they are said to be non-identifiable. Naturally, the identifiability issues discussed in Example 5.2 is of considerable importance to tackle especially when attempting to estimate the parameters of an MA(\\(q\\)) model. For the purpose of illustration, let us again consider the same setting with an MA(1) process in the example below. Example 5.3 (Non-injectivity of the Likelihood function for MA models) Using the same setting as in Example 5.2, we will verify that likelihood is not injective (which is rather trivial). Indeed, \\(\\mathbf{X}_T\\) follows a multivariate normal distrbution with mean \\(\\mathbf{0}\\) and covariance matrix $ and therefore likelihood function for an MA(1) is the following: \\[L\\left( {\\boldsymbol{\\beta} |\\mathbf{X}_T} \\right) = {\\left( {2\\pi } \\right)^{ - \\frac{T}{2}}}{\\left| \\boldsymbol{\\Sigma}(\\boldsymbol{\\beta}) \\right|^{ - \\frac{1}{2}}}\\exp \\left( { - \\frac{1}{2}{\\mathbf{X}_T^T}{\\boldsymbol{\\Sigma}(\\boldsymbol{\\beta})^{ - 1}}\\mathbf{X}_T} \\right),\\] where \\(\\boldsymbol{\\beta} \\equiv [\\theta \\;\\;\\; \\sigma^2]^T\\). Then, it is easy to see that if we take \\[{\\boldsymbol{\\beta}_1} = \\left[ {\\begin{array}{*{20}{c}} \\theta \\\\ {{\\sigma ^2}} \\end{array}} \\right]\\;\\;\\; \\text{and}\\;\\;\\;{\\boldsymbol{\\beta}_2} = \\left[ {\\begin{array}{*{20}{c}} {\\frac{1}{\\theta }} \\\\ {{\\sigma ^2}\\theta } \\end{array}} \\right],\\] we will have \\[L\\left( {\\boldsymbol{\\beta}_1} |\\mathbf{X}_T \\right)= L\\left( {\\boldsymbol{\\beta}_2} |\\mathbf{X}_T \\right),\\] since \\(\\boldsymbol{\\Sigma}(\\boldsymbol{\\beta}_1)=\\boldsymbol{\\Sigma}(\\boldsymbol{\\beta}_2)\\). This non-injectivity of the likelihood is problematic since the MLE is the value of \\(\\boldsymbol{\\beta}\\) that maximzes the likelihood function. Therefore, in this case the solution will not be unique. Given that the identifiability issuees illutrated in Examples 5.2 and 5.3, the accepted rule to choose which MA(\\(q\\)) model to estimate (among the equivalent models) is to only consider the MA(\\(q\\)) models that, when rewritten in an AR(\\(p\\)) form, can be defined as being AR(\\(\\infty\\)) models. These models are called invertible. Let us consider a MA(1) model \\[X_t = \\theta W_{t-1} + W_t,\\] which can be rewritten as \\[W_t = -\\theta W_{t-1} + X_t.\\] Now, the MA(1) model has taken the form of an AR(1) model and, using the recursive approach that we used to study the AR(\\(p\\)) models, we can show that we get to the form \\[W_t = (-\\theta)^t W_0 + \\sum_{j = 0}^t (-\\theta)^j X_{t-j}.\\] If we assume \\(|\\theta| &lt; 1\\), then we finally get to the causal AR(1) representation \\[W_t = \\sum_{j = 0}^{\\infty} (-\\theta)^j X_{t-j}.\\] Therefore, between two MA(1) models with two identical ACFs, we choose the invertible MA(1) which respects the condition \\(|\\theta| &lt; 1\\). The same reasoning as for causal AR(\\(p\\)) models is also applied for invertible MA(\\(q\\)) models thereby restricting the possible values of the parameters of an MA(\\(q\\)) model to the admissable region defined in the same way as for causal AR(\\(p\\)) models. This reasoning also allows us to study MA(\\(q\\)) models similarly to AR(\\(p\\)) models. Indeed, we can write an MA(1) model as \\[X_t = \\theta(B)W_t,\\] where \\(\\theta(B)\\) is the moving average operator defined earlier. Based on this we can rewrite the model as \\[\\frac{1}{\\theta(B)} X_t = W_t,\\] and, if \\(|\\theta| &lt; 1\\), we can rexpress the inverse moving average operator \\(\\theta^{-1}(B)\\) as \\(\\sum_{j = 0}^{\\infty} (-\\theta)^j B^j\\) which provides us with the invertible representation of an MA(1) process seen earlier. 5.1.1 Autocovariance of MA processes In this section, we will briefly discuss the behaviour of the ACF and PACF of MA(\\(q\\)). As an example let us consider the following four MA(\\(q\\)) models: \\[ \\begin{aligned} X_t &amp;= 0.9 W_{t-1} + W_t \\\\ X_t &amp;= -0.9 W_{t-1} + W_t \\\\ Y_t &amp;= 1.2W_{t-1} -0.3 W_{t-2}+ W_t \\\\ Z_t &amp;= -1.5 W_{t-1} + 0.5 W_{t-2} - 0.2 W_{t-3} + W_t. \\end{aligned} \\] The theoretical ACF plots of these three models are shown below. par(mfrow = c(2,2)) plot(theo_acf(ar = 0, ma = 0.9)) plot(theo_acf(ar = 0, ma = -0.9)) plot(theo_acf(ar = 0, ma = c(1.2, -0.3))) plot(theo_acf(ar = 0, ma = c(-1.5, 0.5, -0.2))) Figure 5.1: Theoretical ACF of four MA models with parameters. As you can notice, the values of the ACF plots become zero as soon as the lag of the ACF goes beyond the order \\(q\\) of the MA(\\(q\\)) model. Hence, the ACF plot of an MA(\\(q\\)) model plays the same role that the PACF plays for AR(\\(p\\)) models: it helps determine the order \\(q\\) of the MA(\\(q\\)) process generated a given time series. Let us now consider the PACF plots of the same models defined above. par(mfrow = c(2,2)) plot(theo_pacf(ar = 0, ma = 0.9)) plot(theo_pacf(ar = 0, ma = -0.9)) plot(theo_pacf(ar = 0, ma = c(1.2, 0.3))) plot(theo_pacf(ar = 0, ma = c(-1.5, 0.5, -0.2))) Figure 5.2: Theoretical PACF of four MA models with parameters. In this case, we can see that the PACF plots show a certain exponential (sinusoidal) decay that is usually observed in the ACF plots of AR(\\(p\\)) models. Therefore, the roles of the ACF and PACF in identifying the kind of AR(\\(p\\)) models underlying an observed time series is completely inversed when considering MA(\\(q\\)) models. Indeed, the PACF plot of an MA(\\(q\\)) model has an exponentially decay while the ACF cuts off after the lag has reached the order of the model (\\(q\\) for MA(\\(q\\)) models). Let us simulate \\(T = 200\\) observations from \\((X_t)\\) to obtain the following time series: set.seed(6) Xt = gen_gts(MA(theta = 0.9, sigma2 = 1), n = 200) plot(Xt) Figure 5.3: Simulated MA(1) Process Using this data, let us estimate the ACF and PACF of this time series. corr_analysis(Xt) Figure 5.4: Empirical ACF and PACF of the previously simulated MA(1). ## $ACF ## , , x ## ## ACF ## 0 1.000000000 ## 1 0.472261150 ## 2 -0.018851788 ## 3 -0.033191035 ## 4 -0.120275338 ## 5 -0.085195707 ## 6 0.010283221 ## 7 -0.029794360 ## 8 -0.059643898 ## 9 -0.041696495 ## 10 -0.067652169 ## 11 -0.017665649 ## 12 0.047223407 ## 13 -0.006182177 ## 14 0.005662105 ## 15 0.017117486 ## 16 -0.022504798 ## 17 -0.017749964 ## 18 -0.031372198 ## 19 -0.053242790 ## 20 -0.067897555 ## 21 -0.040768266 ## 22 -0.068425874 ## 23 -0.115004927 ## ## attr(,&quot;n&quot;) ## [1] 200 ## attr(,&quot;class&quot;) ## [1] &quot;simtsACF&quot; &quot;array&quot; ## ## $PACF ## , , x ## ## PACF ## 0 0.4722611501 ## 1 -0.3113151952 ## 2 0.1788676108 ## 3 -0.2801773660 ## 4 0.1970491078 ## 5 -0.1366974352 ## 6 0.0391139097 ## 7 -0.0932530561 ## 8 0.0263694783 ## 9 -0.0987396689 ## 10 0.1062457630 ## 11 -0.0639802219 ## 12 0.0001480401 ## 13 0.0309131733 ## 14 -0.0490788004 ## 15 0.0330684200 ## 16 -0.0586922328 ## 17 -0.0059040552 ## 18 -0.0403650516 ## 19 -0.0585344478 ## 20 0.0354933614 ## 21 -0.1516389254 ## 22 -0.0021330270 ## ## attr(,&quot;n&quot;) ## [1] 200 ## attr(,&quot;class&quot;) ## [1] &quot;PACF&quot; &quot;array&quot; From these plots, using the PACF we could conclude that this function appears to decay as the lags increase (as the PACF of an MA(\\(q\\)) model is expected to do) while using the ACF we could state that the correct order for the MA(\\(q\\)) model is \\(q = 1\\) since the ACF does not appear to be significant after lag 1. Let us now also consider the model selection criteria described in the previous section by applying the select() function to the simulated time series. lynx_select = select(MA(10), Xt, include.mean = FALSE) Figure 5.5: Values of the three model selection criteria for all candidate models included in an MA(10) model for the previously simulated MA(1). Also in this case we see that the model selection criteria all tend to agree with each other on the fact that the best model to describe the observed time series is indeed an MA(1) model. "]
]
