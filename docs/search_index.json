[
["index.html", "Applied Time Series Analysis with R Chapter 1 Preface", " Applied Time Series Analysis with R Stéphane Guerrier, Roberto Molinari and Haotian Xu 2018-08-17 Chapter 1 Preface Welcome to Applied Time Series Analysis with R "],
["about-this-book.html", "1.1 About This Book", " 1.1 About This Book This book is intended as a support for the course of 463 (Applied Time Series). It contains an overview of the basic procedures to adequately approach a time series analysis with insight to more advanced analysis of time series. It firstly introduces the basic concepts and theory to appropriately use the applied tools that are presented in the second (and main) part of the book. In the latter part the reader will learn how to use descriptive analysis to identify the important characteristics of a time serues and then employ modelling and inference techniques (made available through R funtions) that allow to describe a time series and make predictions. The last part of the book will give introductory notions on more advanced analysis of time series where the reader will achieve a basic understanding of the tools available to anayse more complex characteristic of time series. This document is under active development and as a result is likely to contains many errors. As Montesquieu puts it: “La nature semblait avoir sagement pourvu à ce que les sottises des hommes fussent passagères, et les livres les immortalisent.” "],
["contents.html", "1.2 Contents", " 1.2 Contents This book is structured as follows: Basic Elements of Time Series Wold representation deterministic + random Examples of deterministic components (trend + seasonality) Random components: basic time series models Fundamental Representations Conditions for fundamental representations (e.g. gaussian) AutoCovariance and AutoCorrelation Functions Estimators: Empirical ACF Spectral Density and WV Stationarity of Time Series Stationarity vs Non-Stationarity Linear operators and processes Weak and Strong Stationarity SARIMA Models AR(p) Models MA(q) Models ARMA(p,q) Models ARIMA(p,d,q) Models SARIMA(p,d,q)(P,D,Q) Models Descriptive Analysis Raw Data ACF plots Identifying models Other representations: SDF and WV Inference Estimation Inference Model Selection Advanced Topics GARCH State-Space Models Multivariate (VAR) Models "],
["bibliographic-note.html", "1.3 Bibliographic Note", " 1.3 Bibliographic Note This text is heavily inspired by the following three execellent references: “Time Series Analysis and Its Applications”, Fourth Edition, Robert H. Shumway &amp; David S. Stoffer. “Time Series for Macroeconomics and Finance”, John H. Cochrane. “Cours de Séries Temporelles: Théorie et Applications”, Volume 1, Arthur Charpentier. "],
["acknowledgements.html", "1.4 Acknowledgements", " 1.4 Acknowledgements The text has been developed in the open and has benefited greatly from many people being able to alert the authors to problematic areas. We are greatful for the corrections, suggestions, or requests ofclarity from the following: Ziying Wang Haoxian Zhong Zhihan Xiong Nathanael Claussen Justin Lee James Balamuta "],
["license.html", "1.5 License", " 1.5 License This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. This book is aimed at providing the procedures (and relative statistical theory) to allow the reader to appropriately analyse a time series "],
["basic-elements-of-time-series.html", "Chapter 2 Basic Elements of Time Series", " Chapter 2 Basic Elements of Time Series We can start the discussion on the basic elements of time series by using a practical example from real data made available through the R software. The data represent the global mean land–ocean temperature shifts from 1880 to 2015 (with base index being the average temperatures from 1951 to 1980) and this time series is represented in the plot below. These data have been used as a support in favour of the argument that the global temperatures are increasing and that global warming has occured over the last half of the twentieth century. The first approach that one would take is to try and measure the average increase by fitting a (linear) model and testing if the (positive) slope is significant. In order to do so, we would require the residuals from the fitted model to independently and identically distributed (iid). Let us fit a model with the years (time) as explanatory variable and check the residuals from this fit. It can be seen from the upper left plot that the trend appears to be removed and, if looking at the residuals as one would usually do in a (linear) regression framework, the residual plots seem to suggest that the modelling has done a relatively good job since no particular pattern seems to emerge and their distribution is quite close to being Gaussian. However, is it possible to conclude from the plots that the data are iid? More specifically, can we assume that the residuals are independent? This is a fundamental question in order for inference procedures to be carried out in an appropriate manner and to limit false conclusions. Let us provide an example through a simulated data set where we know that there is an upward trend through time (i.e. the slope \\(\\beta = 0.01\\)) and our goal would be to show that this trend exists. Considering this, we simulate two cases where, in the first, the residuals are actually Gaussian iid while, in the second, the residuals are Gaussian but are dependent over time. The first case is shown below. set.seed(9) # Simulate time series with iid residuals y.ind &lt;- cumsum(rep(0.01, 100)) + rnorm(100) # Simulate time series with dependent residuals y.dep &lt;- cumsum(rep(0.01, 100)) + arima.sim(n = 100, list(ar = c(0.8897, -0.4858))) # Define explanatory variable (time) time &lt;- 1:100 # Fit a linear model to estimate the slope (for the iid setting) fit.ind &lt;- lm(y.ind ~ time) summary(fit.ind) ## ## Call: ## lm(formula = y.ind ~ time) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.6000 -0.7028 -0.1409 0.4439 2.7085 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.002092 0.194085 0.011 0.99142 ## time 0.008899 0.003337 2.667 0.00895 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9632 on 98 degrees of freedom ## Multiple R-squared: 0.06767, Adjusted R-squared: 0.05816 ## F-statistic: 7.113 on 1 and 98 DF, p-value: 0.008954 # Fit a linear model to estimate the slope (for the dependent setting) fit.dep &lt;- lm(y.dep ~ time) summary(fit.dep) ## ## Call: ## lm(formula = y.dep ~ time) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.6058 -0.9505 0.2111 1.1174 2.4196 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.071260 0.293703 -0.243 0.8088 ## time 0.008441 0.005049 1.672 0.0977 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.458 on 98 degrees of freedom ## Multiple R-squared: 0.02773, Adjusted R-squared: 0.01781 ## F-statistic: 2.795 on 1 and 98 DF, p-value: 0.09775 As can be seen, the estimated slope (\\(\\approx\\) 0.009) is close to the true slope (0.01) and is significant (i.e. the p-value is smaller than the common rejection level 0.05). Hence, from this inference procedure we can conclude that the slope is significant and is roughly equal to 0.01 (which corresponds to the truth). However, let us explore the same analysis when the residuals are not independent. set.seed(9) # Simulate time series with dependent residuals y.dep &lt;- cumsum(rep(0.01, 100)) + arima.sim(n = 100, list(ar = c(0.8897, -0.4858))) # Define explanatory variable (time) time &lt;- 1:100 # Fit a linear model to estimate the slope (for the dependent setting) fit.dep &lt;- lm(y.dep ~ time) summary(fit.dep) ## ## Call: ## lm(formula = y.dep ~ time) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.2157 -0.9189 -0.1220 0.9663 3.6754 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.276142 0.289520 0.954 0.343 ## time 0.002288 0.004977 0.460 0.647 ## ## Residual standard error: 1.437 on 98 degrees of freedom ## Multiple R-squared: 0.002152, Adjusted R-squared: -0.00803 ## F-statistic: 0.2114 on 1 and 98 DF, p-value: 0.6467 In this case we can observe that the p-value is greater than 0.05 and consequently the slope does not appear to be significant (although it is in reality). Therefore, the inference procedures can be misleading when not taking into account other possible significant variables or, in this case, forms of dependence that can hide true underlying effects. The above examples therefore highlight how the approach to analysing time series does not only rely on finding an appropriate model that describes the evolution of a variable as a function of time (which is deterministic). Indeed, the main focus of time series analysis consists in modelling the dependence structure that describes how random variables impact each other as a function of time. In other words, a time series is a collection of random variables whose interaction and dependence structure is indexed by time. "],
["the-wold-decomposition.html", "2.1 The Wold Decomposition", " 2.1 The Wold Decomposition The previous discussion highlighted how a time series can be decomposed into a deterministic component and a random component. Leaving aside technical rigour, this characteristic of time series was put forward in Wold’s Decomposition Theorem who postulated that a time series \\((Y_t)\\) (where \\(t = 1,...,n\\) represents the time index) can be very generically represented as follows: \\[Y_t = D_t + W_t,\\] where \\(D_t\\) represents the deterministic part (or signal) that can be modelled through the standard modelling techniques (e.g. linear regression) and \\(W_t\\) that, restricting ourselves to a general class of processes, represents the random part (noise) that requires the analytical and modelling approaches that will be tackled in this book. Typically, we have \\(\\mathbb{E}[Y_t] \\neq 0\\) while \\(\\mathbb{E}[W_t] = 0\\) (although we may have \\(\\mathbb{E}[W_t | W_{t-1}, ..., W_1] \\neq 0\\)). Such models impose some parametric structure which represents a convenient and flexible way of studying time series as well as a means to evaluate future values of the series through forecasting. As we will see, predicting future values is one of the main aspects of time series analysis. However, making predictions is often a daunting task or as famously stated by Nils Bohr: “Prediction is very difficult, especially about the future.” There are plenty of examples of predictions that turned out to be completely erroneous. For example, three days before the 1929 crash, Irving Fisher, Professor of Economics at Yale University, famously predicted: “Stock prices have reached what looks like a permanently high plateau”. Another example is given by Thomas Watson, president of IBM, who said in 1943: “I think there is a world market for maybe five computers.” Let us now briefly discuss the two components of a time series. 2.1.1 The deterministic component (Signal) Before shifting our focus to the random component of time series, we will first just briefly underline the main features that should be taken into account for the deterministic component. The first feature that should be analysed is the trend that characterises the time series, more specifically the behaviour of the variable of interest as a specific function of time (as the global temperature time series seen earlier). Let us consider another example of time series based on real data, i.e. the quarterly earnings of Johnson &amp; Johnson between 1960 and 1980 represented below. As can be seen from the plot, the earnings appear to grow over time, therefore we can imagine fitting a line to this data to describe its behaviour (see red line below). Although the line captures a part of the behaviour, it is quite clear that the trend of the time series is not linear. It could therefore be more appropriate to define another function of time to describe it and, consequently, we add a quadratic term of time to obtain the following fit. We can see now that the quadratic function of time allows to better fit the observed time series and closely follow the observations. However, there still appears to be a pattern in the data that isn’t captured by this quadratic model. This pattern appears to be repeated over time: peaks and valleys that seem to occur at regular intervals along the time series. This behaviour is known as seasonality which, in this case, can be explained by the effect of a specific quarter on the behaviour of the earnings. Indeed, it is reasonable to assume that the seasons have impacts on different variables measured over time (e.g. temperatures, earnings linked to sales that vary with seasons, etc.). Let us therefore take the quarters as an explanatory variable and add it to the previous quadratic model (see fit below). This final fit appears to well describe the behaviour of the earnings. Hence, trend and seasonality are the main features that characterize the deterministic component of a time series. However, as discussed earlier, these deterministic components often don’t explain all of the observed time series since there is often a random component characterizing data measured over time. Not considering the latter component can have considerable impacts on the inference procedures (as seen earlier) and it is therefore important to adequately analyse them (see next section). 2.1.2 The random component (Noise) From this section onwards we will refer to time series as being solely the random noise component. Keeping this in mind, a time series is a particular kind of stochastic process which, generally speaking, is a collection of random variables indexed by a set of numbers. Not surprisingly, the index of reference for a time series is given by time and, consequently, a time series is a collection of random variables indexed (or “measured”) over time such as, for example, the daily price of a financial asset or the monthly average temperature in a given location. In terms of notation, a time series is often represented as \\[\\left(X_1, X_2, ..., X_n \\right) \\;\\;\\; \\text{ or } \\;\\;\\; \\left(X_t\\right)_{t = 1,...,n}.\\] The time index \\(t\\) is contained within either the set of reals, \\(\\mathbb{R}\\), or integers, \\(\\mathbb{N}\\). When \\(t \\in \\mathbb{R}\\), the time series becomes a continuous-time stochastic process such a Brownian motion, a model used to represent the random movement of particles within a suspended liquid or gas. However, within this book, we will limit ourselves to the cases where \\(t \\in \\mathbb{N}\\), better known as discrete-time processes. Discrete-time processes are measured sequentially at fixed and equally spaced intervals in time. This implies that we will uphold two general assumptions for the time series considered in this book: \\(t\\) is not random, e.g. the time at which each observation is measured is known, and the time between two consecutive observations is constant. This book will also focus on certain representations of time series based on parametric probabilistic models. For example, one of the fundamental probability models used in time series analysis is called the white noise model and is defined as \\[X_t \\mathop \\sim \\limits^{iid} N(0, \\sigma^2).\\] This statement simply means that \\((X_t)\\) is normally distributed and independent over time. Ideally, this is the type of process that we would want to observe once we have performed a statistical modelling procedure. However, despite it appearing to be an excessively simple model to be considered for time series, it is actually a crucial component to construct a wide range of more complex time series models (see Chapter 2). Indeed, unlike the white noise process, time series are typically not independent over time. For example, if we suppose that the temperature in State College is unusually low on a given dat, then it is reasonable to assume that temperature the day after will also be low. With this in mind, let us present the basic parametric models that are used to build even more complex models to describe and predict the behaviour of a time series. "],
["basicmodels.html", "2.2 Basic Time Series Models", " 2.2 Basic Time Series Models In this section, we introduce some simple time series models that consitute the building blocks for the more complex and flexible classes of time series commonly used in practice. Before doing so it is useful to define \\(\\Omega_t\\) as all the information avaiable up to time \\(t-1\\), i.e. \\[\\Omega_t = \\left(X_{t-1}, X_{t-2}, ..., X_0 \\right).\\] As we will see further on, this compact notation is quite useful. 2.2.1 White Noise As we saw earlier, the White Noise model is the building block for most time series models and, to better specify the notation used throughout this book, is defined as \\[{W_t}\\mathop \\sim \\limits^{iid} N\\left( {0,\\sigma _w^2} \\right).\\] This definition implies that: \\(\\mathbb{E}[W_t | \\Omega_t] = 0\\) for all \\(t\\), \\(\\text{cov}\\left(W_t, W_{t-h} \\right) = \\boldsymbol{1}_{h = 0} \\; \\sigma^2\\) for all \\(t, h\\). More specifically, \\(h \\in \\mathbb{N}^+\\) is the time difference between lagged variables. Therefore, in this process there is an absence of temporal (or serial) dependence and it is homoskedastic (i.e. it has a constant variance). White noise can be categorzied into two sorts of processes: weak and strong. The process \\((W_t)\\) is a weak white noise if \\(\\mathbb{E}[W_t] = 0\\) for all \\(t\\), \\(\\text{var}\\left(W_t\\right) = \\sigma_w^2\\) for all \\(t\\), \\(\\text{cov} \\left(W_t, W_{t-h}\\right) = 0\\), for all \\(t\\), and for all \\(h \\neq 0\\). Note that this definition does not imply that \\(W_t\\) and \\(W_{t-h}\\) are independent (for \\(h \\neq 0\\)) but simply uncorrelated. However, the notion of independence is used to define a strong white noise as \\(\\mathbb{E}[W_t] = 0\\) and \\(\\text{var}(W_t) = \\sigma^2 &lt; \\infty\\), for all \\(t\\), \\(F(W_t) = F(W_{t-h})\\), for all \\(t,h\\) (where \\(F(W_t)\\) denotes the distribution of \\(W_t\\)), \\(W_t\\) and \\(W_{t-h}\\) are independent for all \\(t\\) and for all \\(h \\neq 0\\). It is clear from these definitions that if a process is a strong white noise it is also a weak white noise. However, the converse is not true as shown in the following example: Example 2.1 Let \\(Y_t \\mathop \\sim F_{\\eta+2}\\), where \\(F_{\\eta+2}\\) denotes a Student distribution with \\(t+2\\) degrees of freedom. Assuming the sequence \\((Y_1, \\ldots, Y_n)\\) to be independent, we let \\(X_t = \\sqrt{\\frac{\\eta}{\\eta+2}} Y_t\\). Then, the process \\((X_t)\\) is obviously not a strong white noise as the distribution of \\(X_t\\) changes with \\(t\\). However, this process is a weak white noise since we have: \\(\\mathbb{E}[X_t] = \\sqrt{\\frac{t}{t+2}} \\mathbb{E}[Y_t] = 0\\) for all \\(t\\). \\(\\text{var}(X_t) = \\frac{t}{t+2} \\text{var}(Y_t) = \\frac{t}{t+2} \\frac{t+2}{t} = 1\\) for all \\(t\\). \\(\\text{cov}(X_t, X_{t+h}) = 0\\) (by independence), for all \\(t\\), and for all \\(h \\neq 0\\). The code below presents an example of how to simulate a Gaussian white noise process. n = 1000 # process length sigma2 = 1 # process variance Xt = gen_gts(n, WN(sigma2 = sigma2)) plot(Xt) 2.2.2 Random Walk The term random walk was first introduced by Karl Pearson in the early nineteen-hundreds. Regarding white noise, there exist a large range of random walk processes. For example, one of the simplest forms of a random walk process can be explained as follows: suppose that you are walking on campus and your next step can either be to your left, your right, forward or backward (each with equal probability). Two realizations of such processes are represented below: set.seed(5) RW2dimension(steps = 10^2) RW2dimension(steps = 10^4) Such processes inspired Karl Pearson’s famous quote that “the most likely place to find a drunken walker is somewhere near his starting point.” Empirical evidence of this phenomenon is not too hard to find on a Friday night. In this text, we only consider one very specific form of random walk, namely the Gaussian random walk which can be defined as: \\[X_t = X_{t-1} + W_t,\\] where \\(W_t\\) is a Gaussian white noise process with initial condition \\(X_0 = c\\). (Typically \\(c = 0\\).) This process can be expressed differently by backsubstitution as follows: \\[\\begin{aligned} {X_t} &amp;= {X_{t - 1}} + {W_t} \\\\ &amp;= \\left( {{X_{t - 2}} + {W_{t - 1}}} \\right) + {W_t} \\\\ &amp;= \\vdots \\\\ {X_t} &amp;= \\sum\\limits_{i = 1}^t {{W_i}} + X_0 = \\sum\\limits_{i = 1}^t {{W_i}} + c \\\\ \\end{aligned} \\] The code below presents an example of how to simulate a such process. n = 1000 # process length gamma2 = 1 # innovation variance Xt = gen_gts(n, RW(gamma2 = gamma2)) plot(Xt) 2.2.3 First-Order Autoregressive Model An first-order autoregressive model or AR(1) is a generalization of both the white noise and random walk processes which are both themselves special cases of an AR(1). A (Gaussian) AR(1) process can be defined as \\[{X_t} = {\\phi}{X_{t - 1}} + {W_t},\\] where \\(W_t\\) is a Gaussian white noise. Clearly, an AR(1) with \\(\\phi = 0\\) is a Gaussian white noise and when \\(\\phi = 1\\) the process becomes a random walk. Remark. We generally assume that an AR(1), as well as other time series models, have zero mean. The reason for this assumption is only to simplfy the notation but it is easy to consider an AR(1) process around an arbitrary mean \\(\\mu\\), i.e. \\[\\left(X_t - \\mu\\right) = \\phi \\left(X_{t-1} - \\mu \\right) + W_t,\\] which is of course equivalent to \\[X_t = \\left(1 - \\phi \\right) \\mu + \\phi X_{t-1} + W_t.\\] Thus, we will generally only work with zero mean processes since adding means is simple. Remark. An AR(1) is in fact a linear combination of past realisations of the white noise \\(W_t\\) process. Indeed, we have \\[\\begin{aligned} {X_t} &amp;= {\\phi_t}{X_{t - 1}} + {W_t} = {\\phi}\\left( {{\\phi}{X_{t - 2}} + {W_{t - 1}}} \\right) + {W_t} \\\\ &amp;= \\phi^2{X_{t - 2}} + {\\phi}{W_{t - 1}} + {W_t} = {\\phi^t}{X_0} + \\sum\\limits_{i = 0}^{t - 1} {\\phi^i{W_{t - i}}}. \\end{aligned}\\] Under the assumption of infinite past (i.e. \\(t \\in \\mathbb{Z}\\)) and \\(|\\phi| &lt; 1\\), we obtain \\[X_t = \\sum\\limits_{i = 0}^{\\infty} {\\phi^i {W_{t - i}}},\\] since \\(\\operatorname{lim}_{i \\to \\infty} \\; {\\phi^i}{X_{t-i}} = 0\\). The code below presents an example of how an AR(1) can be simulated. n = 1000 # process length phi = 0.5 # phi parameter sigma2 = 1 # innovation variance Xt = gen_gts(n, AR1(phi = phi, sigma2 = sigma2)) plot(Xt) 2.2.4 Moving Average Process of Order 1 As we have seen in the previous example, an AR(1) can be expressed as a linear combination of all past observations of \\((W_t)\\) while the next process, called a moving average process of order 1 or MA(1), is (in some sense) a “truncated” version of an AR(1). It is defined as \\[\\begin{equation} X_t = \\theta W_{t-1} + W_t, \\end{equation}\\] where (again) \\(W_t\\) denotes a Gaussian white noise process. An example on how to generate an MA(1) is given below: n = 1000 # process length sigma2 = 1 # innovation variance theta = 0.5 # theta parameter Xt = gen_gts(n, MA1(theta = theta, sigma2 = sigma2)) plot(Xt) 2.2.5 Linear Drift A linear drift is a very simple deterministic time series model which can be expressed as \\[X_t = X_{t-1} + \\omega, \\] where \\(\\omega\\) is a constant and with the initial condition \\(X_0 = c\\), where \\(c\\) is an arbitrary constant (typically zero). This process can be expressed in a more familiar form as follows: \\[ {X_t} = {X_{t - 1}} + \\omega = \\left( {{X_{t - 2}} + \\omega} \\right) + \\omega = t{\\omega} + c \\] Therefore, a (linear) drift corresponds to a simple linear model with slope \\(\\omega\\) and intercept \\(c\\). A drift can simply be generated using the code below: n = 100 # process length omega = 0.5 # slope parameter Xt = gen_gts(n, DR(omega = omega)) plot(Xt) "],
["lts.html", "2.3 Composite Stochastic Processes", " 2.3 Composite Stochastic Processes A composite stochastic process can be defined as the sum of underlying (or latent) stochastic processes. In this text, we will use the term latent time series as a synomym for composite stochastic processes. A simple example of such a process is given by \\[\\begin{aligned} Y_t &amp;= Y_{t-1} + W_t + \\delta\\\\ X_t &amp;= Y_t + Z_t, \\end{aligned}\\] where \\(W_t\\) and \\(Z_t\\) are two independent Gaussian white noise processes. This model is often used as a first tool to approximate the number of individuals in the context ecological population dynamics. For example, suppose we want to study the population of Chamois in the Swiss Alps. Let \\(Y_t\\) denote the “true” number of individuals in this population at time \\(t\\). It is reasonable that \\(Y_t\\) is (approximately) the population at the previous time \\(t-1\\) (e.g the previous year) plus a random variation and a drift. This random variation is due to the natural randomness in ecological population dynamics and reflects changes such as the number of predators, the abundance of food, or weather conditions. On the other hand, ecological drift is often of particular interest for ecologists as it can be used to determine the “long” term trends of the population (e.g. if the population is increasing, decreasing, or stable). Of course, \\(Y_t\\) (the number of individauls) is typically unknown and we observe a noisy version of it, denoted as \\(X_t\\). This process corresponds to the true population plus a measurement error since some individuals may not be observed while others may have been counted several times. Interestingly, this process can clearly be expressed as a latent time series model (or composite stochastic process) as follows: \\[\\begin{aligned} R_t &amp;= R_{t-1} + W_t \\\\ S_t &amp;= \\delta t \\\\ X_t &amp;= R_t + S_t + Z_t, \\end{aligned}\\] where \\(R_t\\), \\(S_t\\) and \\(Z_t\\) denote, respectively, a random walk, a drift, and a white noise. The code below can be used to simulate such data: n = 1000 # process length delta = 0.005 # delta parameter (drift) sigma2 = 10 # variance parameter (white noise) gamma2 = 0.1 # innovation variance (random walk) model = WN(sigma2 = sigma2) + RW(gamma2 = gamma2) + DR(omega = delta) Xt = gen_lts(n, model) plot(Xt) In the above graph, the first three plots represent the latent (unobserved) processes (i.e. white noise, random walk, and drift) and the last one represents the sum of the three (i.e. \\((X_t)\\)). Let us consider a real example where these latent processes are useful to describe (and predict) the behavior of economic variables such as Personal Saving Rates (PSR). A process that is used for these settings is the “random-walk-plus-noise” model, meaning that the data can be explained by a random walk process in addition to which we observe some other process (e.g. a white noise model, an autoregressive model such as an AR(1), etc.). The PSR taken from the Federal Reserve of St. Louis from January 1, 1959, to May 1, 2015, is presented in the following plot: # Load savingrt dataset data(&quot;savingrt&quot;) # Simulate based on data savingrt = gts(as.vector(savingrt), start = 1959, freq = 12, unit_ts = &quot;%&quot;, name_ts = &quot;Saving Rates&quot;, data_name = &quot;US Personal Saving Rates&quot;) # Plot savingrt simulation plot(savingrt) It can be observed that the mean of this process seems to vary over time, suggesting that a random walk can indeed be considered as a possible model to explain this data. In addition, aside from some “spikes” and occasional sudden changes, the observations appear to gradually change from one time point to the other, suggesting that some other form of dependence between them could exist. "],
["representations-of-time-series.html", "Chapter 3 Representations of Time Series", " Chapter 3 Representations of Time Series In this chapter we will discuss and formalize how knowledge about \\(X_{t-1}\\) (or more generally about all the information from the past, \\(\\Omega_t\\)) can provide us with some information about the properties of \\(X_t\\). In particular, we will consider the correlation (or covariance) of \\(X_t\\) at different times such as \\(\\text{corr} \\left(X_t, X_{t+h}\\right)\\). This “form” of correlation (covariance) is called the autocorrelation (autocovariance) and is a very useful tool in time series analysis. However, if we do not assume that a time series is characterized by a certain form of “stability”, it would be rather difficult to estimate \\(\\text{corr} \\left(X_t, X_{t+h}\\right)\\) as this quantity would depend on both \\(t\\) and \\(h\\) leading to more parameters to estimate than observations available. Therefore, the concept of stationarity is convenient in this context as it allows (among other things) to assume that \\[\\text{corr} \\left(X_t, X_{t+h}\\right) = \\text{corr} \\left(X_{t+j}, X_{t+h+j}\\right), \\;\\;\\; \\text{for all $j$},\\] implying that the autocorrelation (or autocovariance) is only a function of the lag between observations, rather than time itself. These first of these two concepts (i.e. autocorrelation and stationarity) will be discussed in this chapter while stationarity will be discussed in the following one. Before moving on, it is helpful to remember that correlation (or autocorrelation) is only appropriate to measure a very specific kind of dependence, i.e. the linear dependence. There are many other forms of dependence as illustrated in the bottom panels of the graph below, which all have a (true) zero correlation: Figure 3.1: Different forms of dependence and their Pearson’s r values Several other metrics have been introduced in the literature to assess the degree of “dependence” of two random variables, however this goes beyond the material discussed in this chapter. "],
["the-autocorrelation-and-autocovariance-functions.html", "3.1 The Autocorrelation and Autocovariance Functions", " 3.1 The Autocorrelation and Autocovariance Functions Definition 3.1 The autocovariance function of a series \\((X_t)\\) is defined as \\[{\\gamma_x}\\left( {t,t+h} \\right) = \\text{cov} \\left( {{X_t},{X_{t+h}}} \\right),\\] where the definition of covariance is given by: \\[ \\text{cov} \\left( {{X_t},{X_{t+h}}} \\right) = \\mathbb{E}\\left[ {{X_t}{X_{t+h}}} \\right] - \\mathbb{E}\\left[ {{X_t}} \\right]\\mathbb{E}\\left[ {{X_{t+h}}} \\right]. \\] Similarly, the above expectations are defined to be: \\[\\begin{aligned} \\mathbb{E}\\left[ {{X_t}} \\right] &amp;= \\int\\limits_{ - \\infty }^\\infty {x \\cdot {f_t}\\left( x \\right)dx}, \\\\ \\mathbb{E}\\left[ {{X_t}{X_{t+h}}} \\right] &amp;= \\int\\limits_{ - \\infty }^\\infty {\\int\\limits_{ - \\infty }^\\infty {{x_1}{x_2} \\cdot f_{t,t+h}\\left( {{x_1},{x_2}} \\right)d{x_1}d{x_2}} } , \\end{aligned} \\] where \\({f_t}\\left( x \\right)\\) and \\(f_{t,t+h}\\left( {{x_1},{x_2}} \\right)\\) denote, respectively, the density of \\(X_t\\) and the joint density of the pair \\((X_t, X_{t+h})\\). For the notation, it should be clear that \\(X_t\\) is assumed to be a continous random variable. Since we generally consider stochastic processes with constant zero mean, we often have \\[{\\gamma_x}\\left( {t,t+h} \\right) = \\mathbb{E}\\left[X_t X_{t+h} \\right]. \\] In addition, we normally drop the subscript referring to the time series (i.e. \\(x\\) in this case) if it is clear from the context which time series the autocovariance refers to. For example, we generally use \\({\\gamma}\\left( {t,t+h} \\right)\\) instead of \\({\\gamma_x}\\left( {t,t+h} \\right)\\). Moreover, the notation is even further simplified when the covariance of \\(X_t\\) and \\(X_{t+h}\\) is the same as that of \\(X_{t+j}\\) and \\(X_{t+h+j}\\) (for all \\(j\\)), i.e. the covariance depends only on the time between observations and not on the specific time \\(t\\). This is an important property called stationarity, which will be discussed in the next section. In this case, we simply use to following notation: \\[\\gamma \\left( {h} \\right) = \\text{cov} \\left( X_t , X_{t+h} \\right). \\] This notation will generally be used throughout the text and implies certain properties (i.e. stationarity) on the process \\((X_t)\\). Several remarks can be made on the autocovariance: The autocovariance function is symmetric. That is, \\({\\gamma}\\left( {h} \\right) = {\\gamma}\\left( -h \\right)\\) since \\(\\text{cov} \\left( {{X_t},{X_{t+h}}} \\right) = \\text{cov} \\left( X_{t+h},X_{t} \\right)\\). The autocovariance function “contains” the variance of the process as \\(\\text{var} \\left( X_{t} \\right) = {\\gamma}\\left( 0 \\right)\\). We have that \\(|\\gamma(h)| \\leq \\gamma(0)\\) for all \\(h\\). The proof of this inequality is direct and follows from the Cauchy-Schwarz inequality, i.e. \\[ \\begin{aligned} \\left(|\\gamma(h)| \\right)^2 &amp;= \\gamma(h)^2 = \\left(\\mathbb{E}\\left[\\left(X_t - \\mathbb{E}[X_t] \\right)\\left(X_{t+h} - \\mathbb{E}[X_{t+h}] \\right)\\right]\\right)^2\\\\ &amp;\\leq \\mathbb{E}\\left[\\left(X_t - \\mathbb{E}[X_t] \\right)^2 \\right] \\mathbb{E}\\left[\\left(X_{t+h} - \\mathbb{E}[X_{t+h}] \\right)^2 \\right] = \\gamma(0)^2. \\end{aligned} \\] Just as any covariance, \\({\\gamma}\\left( {h} \\right)\\) is “scale dependent” since \\({\\gamma}\\left( {h} \\right) \\in \\mathbb{R}\\), or \\(-\\infty \\le {\\gamma}\\left( {h} \\right) \\le +\\infty\\). We therefore have: if \\(\\left| {\\gamma}\\left( {h} \\right) \\right|\\) is “close” to zero, then \\(X_t\\) and \\(X_{t+h}\\) are “weakly” (linearly) dependent; if \\(\\left| {\\gamma}\\left( {h} \\right) \\right|\\) is “far” from zero, then the two random variable present a “strong” (linear) dependence. However it is generally difficult to asses what “close” and “far” from zero means in this case. \\({\\gamma}\\left( {h} \\right)=0\\) does not imply that \\(X_t\\) and \\(X_{t+h}\\) are independent but simply \\(X_t\\) and \\(X_{t+h}\\) are uncorrelated. The independence is only implied by \\({\\gamma}\\left( {h} \\right)=0\\) in the jointly Gaussian case. As hinted in the introduction, an important related statistic is the correlation of \\(X_t\\) with \\(X_{t+h}\\) or autocorrelation, which is defined as \\[\\rho \\left( h \\right) = \\text{corr}\\left( {{X_t},{X_{t + h}}} \\right) = \\frac{{\\text{cov}\\left( {{X_t},{X_{t + h}}} \\right)}}{{{\\sigma _{{X_t}}}{\\sigma _{{X_{t + h}}}}}} = \\frac{\\gamma(h) }{\\gamma(0)}.\\] Similarly to \\(\\gamma(h)\\), it is important to note that the above notation implies that the autocorrelation function is only a function of the lag \\(h\\) between observations. Thus, autocovariances and autocorrelations are one possible way to describe the joint distribution of a time series. Indeed, the correlation of \\(X_t\\) with \\(X_{t+1}\\) is an obvious measure of how persistent a time series is. Remember that just as with any correlation: \\(\\rho \\left( h \\right)\\) is “scale free” so it is much easier to interpret than \\(\\gamma(h)\\). \\(|\\rho \\left( h \\right)| \\leq 1\\) since \\(|\\gamma(h)| \\leq \\gamma(0)\\). Causation and correlation are two very different things! 3.1.1 A Fundamental Representation Autocovariances and autocorrelations also turn out to be very useful tools as they are one of the fundamental representations of time series. Indeed, if we consider a zero mean normally distributed process, it is clear that its joint distribution is fully characterized by the autocovariances \\(\\mathbb{E}[X_t X_{t+h}]\\) (since the joint probability density only depends of these covariances). Once we know the autocovariances we know everything there is to know about the process and therefore: if two processes have the same autocovariance function, then they are the same process. 3.1.2 Admissible Autocorrelation Functions Since the autocorrelation is related to a fundamental representation of time series, it implies that one might be able to define a stochastic process by picking a set of autocorrelation values (assuming for example that \\(\\text{var}(X_t) = 1\\)). However, it turns out that not every collection of numbers, say \\(\\{\\rho_1, \\rho_2, ...\\}\\), can represent the autocorrelation of a process. Indeed, two conditions are required to ensure the validity of an autocorrelation sequence: \\(\\operatorname{max}_j \\; | \\rho_j| \\leq 1\\). \\(\\text{var} \\left[\\sum_{j = 0}^\\infty \\alpha_j X_{t-j} \\right] \\geq 0 \\;\\) for all \\(\\{\\alpha_0, \\alpha_1, ...\\}\\). The first condition is obvious and simply reflects the fact that \\(|\\rho \\left( h \\right)| \\leq 1\\) but the second is far more difficult to verify. To further our understanding of the latter we let \\(\\alpha_j = 0\\) for \\(j &gt; 1\\), then condition two implies that \\[\\text{var} \\left[ \\alpha_0 X_{t} + \\alpha_1 X_{t-1} \\right] = \\gamma_0 \\begin{bmatrix} \\alpha_0 &amp; \\alpha_1 \\end{bmatrix} \\begin{bmatrix} 1 &amp; \\rho_1\\\\ \\rho_1 &amp; 1 \\end{bmatrix} \\begin{bmatrix} \\alpha_0 \\\\ \\alpha_1 \\end{bmatrix} \\geq 0. \\] Thus, the matrix \\[ \\boldsymbol{A}_1 = \\begin{bmatrix} 1 &amp; \\rho_1\\\\ \\rho_1 &amp; 1 \\end{bmatrix} \\] must be positive semi-definite. Taking the determinant we have \\[\\operatorname{det} \\left(\\boldsymbol{A}_1\\right) = 1 - \\rho_1^2 \\] implying that the condition \\(|\\rho_1| \\leq 1\\) must be respected. Now, let \\(\\alpha_j = 0\\) for \\(j &gt; 2\\), then we must verify that: \\[\\text{var} \\left[ \\alpha_0 X_{t} + \\alpha_1 X_{t-1} + \\alpha_2 X_{t-2} \\right] = \\gamma_0 \\begin{bmatrix} \\alpha_0 &amp; \\alpha_1 &amp;\\alpha_2 \\end{bmatrix} \\begin{bmatrix} 1 &amp; \\rho_1 &amp; \\rho_2\\\\ \\rho_1 &amp; 1 &amp; \\rho_1 \\\\ \\rho_2 &amp; \\rho_1 &amp; 1 \\end{bmatrix} \\begin{bmatrix} \\alpha_0 \\\\ \\alpha_1 \\\\ \\alpha_2 \\end{bmatrix} \\geq 0. \\] Again, this implies that the matrix \\[ \\boldsymbol{A}_2 = \\begin{bmatrix} 1 &amp; \\rho_1 &amp; \\rho_2\\\\ \\rho_1 &amp; 1 &amp; \\rho_1 \\\\ \\rho_2 &amp; \\rho_1 &amp; 1 \\end{bmatrix} \\] must be positive semi-definite and it is easy to verify that \\[\\operatorname{det} \\left(\\boldsymbol{A}_2\\right) = \\left(1 - \\rho_2 \\right)\\left(- 2 \\rho_1^2 + \\rho_2 + 1\\right). \\] Thus, this implies that \\[\\begin{aligned} &amp;- 2 \\rho_1^2 + \\rho_2 + 1 \\geq 0 \\Rightarrow 1 \\geq \\rho_2 \\geq 2 \\rho_1^2 - 1 \\\\ &amp;\\Rightarrow 1 - \\rho_1^2 \\geq \\rho_2 - \\rho_1^2 \\geq -(1 - \\rho_1^2)\\\\ &amp;\\Rightarrow 1 \\geq \\frac{\\rho_2 - \\rho_1^2 }{1 - \\rho_1^2} \\geq -1. \\end{aligned}\\] Therefore, \\(\\rho_1\\) and \\(\\rho_2\\) must lie in a parabolic shaped region defined by the above inequalities as illustrated in Figure 3.2. Figure 3.2: Admissible autocorrelation functions From our derivation, it is clear that the restrictions on the autocorrelation are very complicated, thereby justifying the need for other forms of fundamental representation which we will explore later in this text. Before moving on to the estimation of the autocorrelation and autocovariance functions, we must first discuss the stationarity of \\((X_t)\\), which will provide a convenient framework in which \\(\\gamma(h)\\) and \\(\\rho(h)\\) can be used (rather that \\(\\gamma(t,t+h)\\) for example) and (easily) estimated. "],
["estimation-of-moments.html", "3.2 Estimation of Moments", " 3.2 Estimation of Moments In this section, we discuss how moments and related quantities of stationary process can be estimated. Informally speaking, the use of “averages” is meaningful for such processes suggesting that classical moments estimators can be employed. Indeed, suppose that one is interested in estimating \\(\\alpha \\equiv \\mathbb{E}[m (X_t)]\\), where \\(m(\\cdot)\\) is a known function of \\(X_t\\). If \\(X_t\\) is a strongly stationary process, we have \\[\\alpha = \\int m(x) \\, f(x) dx\\] where \\(f(x)\\) denotes the density of \\(X_t, \\; \\forall t\\). Replacing \\(f(x)\\) by \\(f_n(x)\\), the empirical density, we obtain the following estimator \\[\\hat{\\alpha} = \\frac{1}{n} \\sum_{i = 1}^n m\\left(x_i\\right).\\] In the next subsection, we examine how this simple idea can be used to estimate the mean, autocovariance and autocorrelation functions. Moreover, we discuss some of the properties of these estimators. 3.2.1 Estimation of the Mean Function If a time series is stationary, the mean function is constant and a possible estimator of this quantity is, as discussed above, given by \\[\\bar{X} = {\\frac{1}{n}\\sum\\limits_{t = 1}^n {{X_t}} }.\\] Naturally, the \\(k\\)-th moment, say \\(\\beta_k \\equiv \\mathbb{E}[X_t^k]\\) can be estimated by \\[\\hat{\\beta}_k = {\\frac{1}{n}\\sum\\limits_{t = 1}^n {{X_t^k}} }, \\;\\; k \\in \\left\\{x \\in \\mathbb{N} : \\, 0 &lt; x &lt; \\infty \\right\\}.\\] The variance of such an estimator can be derived as follows: \\[\\begin{equation} \\begin{aligned} \\text{var} \\left( \\hat{\\beta}_k \\right) &amp;= \\text{var} \\left( {\\frac{1}{n}\\sum\\limits_{t = 1}^n {{X_t^k}} } \\right) \\\\ &amp;= \\frac{1}{{{n^2}}}\\text{var} \\left( {{{\\left[ {\\begin{array}{*{20}{c}} 1&amp; \\cdots &amp;1 \\end{array}} \\right]}_{1 \\times n}}{{\\left[ {\\begin{array}{*{20}{c}} {{X_1^k}} \\\\ \\vdots \\\\ {{X_n^k}} \\end{array}} \\right]}_{n \\times 1}}} \\right) \\\\ &amp;= \\frac{1}{{{n^2}}}{\\left[ {\\begin{array}{*{20}{c}} 1&amp; \\cdots &amp;1 \\end{array}} \\right]_{1 \\times n}} \\, \\boldsymbol{\\Sigma}(k) \\, {\\left[ {\\begin{array}{*{20}{c}} 1 \\\\ \\vdots \\\\ 1 \\end{array}} \\right]_{n \\times 1}}, \\end{aligned} \\tag{3.1} \\end{equation}\\] where \\(\\boldsymbol{\\Sigma}(k) \\in \\mathbb{R}^{n \\times n}\\) and its \\(i\\)-th, \\(j\\)-th element is given by \\[ \\left(\\boldsymbol{\\Sigma}(k)\\right)_{i,j} = \\text{cov} \\left(X_i^k, X_j^k\\right).\\] In the case \\(k = 1\\), (3.1) can easily be further simplified. Indeed, we have \\[\\begin{aligned} \\text{var} \\left( {\\bar X} \\right) &amp;= \\text{var} \\left( {\\frac{1}{n}\\sum\\limits_{t = 1}^n {{X_t}} } \\right) \\\\ &amp;= \\frac{1}{{{n^2}}}{\\left[ {\\begin{array}{*{20}{c}} 1&amp; \\cdots &amp;1 \\end{array}} \\right]_{1 \\times n}}\\left[ {\\begin{array}{*{20}{c}} {\\gamma \\left( 0 \\right)}&amp;{\\gamma \\left( 1 \\right)}&amp; \\cdots &amp;{\\gamma \\left( {n - 1} \\right)} \\\\ {\\gamma \\left( 1 \\right)}&amp;{\\gamma \\left( 0 \\right)}&amp;{}&amp; \\vdots \\\\ \\vdots &amp;{}&amp; \\ddots &amp; \\vdots \\\\ {\\gamma \\left( {n - 1} \\right)}&amp; \\cdots &amp; \\cdots &amp;{\\gamma \\left( 0 \\right)} \\end{array}} \\right]_{n \\times n}{\\left[ {\\begin{array}{*{20}{c}} 1 \\\\ \\vdots \\\\ 1 \\end{array}} \\right]_{n \\times 1}} \\\\ &amp;= \\frac{1}{{{n^2}}}\\left( {n\\gamma \\left( 0 \\right) + 2\\left( {n - 1} \\right)\\gamma \\left( 1 \\right) + 2\\left( {n - 2} \\right)\\gamma \\left( 2 \\right) + \\cdots + 2\\gamma \\left( {n - 1} \\right)} \\right) \\\\ &amp;= \\frac{1}{n}\\sum\\limits_{h = - n}^n {\\left( {1 - \\frac{{\\left| h \\right|}}{n}} \\right)\\gamma \\left( h \\right)} . \\\\ \\end{aligned} \\] Obviously, when \\(X_t\\) is a white noise process, the above formula reduces to the usual \\(\\text{var} \\left( {\\bar X} \\right) = \\sigma^2_w/n\\). In the following example, we consider the case of an AR(1) process and discuss how \\(\\text{var} \\left( {\\bar X} \\right)\\) can be obtained or estimated. Example 3.1 For an AR(1), we have \\(\\gamma(h) = \\phi^h \\sigma_w^2 \\left(1 - \\phi^2\\right)^{-1}\\). Therefore, we obtain (after some computations): \\[\\begin{equation} \\text{var} \\left( {\\bar X} \\right) = \\frac{\\sigma_w^2 \\left( n - 2\\phi - n \\phi^2 + 2 \\phi^{n + 1}\\right)}{n^2\\left(1-\\phi^2\\right)\\left(1-\\phi\\right)^2}. \\end{equation}\\] Unfortunately, deriving such an exact formula is often difficult when considering more complex models. However, asymptotic approximations are often employed to simplify the calculation. For example, in our case we have \\[\\mathop {\\lim }\\limits_{n \\to \\infty } \\; n \\text{var} \\left( {\\bar X} \\right) = \\frac{\\sigma_w^2}{\\left(1-\\phi\\right)^2},\\] providing the following approximate formula: \\[\\text{var} \\left( {\\bar X} \\right) \\approx \\frac{\\sigma_w^2}{n \\left(1-\\phi\\right)^2}.\\] Alternatively, simulation methods can also be employed. For example, a possible strategy would be parametric bootstrap. Theorem 3.1 1. Simulate a new sample under the postulated model, i.e. \\(X_t^* \\sim F_{\\boldsymbol{theta}}{\\) (note: if \\(\\boldsymbol{theta}\\) is unknown it can be replace by \\(\\hat{\\boldsymbol{theta}}\\), a suitable estimator). 2. Compute the statistics of interest on the simulated sample \\((X_t^*)\\). 3. Repeat Steps 1 and 2 \\(B\\) times where \\(B\\) is sufficiently “large” (typically \\(100 \\leq B \\leq 10000\\)). 4. Compute the empirical variance of the statistics of interest based on the \\(B\\) independent replications. In our example, we would consider \\((X_t^*)\\) to be \\({\\bar{X}^*}\\) and seek to obtain: \\[\\hat{\\sigma}^2_B = \\frac{1}{B-1} \\sum_{i = 1}^B \\left(\\bar{X}^*_i - \\bar{X}^* \\right)^2, \\;\\;\\; \\text{where} \\;\\;\\; \\bar{X}^* = \\frac{1}{B} \\sum_{i=1}^B \\bar{X}^*_i,\\] where \\(\\bar{X}^*_i\\) denotes the value of the mean estimated on the \\(i\\)-th simulated sample. The figure below generated by the following code compares these three methods for \\(n = 10\\), \\(B = 1000\\), \\(\\sigma^2 = 1\\) and a grid of values for \\(\\phi\\) going from \\(-0.95\\) to \\(0.95\\): # Define sample size n = 10 # Number of Monte-Carlo replications B = 5000 # Define grid of values for phi phi = seq(from = 0.95, to = -0.95, length.out = 30) # Define result matrix result = matrix(NA,B,length(phi)) # Start simulation for (i in seq_along(phi)){ # Define model model = AR1(phi = phi[i], sigma2 = 1) # Monte-Carlo for (j in seq_len(B)){ # Simulate AR(1) Xt = gen_gts(n, model) # Estimate Xbar result[j,i] = mean(Xt) } } # Estimate variance of Xbar var.Xbar = apply(result,2,var) # Compute theoretical variance var.theo = (n - 2*phi - n*phi^2 + 2*phi^(n+1))/(n^2*(1-phi^2)*(1-phi)^2) # Compute (approximate) variance var.approx = 1/(n*(1-phi)^2) # Compare variance estimations plot(NA, xlim = c(-1,1), ylim = range(var.approx), log = &quot;y&quot;, ylab = expression(paste(&quot;var(&quot;, bar(X), &quot;)&quot;)), xlab= expression(phi), cex.lab = 1) grid() lines(phi,var.theo, col = &quot;deepskyblue4&quot;) lines(phi, var.Xbar, col = &quot;firebrick3&quot;) lines(phi,var.approx, col = &quot;springgreen4&quot;) legend(&quot;topleft&quot;,c(&quot;Theoretical variance&quot;,&quot;Bootstrap variance&quot;,&quot;Approximate variance&quot;), col = c(&quot;deepskyblue4&quot;,&quot;firebrick3&quot;,&quot;springgreen4&quot;), lty = 1, bty = &quot;n&quot;,bg = &quot;white&quot;, box.col = &quot;white&quot;, cex = 1.2) It can be observed that the variance of \\(\\bar{X}\\) typically increases with \\(\\phi\\). As expected when \\(\\phi = 0\\), we have \\(\\text{var}(\\bar{X}) = 1/n\\) — in this case the process is a white noise. Moreover, the bootstrap approach appears to approximate well the curve of (@ref(eq:chap2_exAR1)), while the asymptotic formula provides a reasonable approximation for \\(\\phi\\) being between -0.5 and 0.5. Naturally, the quality of this approximation would be far better for a larger sample size (here we consider \\(n = 10\\), which is a little “extreme”). 3.2.2 Sample Autocovariance and Autocorrelation Functions A natural estimator of the autocovariance function is given by: \\[\\hat \\gamma \\left( h \\right) = \\frac{1}{T}\\sum\\limits_{t = 1}^{T - h} {\\left( {{X_t} - \\bar X} \\right)\\left( {{X_{t + h}} - \\bar X} \\right)} \\] leading to the following “plug-in” estimator of the autocorrelation function: \\[\\hat \\rho \\left( h \\right) = \\frac{{\\hat \\gamma \\left( h \\right)}}{{\\hat \\gamma \\left( 0 \\right)}}.\\] A graphical representation of the autocorrelation function is often the first step for any time series analysis (again assuming the process to be stationary). Consider the following simulated example: # Set seed for reproducibility set.seed(2241) # Simulate 100 observation from a Gaussian white noise Xt = gen_gts(100, WN(sigma2 = 1)) # Compute autocorrelation acf_Xt = ACF(Xt) # Plot autocorrelation plot(acf_Xt, show.ci = FALSE) In this example, the true autocorrelation is equal to zero at any lag \\(h \\neq 0\\), but obviously the estimated autocorrelations are random variables and are not equal to their true values. It would therefore be useful to have some knowledge about the variability of the sample autocorrelations (under some conditions) to assess whether the data comes from a completely random series or presents some significant correlation at certain lags. The following result provides an asymptotic solution to this problem: Theorem 3.2 If \\(X_t\\) is a strong white noise with finite fourth moment, then \\(\\hat{\\rho}(h)\\) is approximately normally distributed with mean \\(0\\) and variance \\(n^{-1}\\) for all fixed \\(h\\). The proof of this Theorem is given in Appendix ??. Using this result, we now have an approximate method to assess whether peaks in the sample autocorrelation are significant by determining whether the observed peak lies outside the interval \\(\\pm 2/\\sqrt{T}\\) (i.e. an approximate 95% confidence interval). Returning to our previous example and adding confidence bands to the previous graph, we obtain: # Plot autocorrelation with confidence bands plot(acf_Xt) It can now be observed that most peaks lie within the interval \\(\\pm 2/\\sqrt{T}\\) suggesting that the true data generating process is uncorrelated. Example 3.2 To illustrate how the autocorrelation function can be used to reveal some “features” of a time series, we download the level of the Standard &amp; Poor’s 500 index, often abbreviated as the S&amp;P 500. This financial index is based on the market capitalization of 500 large companies having common stock listed on the New York Stock Exchange or the NASDAQ Stock Market. The graph below shows the index level and daily returns from 1990. # Load package library(quantmod) # Download S&amp;P index getSymbols(&quot;^GSPC&quot;, from=&quot;1990-01-01&quot;, to = Sys.Date()) ## [1] &quot;GSPC&quot; # Compute returns GSPC.ret = ClCl(GSPC) # Plot index level and returns par(mfrow = c(1,2)) plot(GSPC, main = &quot; &quot;, ylab = &quot;Index level&quot;) ## Warning in plot.xts(GSPC, main = &quot; &quot;, ylab = &quot;Index level&quot;): only the ## univariate series will be plotted plot(GSPC.ret, main = &quot; &quot;, ylab = &quot;Daily returns&quot;) From these graphs, it is clear that the returns are not identically distributed as the variance seems to vary with time, and clusters with either high or low volatility can be observed. These characteristics of financial time series is well known and in the Chapter 5, we will discuss how the variance of such process can be approximated. Nevertheless, we compute the empirical autocorrelation function of the S&amp;P 500 return to evaluate the degree of “linear” dependence between observations. The graph below presents the empirical autocorrelation. sp500 = na.omit(GSPC.ret) names(sp500) = paste(&quot;S&amp;P 500 (1990-01-01 - &quot;,Sys.Date(),&quot;)&quot;, sep = &quot;&quot;) plot(ACF(sp500)) As expected, the autocorrelation is small but it might be reasonable to believe that this sequence is not purely uncorrelated. Unfortunately, Theorem 1 is based on an asymptotic argument and since the confidence bands constructed are also asymptotic, there are no “exact” tools that can be used in this case. To study the validity of these results when \\(n\\) is “small” we performed a simulation. In the latter, we simulated processes following from a Gaussian white noise and examined the empirical distribution of \\(\\hat{\\rho}(3)\\) with different sample sizes (i.e. \\(n\\) is set to 5, 10, 30 and 300). Intuitively, the “quality” of the approximation provided by Theorem 1 should increase with the sample size \\(n\\). The code below performs such a simulation and compares the empirical distribution of \\(\\sqrt{n} \\hat{\\rho}(3)\\) with a normal distribution with mean 0 and variance 1 (its asymptotic distribution), which is depicted using a red line. # Number of Monte Carlo replications B = 10000 # Define considered lag h = 3 # Sample size considered N = c(5, 10, 30, 300) # Initialisation result = matrix(NA,B,length(N)) # Set seed set.seed(1) # Start Monte Carlo for (i in seq_len(B)){ for (j in seq_along(N)){ # Simluate process Xt = rnorm(N[j]) # Save autocorrelation at lag h result[i,j] = acf(Xt, plot = FALSE)$acf[h+1] } } # Plot results par(mfrow = c(2,length(N)/2)) for (i in seq_along(N)){ # Estimated empirical distribution hist(sqrt(N[i])*result[,i], col = &quot;royalblue1&quot;, main = paste(&quot;Sample size n =&quot;,N[i]), probability = TRUE, xlim = c(-4,4), xlab = &quot; &quot;) # Asymptotic distribution xx = seq(from = -10, to = 10, length.out = 10^3) yy = dnorm(xx,0,1) lines(xx,yy, col = &quot;red&quot;, lwd = 2) } As expected, it can clearly be observed that the asymptotic approximation is quite poor when \\(n = 5\\) but as the sample size increases the approximation improves and is very close when, for example, \\(n = 300\\). This simulation could suggest that Theorem 1 provides a relatively “close” approximation of the distribution of \\(\\hat{\\rho}(h)\\). 3.2.3 Robustness Issues The data generating process delivers a theoretical autocorrelation (autocovariance) function that, as explained in the previous section, can then be estimated through the sample autocorrelation (autocovariance) functions. However, in practice, the sample is often issued from a data generating process that is “close” to the true one, meaning that the sample suffers from some form of small contamination. This contamination is typically represented by a small amount of extreme observations that are called “outliers” that come from a process that is different from the true data generating process. The fact that the sample can suffer from outliers implies that the standard estimation of the autocorrelation (autocovariance) functions through the sample functions could be highly biased. The standard estimators presented in the previous section are therefore not “robust” and can behave badly when the sample suffers from contamination. To illustrate this limitation of a classical estimator, we consider the following two processes: \\[ \\begin{aligned} X_t &amp;= \\phi X_{t-1} + W_t, \\;\\;\\; W_t \\sim \\mathcal{N}(0,\\sigma_w^2),\\\\ Y_t &amp;= \\begin{cases} X_t &amp; \\quad \\text{with probability } 1 - \\epsilon\\\\ U_t &amp; \\quad \\text{with probability } \\epsilon\\\\ \\end{cases}, \\;\\;\\; U_t \\sim \\mathcal{N}(0,\\sigma_u^2), \\end{aligned} \\] when \\(\\epsilon\\) is “small” and \\(\\sigma_u^2 \\gg \\sigma_w^2\\), the process \\((Y_t)\\) can be interpreted as a “contaminated” version of \\((X_t)\\). The figure below represents one relalization of the processes \\((X_t)\\) and \\((Y_t)\\) using the following setting: \\(n = 100\\), \\(\\sigma_u^2 = 10\\), \\(\\phi = 0,5\\), \\(\\sigma_w^2 = 1\\) as well as \\(\\alpha = 0.05\\). Next, we consider a simulated example to highlight how the performance of a “classical” autocorrelation can deteriorate if the sample is contaminated ( i.e. what is the impact of using \\(Y_t\\) instead of \\(X_t\\), the “uncontaminated” process). In this simulation, we will use the setting presented above and consider \\(B = 10^3\\) bootstrap replications. The boxplots in each figure show how the standard autocorrelation estimator is centered around the true value (red line) when the sample is not contaminated (left boxplot) while it is considerably biased when the sample is contaminated (right boxplot), especially at the smallest lags. Indeed, it can be seen how the boxplots under contamination are often close to zero indicating that it does not detect much dependence in the data although it should. This is a known result in robustness, more specifically that outliers in the data can break the dependence structure and make it more difficult for the latter to be detected. In order to limit this problem, different robust estimators exist for time series problems which are designed to reduce contamination during the estimation procedure. Among these estimators, there are a few that estimate the autocorrelation (autocovariance) functions in a robust manner. One of these estimators is provided in the robacf() function in the “robcor” package. The following simulated example shows how it limits bias from contamination. Unlike in the previous simulation, we shall only consider data issued from the contaminated model, \\(Y_t\\), and compare the performance of two estimators (i.e. classical and robust autocorrelation estimators): The robust estimator remains close to the true value represented by the red line in the boxplots as opposed to the standard estimator. It can also be observed that to reduce the bias induced by contamination in the sample, robust estimators pay a certain price in terms of efficiency as highlighted by the boxplots that show more variability compared to those of the standard estimator. To assess how much is “lost” by the robust estimator compared to the classical one in terms of efficiency, we consider one last simulation where we examine the performance of two estimators on data issued from the uncontaminated model, i.e. \\((X_t)\\). Therefore, the only difference between this simulation and the previous one is the value of \\(\\alpha\\) set equal to \\(0\\); the code shall thus be omitted and the results are depicted below: It can be observed that both estimators provide extremely similar results, although the robust estimator is slightly more variable. Next, we consider the issue of robustness on the real data set coming from the domain of hydrology presented in Section ??. This data concerns monthly precipitation (in mm) over a certain period of time (1907 to 1972). Let us compare the standard and robust estimators of the autocorrelation functions: # TO DO It can be seen that, under certain assumptions (e.g. linear dependence), the standard estimator does not detect any significant autocorrelation between lags since the estimations all lie within the asymptotic confidence intervals. However, many of the robust estimations lie outside these confidence intervals at different lags indicating that there could be dependence within the data. If one were only to rely on the standard estimator in this case, there may be erroneous conclusions drawn on this data. Robustness issues therefore need to be considered for any time series analysis, not only when estimating the autocorrelation (autocovariance) functions. Finally, we return to S&amp;P 500 returns and compare the classical and robust autocorrelation estimators, which are presented in the figure below. # TO DO It can be observed that both estimators are very similar. Nevertheless, some small discrepancies can be observed. In particular, the robust estimators seem to indicate an absence of linear dependence while a slightly different interpretation might be achieved with the classical estimator. 3.2.4 Sample Cross-Covariance and Cross-Correlation Functions A natural estimator of the cross-covariance function is given by: \\[{{\\hat \\gamma }_{XY}}\\left( h \\right) = \\frac{1}{T}\\sum\\limits_{t = 1}^{T - h} {\\left( {{X_{t + h}} - \\bar X} \\right)\\left( {{Y_t} - \\bar Y} \\right)} \\] With this in mind, the “plug-in” estimator for the cross-correlation function follows: \\[{{\\hat \\rho }_{XY}}\\left( h \\right) = \\frac{{{{\\hat \\gamma }_{XY}}\\left( h \\right)}}{{\\sqrt {{{\\hat \\gamma }_X}\\left( 0 \\right)} \\sqrt {{{\\hat \\gamma }_Y}\\left( 0 \\right)} }}\\] Both of the above estimators are again only symmetric under the above index and lag transformation. "]
]
