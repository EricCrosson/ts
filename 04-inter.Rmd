# Under development

## Moving Average Models `r emo::ji("warning")`

The class of AR($p$) models is a very general class that allows to take into account different forms of linear dependence between past and future observations. However, there are some forms of linear dependence that can appear as "shocks" in the innovation noise of a time series. In this sense, we have already seen a model that describes a certain form of this dependence which is the MA(1) defined as

$$X_t = \theta W_{t-1} + W_t$$
where $(W_t)$ is a white noise process. As can be seen, the observed time series $(X_t)$ is a linear combination of the innovation process. In this section we generalise this process to the class of MA($q$) processes that are defined as follows.

```{definition, label="maq", name="Moving Average of Order Q"}
A Moving Average of Order $q$ or MA($q$) model is defined as follows:

$${X_t} = \theta_1 W_{t-1} + ... + \theta_q W_{t-q} + W_t,$$
  
where $\theta_q \neq 0$.
```

If we make use of the backshift operator defined earlier in this chapter, we can rewrite this model as:

\[\begin{aligned}
  X_t &= \theta_1 B W_t + ... + \theta_q  B^q W_t + W_t \\ 
  &= (\theta_1  B + ... + \theta_q  B^q + 1) W_t .\\
\end{aligned} \]

Based on this we can deliver the following definition.

```{definition, label="maqo", name="Moving Average Operator"}
The moving average operator is defined as
$$\theta(B) \equiv 1 + \theta_1 B + ... + \theta_q B^q.$$
```

This allows us to write an MA($q$) process as

$$X_t = \theta (B) W_t .$$
Following this definition, it is possible to see that an MA($q$) process is always stationary. Indeed, an MA($q$) respects the defintion of a linear process (see Definition \@ref(def:lp)) where $\psi_0 = 1$, $\psi_j = \theta_j$ for $j = 1, ..., q$, and $\psi_j = 0$ for $j > q$ and, based on this, we can show its stationarity.

```{example, name="Stationarity of an MA(q) Process"}
Based on the above definitions, we can rewrite an MA(q) process as $X_t = \sum\limits_{i = 0}^q {{\theta _i}{W_{t - i}}}$, where $\theta_0 = 1$ and assume the condition that $\sum\limits_{j = 0}^q {\theta _j^2} < \infty$ (if $\theta_j < \infty$ for all $j$ and $q < \infty$, this condition is verified). Using this expression we start verifying (weak) stationarity by checking the expectation of an MA(q) process which is given by

$$E\left[ X_t \right] = E\left[ \sum\limits_{i = 0}^q {{\theta _i}{W_{t - i}}} \right] = \sum\limits_{i = 0}^q {{\theta _i}}  E\left[ {W_{t - i}} \right] = 0,$$
  
which confirms that the expectation is constant. We now have to verify the conditions on the autocovariance function which is derived as follows:

\begin{align}
Cov \left( {{X_{t + h}},{X_t}} \right) &= Cov \left( {\sum\limits_{j = 0}^q {{\theta _j}{W_{t + h - j}}} ,\sum\limits_{i = 0}^q {{\theta _i}{W_{t - i}}} } \right) \\
&= \sum\limits_{j = 0}^q {\sum\limits_{i = 0}^q {{\theta _j}{\theta _i}Cov \left( {{W_{t + h - j}},{W_{t - i}}} \right)} }  \\
&= \sum\limits_{j = 0}^q {\sum\limits_{i = 0}^q {{\theta _j}{\theta _i}\underbrace {Cov \left( {{W_{t + h - j}},{W_{t - i}}} \right)}_{ = 0 \, \text{for} \, i = j - h}} }  + {1_{\left\{ {\left| h \right| \leqslant q} \right\}}}\sum\limits_{j = 0}^{q - \left| h \right|} {{\theta _{j + \left| h \right|}}{\theta _j} \underbrace{Cov \left( {{W_t},{W_t}} \right)}_{= \sigma^2}} \\
&= {1_{\left\{ {\left| h \right| \leqslant q} \right\}}}{\sigma ^2}\sum\limits_{j = 0}^{q - \left| h \right|} {{\theta _{j + \left| h \right|}}{\theta _j}}
\end{align}

As a result, we have:

\[{1_{\left\{ {\left| h \right| \leqslant q} \right\}}}{\sigma ^2}\sum\limits_{j = 0}^{q - \left| h \right|} {{\theta _{j + \left| h \right|}}{\theta _j}}  \leqslant {\sigma ^2}\sum\limits_{j = 0}^q {\theta _j^2}  < \infty \]

Given these results, we can see that the variance is given by ${\sigma ^2}\sum\limits_{j = 0}^q {\theta _j^2}$ which is finite and does not depend on time and the covariance only depends on the lag $h$ (not on the time $t$). Therefore, an MA(q) process is weakly stationary.
```

Although an MA($q$) process is always weakly stationary, it is important to well define these models since they can be characterized by certain parametrizations that don't allow them to be uniquely identify them. Indeed, the latter issues can be referred to as identifiability issues in which different MA($q$) models (of the same order $q$) can produce identical autocovariance functions. 

```{example, name="Non-uniqueness of MA models"}
For example, consider the following two MA(1) processes:

$$
\begin{aligned}
X_t &= \frac{1}{\theta}W_{t-1} + W_t,&{}& W_t\sim \mathcal{N} (0, \sigma^2\theta^2) \\
Y_t &= \theta Z_{t-1} + Z_t,&{}& Z_t\sim \mathcal{N} (0,\sigma^2)
\end{aligned}
$$

By observation, one can note that the models share the same expectation:

\[E\left[ {{X_t}} \right] = E\left[ {{Y_t}} \right] = 0\]

However, for the autocovariance, the process requires a bit more effort.

\begin{align}
Cov \left( {{X_t},{X_{t + h}}} \right) &= Cov \left( {{W_t} + \frac{1}{\theta }{W_{t - 1}},{W_{t + h}} + \frac{1}{\theta }{W_{t + h - 1}}} \right) = {1_{\left\{ {h = 0} \right\}}}{\sigma ^2}{\theta ^2} + {\sigma ^2} + {1_{\left\{ {\left| h \right| = 1} \right\}}}\frac{{{\sigma ^2}{\theta ^2}}}{\theta } = {\sigma ^2}\left( {{1_{\left\{ {h = 0} \right\}}}{\theta ^2} + 1 + {1_{\left\{ {\left| h \right| = 1} \right\}}}\theta } \right) \\
Cov \left( {{Y_t},{Y_{t + h}}} \right) &= Cov \left( {{Z_t} + \theta {Z_{t - 1}},{Z_{t + h}} + \theta {Z_{t + h - 1}}} \right) = {1_{\left\{ {h = 0} \right\}}}{\sigma ^2}{\theta ^2} + {\sigma ^2} + {1_{\left\{ {\left| h \right| = 1} \right\}}}{\sigma ^2}\theta  = {\sigma ^2}\left( {{1_{\left\{ {h = 0} \right\}}}{\theta ^2} + 1 + {1_{\left\{ {\left| h \right| = 1} \right\}}}\theta } \right)
\end{align}

From this we can see that $Cov \left( {{X_t},{X_{t + h}}} \right) = Cov \left( {{Y_t},{Y_{t + h}}} \right)$. 
```

Therefore, aside from having the same ACF, these two processes also have the same autocovariance function (due to the Gaussian innovations) and therefore are the same process (as highlighted in Chapter 2). In fact, among other orders $q$, we can determine MA(1) models that have identical ACF simply by defining $\theta$ and $\frac{1}{\theta}$ as the parameters of the two MA(1) models (as for the above example). Due to this issue, there is no objective criterion to determine which of the two models is to be considered for a given time series. 

The latter problem is of considerable importance to tackle especially when attempting to estimate the parameters of an MA($q$) model. For the purpose of illustration, let us again consider the example of the MA(1) process underlying the time series $(X_t)$ and $(Y_t)$ illustrated above and define their vector of observations as follows

\[\vec X = \left[ {\begin{array}{*{20}{c}}
  {{X_1}} \\ 
   \vdots  \\ 
  {{X_T}} 
\end{array}} \right],\vec Y = \left[ {\begin{array}{*{20}{c}}
  {{Y_1}} \\ 
   \vdots  \\ 
  {{Y_T}} 
\end{array}} \right].\]

Thus, the covariance matrix of both models is given by:

\[Cov \left( {\vec X} \right) = {\sigma ^2}\left[ {\begin{array}{*{20}{c}}
  {\left( {1 + {\theta ^2}} \right)}&\theta &0& \cdots &0 \\ 
  \theta &{\left( {1 + {\theta ^2}} \right)}&\theta &{}& \vdots  \\ 
  0&\theta &{\left( {1 + {\theta ^2}} \right)}&{}&{} \\ 
   \vdots &{}&{}& \ddots &{} \\ 
  0& \cdots &{}&{}&{\left( {1 + {\theta ^2}} \right)} 
\end{array}} \right] = Cov \left( {\vec Y} \right) = \Omega \]

If we were to denote $\vec \beta$ as being the parameter vector of the MA(1) model of $(X_t)$, then we could define the likelihood function (to compute the MLE estimator) as follows:

\[L\left( {\vec \beta |\vec X} \right) = {\left( {2\pi } \right)^{ - \frac{N}{2}}}{\left| \Omega  \right|^{ - \frac{1}{2}}}\exp \left( { - \frac{1}{2}{{\vec X}^T}{\Omega ^{ - 1}}\vec X} \right)\]

The MLE is the value of $\vec \beta$ that maximzes the likelihood function above. However, this solution is not unique since there are two value of $\vec \beta$ that maximise the likelihood which are ${{\vec \beta }_1} = \left[ {\begin{array}{*{20}{c}}
  \theta  \\ 
  {{\sigma ^2}} 
\end{array}} \right],{{\vec \beta }_2} = \left[ {\begin{array}{*{20}{c}}
  {\frac{1}{\theta }} \\ 
  {{\sigma ^2}\theta } 
\end{array}} \right]$. Indeed, for these two parameter vectors we would therefore have

\[L\left( {{{\vec \beta }_1}|\vec X} \right) = L\left( {{{\vec \beta }_2}|\vec X} \right).\]

Given that this identifiability issue can greatly affect estimation procedures, the accepted rule to choose which MA($q$) model to estimate is to only consider the MA($q$) models that, when rewritten in an AR($p$) form, can be defined as being AR($\infty$) models. These models are called *invertible*. Let us consider the example MA(1) model

$$X_t = \theta W_{t-1} + W_t,$$
which can be rewritten as

$$W_t = -\theta W_{t-1} + X_t.$$
Now the MA(1) model has taken the form of an AR(1) model and, using the recursive approach that we used to study the AR($p$) models, we can show that we get to the form

$$W_t = (-\theta)^t W_0 + \sum_{j = 0}^t (-\theta)^j X_{t-j}.$$
If we assume $|\theta| < 1$, then we finally get to the causal AR(1) representation

$$W_t = \sum_{j = 0}^{\infty} (-\theta)^j X_{t-j}.$$

Therefore, between two MA(1) models with two identical ACFs, we choose the invertible MA(1) which respects the condition $|\theta| < 1$. The same reasoning as for causal AR($p$) models is also applied for invertible MA($q$) models thereby restricting the possible values of the parameters of an MA($q$) model to the admissable region defined in the same way as for causal AR($p$) models. This reasoning also allows us to study MA($q$) models similarly to AR($p$) models. Indeed, we can write an MA(1) model as

$$X_t = \theta(B)W_t,$$
where $\theta(B)$ is the moving average operator defined earlier. Based on this we can rewrite the model as

$$\frac{1}{\theta(B)} X_t = W_t,$$
and, if $|\theta| < 1$, we can rexpress the inverse moving average operator $\theta^{-1}(B)$ as $\sum_{j = 0}^{\infty} (-\theta)^j B^j$ which provides us with the invertible representation of an MA(1) process seen earlier.

To conclude this section on MA($q$) models, let us consider the behaviour of the ACF and PACF for these kinds of models. As an example let us consider the following three MA($q$) models:

$$
\begin{aligned}
X_t &= 0.9 W_{t-1} + W_t \\
Y_t &= 1.2W_{t-1} -0.3 W_{t-2}+ W_t \\
Z_t &= -1.5 W_{t-1} + 0.5 W_{t-2} - 0.2 W_{t-3} + W_t.
\end{aligned}
$$
The theoretical ACF plots of these three models are shown below.

```{r}
par(mfrow = c(1,3))
plot(theo_acf(ar = 0, ma = 0.9))
plot(theo_acf(ar = 0, ma = c(1.2, -0.3)))
plot(theo_acf(ar = 0, ma = c(-1.5, 0.5, -0.2)))
```

As you can notice, the values of the ACF plots become zero as soon as the lag of the ACF goes beyond the order $q$ of the MA($q$) model. Hence, the ACF plot of an MA($q$) model plays the same role that the PACF plays for AR($p$) models: it helps determine the order $q$ of the MA($q$) process generated a given time series. Let us now consider the PACF plots of the same models defined above.

```{r}
par(mfrow = c(1,3))
plot(theo_pacf(ar = 0, ma = 0.9))
plot(theo_pacf(ar = 0, ma = c(1.2, 0.3)))
plot(theo_pacf(ar = 0, ma = c(-1.5, 0.5, -0.2)))
```

In this case, we can see that the PACF plots show a certain exponential (sinusoidal) decay that is usually observed in the ACF plots of AR($p$) models. Therefore, the roles of the ACF and PACF in identifying the kind of AR($p$) models underlying an observed time series is completely inversed when considering MA($q$) models. Indeed, the PACF plot of an MA($q$) model has an exponentially decay while the ACF cuts off after the lag has reached the order of the model ($q$ for MA($q$) models). Let us simulate $T = 200$ observations from $(X_t)$ to obtain the following time series

```{r}
set.seed(6)
Xt = gen_gts(MA(theta = 0.9, sigma2 = 1), n = 200)
plot(Xt)
```

Using this data, let us estimate the ACF and PACF of this time series.

```{r}
par(mfrow = c(1,2))
plot(auto_corr(Xt))
plot(auto_corr(Xt, pacf = TRUE))
```

From these plots, using the PACF we could conclude that this function appears to decay as the lags increase (as the PACF of an MA($q$) model is expected to do) while using the ACF we could state that the correct order for the MA($q$) model is $q = 1$ since the ACF does not appear to be significant after lag 1. Let us now also consider the model selection criteria described in the previous section by applying the `select()` function to the simulated time series.

```{r}
best_ma = select(MA(10), Xt)
```

Also in this case we see that the model selection criteria all tend to agree with each other on the fact that the best model to describe the observed time series is indeed an MA(1) model.